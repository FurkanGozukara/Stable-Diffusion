# DreamBooth Got Buffed - 22 January Update - Much Better Success Train Stable Diffusion Models Web UI

## Full tutorial link > https://www.youtube.com/watch?v=KwxNcGhHuLY

[![DreamBooth Got Buffed - 22 January Update - Much Better Success Train Stable Diffusion Models Web UI](https://img.youtube.com/vi/KwxNcGhHuLY/sddefault.jpg)](https://www.youtube.com/watch?v=KwxNcGhHuLY "DreamBooth Got Buffed - 22 January Update - Much Better Success Train Stable Diffusion Models Web UI")

[![image](https://img.shields.io/discord/772774097734074388?label=Discord&logo=discord)](https://discord.com/servers/software-engineering-courses-secourses-772774097734074388) [![Hits](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/DreamBooth-Got-Buffed-22-January-Update-Much-Better-Success-Train-Stable-Diffusion-Models-Web-UI.md.svg?style=plastic&label=Hits%20Since%2025.08.27&labelColor=007ec6&logo=SECourses)](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/DreamBooth-Got-Buffed-22-January-Update-Much-Better-Success-Train-Stable-Diffusion-Models-Web-UI.md)
[![Patreon](https://img.shields.io/badge/Patreon-Support%20Me-F2EB0E?style=for-the-badge&logo=patreon)](https://www.patreon.com/c/SECourses) [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&logoColor=black)](https://www.buymeacoffee.com/DrFurkan) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/Medium-Follow%20Me-800080?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@furkangozukara) [![Codio](https://img.shields.io/static/v1?style=for-the-badge&message=Articles&color=4574E0&logo=Codio&logoColor=FFFFFF&label=CivitAI)](https://civitai.com/user/SECourses/articles) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/DeviantArt-Follow%20Me-990000?style=for-the-badge&logo=deviantart&logoColor=white)](https://www.deviantart.com/monstermmorpg)

[![YouTube Channel](https://img.shields.io/badge/YouTube-SECourses-C50C0C?style=for-the-badge&logo=youtube)](https://www.youtube.com/SECourses)  [![Furkan G√∂z√ºkara LinkedIn](https://img.shields.io/badge/LinkedIn-Follow%20Me-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/furkangozukara/)   [![Udemy](https://img.shields.io/static/v1?style=for-the-badge&message=Stable%20Diffusion%20Course&color=A435F0&logo=Udemy&logoColor=FFFFFF&label=Udemy)](https://www.udemy.com/course/stable-diffusion-dreambooth-lora-zero-to-hero/?referralCode=E327407C9BDF0CEA8156) [![Twitter Follow Furkan G√∂z√ºkara](https://img.shields.io/badge/Twitter-Follow%20Me-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/GozukaraFurkan)


Our Discord : [https://discord.gg/HbqgGaZVmr.](https://discord.gg/HbqgGaZVmr.) Newest update of DreamBooth extension of Automatic1111 brought huge quality and success improvement. If I have been of assistance to you and you would like to show your support for my work, please consider becoming a patron on ü•∞ [https://www.patreon.com/SECourses](https://www.patreon.com/SECourses)

Playlist of Stable Diffusion Tutorials, #Automatic1111 and Google Colab Guides, DreamBooth, Textual Inversion / Embedding, #LoRA, AI Upscaling, Pix2Pix, Img2Img:

[https://www.youtube.com/playlist?list=PL_pbwdIyffsmclLl0O144nQRnezKlNdx3](https://www.youtube.com/playlist?list=PL_pbwdIyffsmclLl0O144nQRnezKlNdx3)

In this video, I have explained how to use the newest DreamBooth update of Automatic1111 Web UI extension. With new update, now it is much more successful to teach your subjects into any Stable Diffusion model.

The update has just been released today : 22 January 2023

Zero To Hero Stable Diffusion DreamBooth Tutorial By Using Automatic1111 Web UI - Ultra Detailed

[https://youtu.be/Bdl-jWR3Ukc](https://youtu.be/Bdl-jWR3Ukc)

#Dreambooth revision: fd51c0b2ed20566c60affa853a32ebce1b0a1139

SD-WebUI revision: d8f8bcb821fa62e943eb95ee05b8a949317326fe

AUTOMATIC1111: [https://github.com/AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)

Git Bash download link : [https://git-scm.com/downloads](https://git-scm.com/downloads)

How To Do Stable Diffusion Textual Inversion (TI) / Text Embeddings By Automatic1111 Web UI Tutorial

[https://youtu.be/dNOpWt-epdQ](https://youtu.be/dNOpWt-epdQ)

Our discord channel : [https://discord.com/invite/HbqgGaZVmr](https://discord.com/invite/HbqgGaZVmr)

[00:00:00](https://youtu.be/KwxNcGhHuLY?t=0) Introduction to the new buffed DreamBooth extension

[00:00:30](https://youtu.be/KwxNcGhHuLY?t=30) How to checkout the SD and DreamBooth version used in this video by commit hash IDs

[00:01:40](https://youtu.be/KwxNcGhHuLY?t=100) How to compose DreamBooth training model

[00:02:13](https://youtu.be/KwxNcGhHuLY?t=133) Best configuration of settings tab of DreamBooth extension training

[00:03:37](https://youtu.be/KwxNcGhHuLY?t=217) Lowest VRAM settings to use DreamBooth extension and do DreamBooth training

[00:03:59](https://youtu.be/KwxNcGhHuLY?t=239) Why not use --no-half on SD 1.5 and use on SD 2.1

[00:04:46](https://youtu.be/KwxNcGhHuLY?t=286) New setting AdamW Weight Decay

[00:05:10](https://youtu.be/KwxNcGhHuLY?t=310) New setting Scale Prior Loss

[00:06:14](https://youtu.be/KwxNcGhHuLY?t=374) How exactly filewords work in Stable Diffusion DreamBooth training

[00:08:53](https://youtu.be/KwxNcGhHuLY?t=533) Sample images generated during training

[00:09:30](https://youtu.be/KwxNcGhHuLY?t=570) Prompting difference of new DreamBooth extension than previous versions

[00:10:25](https://youtu.be/KwxNcGhHuLY?t=625) How to test different checkpoints saved during training by X/Y plot script

From official paper : [https://arxiv.org/pdf/2208.12242.pdf](https://arxiv.org/pdf/2208.12242.pdf)

Our new approach, DreamBooth, addresses the limitation of current text-to-image models by allowing for "personalization" of these models to better fit the specific needs of users. By providing just a few images of a subject as input, DreamBooth fine-tunes a pre-trained text-to-image model (such as Imagen) to learn to associate a unique identifier with that subject. This allows for the generation of novel, photorealistic images of the subject in various scenes, poses, views, and lighting conditions, even those not present in the reference images.

Our technique utilizes a new autogenous class-specific prior preservation loss which enables the preservation of the subject's key features while still allowing for diverse synthesis of the subject. This opens up possibilities for a wide range of previously unassailable tasks such as subject recontextualization, text-guided view synthesis, appearance modification, and artistic rendering.

Imagine your own dog traveling the world, your favorite bag on display in the most exclusive showrooms, or your parrot as the main character of an illustrated storybook. These are just a few examples of the type of creative and unique content that can be generated using DreamBooth. Our approach allows for the natural and seamless integration of specific subjects into new and diverse contexts, making the impossible possible.

Our goal is to use just a few casually captured images of a specific subject, without any textual description, to generate new images of the subject with high detail fidelity and variations guided by text prompts. The input images can be captured in varying settings and contexts and the output variations can include changes in the subject's location, properties such as color, shape, and species, as well as modifications to the subject's pose, expression, material, and other semantic changes. Our approach utilizes the powerful prior of text-to-image models to enable a wide range of modifications.

To accomplish this, we first implant the subject instance into the output domain of the model and assign it a unique identifier. We present a new method for fine-tuning the model to use its prior for the specific subject instance while also addressing issues of overfitting and language drift. Our approach includes an autogenous class-specific prior preservation loss which encourages the model to generate diverse instances of the same class as the subject.

Our goal is to add a new key-value pair to the text-to-image model's "dictionary" that will allow us to generate fully-novel images of a specific subject with meaningful semantic modifications guided by a text prompt. We achieve this by fine-tuning the model with a small number of images of the subject. The question then becomes how to guide this process.



### Video Transcription


- [00:00:01](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=1) Greetings everyone. In this video I will&nbsp; show you the newest update of DreamBooth&nbsp;&nbsp;

- [00:00:05](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=5) extension of Automatic1111 web UI. The new&nbsp; update brought huge improvement to teach our&nbsp;&nbsp;

- [00:00:11](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=11) subjects into Stable Diffusion models. It&nbsp; is like 10x better than before. This will&nbsp;&nbsp;

- [00:00:16](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=16) be a short video of best settings. Therefore,&nbsp; if you are interested in a detailed tutorial,&nbsp;&nbsp;

- [00:00:21](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=21) please first watch my very thorough DreamBooth&nbsp; tutorial. Let me show you it. This is the video&nbsp;&nbsp;

- [00:00:27](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=27) that you need to watch. So let me show you&nbsp; which commit, which version of DreamBooth and&nbsp;&nbsp;

- [00:00:31](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=31) SD web UI I am using. This is the DreamBooth&nbsp; revision and this is the SD web UI revision.&nbsp;&nbsp;

- [00:00:37](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=37) To use these specific versions, just open the Git&nbsp; Bash. You need to install it from Google and then&nbsp;&nbsp;

- [00:00:44](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=44) move to your folder installation folder, like&nbsp; this CD and drag and drop. Then use git checkout&nbsp;&nbsp;

- [00:00:52](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=52) command like this and paste the commit ID.&nbsp; And now it will checkout to that version.&nbsp;&nbsp;

- [00:01:00](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=60) With the newest update, you are able to obtain&nbsp; very highly stylized and very good results with&nbsp;&nbsp;

- [00:01:06](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=66) a very simple command like this. You see, I&nbsp; just added Tomer Hanuka to my token, ohwx and,&nbsp;&nbsp;

- [00:01:15](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=75) as usually, I am using the negative prompt and it&nbsp; is able to stylize it very well. This is the data&nbsp;&nbsp;

- [00:01:21](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=81) set I have used. It is not great, but it is&nbsp; not also very bad. All of the backgrounds and&nbsp;&nbsp;

- [00:01:26](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=86) the clothes are different, different angles.&nbsp; Okay, now let me show you the best settings.&nbsp;&nbsp;

- [00:01:33](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=93) This is not necessary, but make sure that you&nbsp; have selected the version 1.5, pruned ckpt here,&nbsp;&nbsp;

- [00:01:39](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=99) or whatever the model that you want to train on.&nbsp; Go to the DreamBooth tab in the create menu. Give&nbsp;&nbsp;

- [00:01:45](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=105) a name like tutorial. Now we are checking whether&nbsp; that 512 pixel model or not. Source checkpoint&nbsp;&nbsp;

- [00:01:55](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=115) this is the important one: version 1.5 pruned and&nbsp; create model. You should get a message like this:&nbsp;&nbsp;

- [00:02:02](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=122) checkpoint successfully extracted to our working&nbsp; folders. Go to the settings tab and in here click&nbsp;&nbsp;

- [00:02:08](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=128) performance wizard first. There is also new LoRA&nbsp; version. However, I couldn't get good results&nbsp;&nbsp;

- [00:02:14](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=134) with LoRA yet. Hopefully I will make another new&nbsp; video for LoRA, so I am not picking it. Do not&nbsp;&nbsp;

- [00:02:20](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=140) check this checkbox. It is usually not working. By&nbsp; the way, with the new model, with the new update,&nbsp;&nbsp;

- [00:02:28](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=148) it is working much faster than before. In terms of&nbsp; number of epochs required to train your subject.&nbsp;&nbsp;

- [00:02:35](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=155) For example, I was able to teach my face with&nbsp; just 30 epochs, unlike previously I was using&nbsp;&nbsp;

- [00:02:42](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=162) like 150 epochs. So still, you can set this as&nbsp; 200, set this as 0, set this as 5 because it is&nbsp;&nbsp;

- [00:02:50](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=170) getting overtrained very quickly. With the newest&nbsp; update I think they have fixed a lot of things and&nbsp;&nbsp;

- [00:02:57](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=177) now this is not automatically checking. Checked&nbsp; for me. I have RTX 3060. When you check this it&nbsp;&nbsp;

- [00:03:04](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=184) will actually reduce the memory usage, but it will&nbsp; slow down your process and this is also for saving&nbsp;&nbsp;

- [00:03:11](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=191) VRAM. So this is constant with warmup. It is&nbsp; fine. I didn't change the learning rate. It works&nbsp;&nbsp;

- [00:03:16](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=196) fine. Do not check this box and do not check this&nbsp; checkbox as well. Here, this is very important to&nbsp;&nbsp;

- [00:03:23](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=203) understand whether you started overtraining or&nbsp; not. So photo of ohwx man by Tomer Hanuka. The&nbsp;&nbsp;

- [00:03:33](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=213) ohwx is our token. It is a very rare token and man&nbsp; is class token. Then in the advanced tab, if you&nbsp;&nbsp;

- [00:03:40](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=220) have more than 12GB VRAM, you can check this "use&nbsp; EMA". It will improve your success. Use 8bit Adam.&nbsp;&nbsp;

- [00:03:46](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=226) We have 16bf and xformers. By the way, let me also&nbsp; show you the command line arguments I used. The&nbsp;&nbsp;

- [00:03:52](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=232) only command line argument I used is --xformers,&nbsp; and I am also using disable safe unpicked. But&nbsp;&nbsp;

- [00:03:58](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=238) this is not necessary. I didn't add --no-half,&nbsp; because this is necessary for SD version 2.1.&nbsp;&nbsp;

- [00:04:09](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=249) This will slow down your image generation and&nbsp; also training, but this is all only necessary&nbsp;&nbsp;

- [00:04:15](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=255) for SD 2.1. So if you are not working with SD 2.1&nbsp; version, do not add this. It will slow down your&nbsp;&nbsp;

- [00:04:21](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=261) process. I am checking cache latents. This will&nbsp; speed up the training but it will increase the&nbsp;&nbsp;

- [00:04:27](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=267) VRAM usage. Since I have 12GB, it works fine. I am&nbsp; also training UNET. If you don't have enough VRAM&nbsp;&nbsp;

- [00:04:34](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=274) then you should uncheck this. Okay, It gave an&nbsp; error when I unchecked it. I think they will fix&nbsp;&nbsp;

- [00:04:42](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=282) it. So I am leaving these default. There is a new&nbsp; settings, as you can see, the Weight Decay AdamW&nbsp;&nbsp;

- [00:04:49](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=289) optimizer. It says that this will more generalize&nbsp; your images as you make this number bigger. If you&nbsp;&nbsp;

- [00:04:56](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=296) want your subject to be as close much as to you.&nbsp; By default it is 0.01. I did left it default and&nbsp;&nbsp;

- [00:05:03](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=303) it worked very well. And the pad tokens: these&nbsp; are for when you are using the [filewords] and&nbsp;&nbsp;

- [00:05:08](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=308) shuffle tags, image captions related. There's also&nbsp; one new, another option: prior loss. I asked this&nbsp;&nbsp;

- [00:05:15](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=315) to the developer of the extension developer. The&nbsp; answer of the developer is this: As you can see,&nbsp;&nbsp;

- [00:05:21](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=321) Scale prior loss loss, decrease the prior loss&nbsp; weight as training progress. When you enable it,&nbsp;&nbsp;

- [00:05:26](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=326) you get a "minimum prior loss" setting and&nbsp; "prior loss target". The target is: at what epoch&nbsp;&nbsp;

- [00:05:30](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=330) should the prior loss weight reach the minimum. He&nbsp; also commented as not sure if it matters or helps,&nbsp;&nbsp;

- [00:05:38](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=338) but it will stand to the reason that as we train&nbsp; our model, we want the weight of the class images&nbsp;&nbsp;

- [00:05:43](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=343) to be lower than that the instance images, as the&nbsp; model should already better know the subject. I&nbsp;&nbsp;

- [00:05:49](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=349) made test with enabling this and disabling it. I&nbsp; think when this is not enabled, it worked better&nbsp;&nbsp;

- [00:05:54](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=354) for me, but it is up to you to test it. Then&nbsp; we go to the concept. Okay, In the directories,&nbsp;&nbsp;

- [00:06:00](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=360) as usual, we are setting our first training set&nbsp; directory. This is my training set directory.&nbsp;&nbsp;

- [00:06:06](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=366) So I copied it, I pasted it, then set it set a&nbsp; new. If you have prior classification images,&nbsp;&nbsp;

- [00:06:12](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=372) like I have, you can give it its directory&nbsp; or you can give a new directory like this.&nbsp;&nbsp;

- [00:06:18](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=378) Okay, Now [filewords]. People are getting&nbsp; very confused with this. Let I will explain&nbsp;&nbsp;

- [00:06:23](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=383) to you what is this actually in this video.&nbsp; Let's say you have typed as ohwx here and&nbsp;&nbsp;

- [00:06:30](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=390) you have type of [filewords] here. So this&nbsp; will become like this when it is processing.&nbsp;&nbsp;

- [00:06:36](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=396) For example, let's say I am using image captions,&nbsp; like in this example, and the caption of the&nbsp;&nbsp;

- [00:06:42](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=402) image is like this. Okay, Let me show you.&nbsp; So when I use the configuration like this,&nbsp;&nbsp;

- [00:06:52](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=412) it will append the instance token I have written&nbsp; in the [filewords] to the beginning. Then it will&nbsp;&nbsp;

- [00:06:57](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=417) read the captions here. So the final prompt&nbsp; will be like this. If I also add a word here,&nbsp;&nbsp;

- [00:07:04](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=424) let's say example word, then this example word&nbsp; will be appended here. So this is also equal to&nbsp;&nbsp;

- [00:07:12](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=432) using like this. When you use it like this, it&nbsp; will become actually exact exactly like this. So&nbsp;&nbsp;

- [00:07:19](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=439) this is how [filewords] and instance token works.&nbsp; I am not using any [filewords] and image caption.&nbsp;&nbsp;

- [00:07:26](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=446) So I am setting as the instance token as ohwx man.&nbsp; So the ohwx is our token and the man is our class.&nbsp;&nbsp;

- [00:07:37](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=457) In this video I have very clearly and very&nbsp; detailedly and very technically explained&nbsp;&nbsp;

- [00:07:43](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=463) how the Stable Diffusion works, how it is&nbsp; composed by vectors and different tokens.&nbsp;&nbsp;

- [00:07:49](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=469) For class prompt, we are using photo of man&nbsp; and for sample image prompt: we are using&nbsp;&nbsp;

- [00:07:57](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=477) photo of ohwx man as you can see, then, class&nbsp; images per instance, I have used 48 images and in&nbsp;&nbsp;

- [00:08:10](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=490) the saving tab, make sure that you are generating&nbsp; a ckpt file and saving during training because,&nbsp;&nbsp;

- [00:08:16](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=496) when you see it is over training, you will&nbsp; see use the certain checkpoints and then&nbsp;&nbsp;

- [00:08:22](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=502) click save settings. Okay, I think&nbsp; everything is pretty much ready and.&nbsp;&nbsp;

- [00:08:30](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=510) Just hit train tab and it will start generating&nbsp; the classification images and after that it&nbsp;&nbsp;

- [00:08:37](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=517) will start training the model. You see, the&nbsp; classification images are very weird and. If&nbsp;&nbsp;

- [00:08:43](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=523) you handpick the good classification images then&nbsp; it may improve your success. It is up to you to&nbsp;&nbsp;

- [00:08:49](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=529) test. But I didn't touch the classification images&nbsp; actually. I just used whatever it generated.&nbsp;&nbsp;

- [00:08:55](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=535) Okay, here the training samples generated&nbsp; during my training. So, as you can see,&nbsp;&nbsp;

- [00:09:01](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=541) even in the 10th epoch this is the I have saved&nbsp; preview images and checkpoints every 10 epoch.&nbsp;&nbsp;

- [00:09:08](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=548) It is already learned a very good my subject,&nbsp; my face. And after 30 epochs we are losing the&nbsp;&nbsp;

- [00:09:15](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=555) styling with by using the sanity. As you can see,&nbsp; in the sanity we used by Tomer Hanuka style. So I&nbsp;&nbsp;

- [00:09:23](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=563) decided to do my tests on the just the 30 epochs&nbsp; and it worked very well, as I have shown you.&nbsp;&nbsp;

- [00:09:31](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=571) Also, when we analyze that you see, now&nbsp; we don't even need to increase the prompt&nbsp;&nbsp;

- [00:09:38](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=578) emphasis of our token, just using, like this, 1.1&nbsp; emphasis, and it is working, working very well.&nbsp;&nbsp;

- [00:09:45](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=585) And with 30 step and with 720 steps training,&nbsp; it is able to fully stylize my face and able&nbsp;&nbsp;

- [00:09:56](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=596) to generate very beautiful images. This&nbsp; is different than the previous trainings&nbsp;&nbsp;

- [00:10:01](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=601) of the DreamBooth extension of Automatic1111. So&nbsp; therefore, now, the things are really different,&nbsp;&nbsp;

- [00:10:07](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=607) really improved I think what they did is now they&nbsp; are properly able to keep up the prior loss and&nbsp;&nbsp;

- [00:10:14](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=614) previously they were not able to keep that.&nbsp; So now, with a very few number of epochs, the&nbsp;&nbsp;

- [00:10:22](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=622) model is able to learn our subject very well. You&nbsp; still can do a test of different epochs, so to do&nbsp;&nbsp;

- [00:10:31](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=631) that, you just need to use the X/Y plot and in&nbsp; here you can give the checkpoint names. You see&nbsp;&nbsp;

- [00:10:40](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=640) it's showing all of the names like this and just&nbsp; delete the ones that you don't want to test. And,&nbsp;&nbsp;

- [00:10:45](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=645) the Y plot. I suggest you to test CFG value.&nbsp; You can test 8, 7 9, 10, 11, 12 and whatever&nbsp;&nbsp;

- [00:10:52](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=652) you want. And when you do test you can see which&nbsp; epoch is working best for you. And I am really&nbsp;&nbsp;

- [00:11:02](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=662) happy with the newest update because it's&nbsp; certainly improved the training quality. So check&nbsp;&nbsp;

- [00:11:09](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=669) out the newest DreamBooth extension. Thank you&nbsp; very much for watching. Please like, subscribe and&nbsp;&nbsp;

- [00:11:15](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=675) leave a comment. You can join our discord channel&nbsp; and discuss everything and ask any questions. Go&nbsp;&nbsp;

- [00:11:21](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=681) to the about tab of our YouTube channel and&nbsp; in the bottom you will see official discord&nbsp;&nbsp;

- [00:11:25](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=685) channel link. And if you support us on Patreon,&nbsp; I would appreciate very much. This is keeping&nbsp;&nbsp;

- [00:11:30](https://www.youtube.com/watch?v=KwxNcGhHuLY&t=690) me to do more research and produce better quality&nbsp; videos. Hopefully see you later in another video.
