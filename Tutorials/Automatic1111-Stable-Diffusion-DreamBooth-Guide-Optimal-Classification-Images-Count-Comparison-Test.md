# Automatic1111 Stable Diffusion DreamBooth Guide: Optimal Classification Images Count Comparison Test

## Full tutorial link > https://www.youtube.com/watch?v=Tb4IYIYm4os

[![Automatic1111 Stable Diffusion DreamBooth Guide: Optimal Classification Images Count Comparison Test](https://img.youtube.com/vi/Tb4IYIYm4os/sddefault.jpg)](https://www.youtube.com/watch?v=Tb4IYIYm4os "Automatic1111 Stable Diffusion DreamBooth Guide: Optimal Classification Images Count Comparison Test")

[![image](https://img.shields.io/discord/772774097734074388?label=Discord&logo=discord)](https://discord.com/servers/software-engineering-courses-secourses-772774097734074388) [![Hits](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/Automatic1111-Stable-Diffusion-DreamBooth-Guide-Optimal-Classification-Images-Count-Comparison-Test.md.svg?style=plastic&label=Hits%20Since%2025.08.27&labelColor=007ec6&logo=SECourses)](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/Automatic1111-Stable-Diffusion-DreamBooth-Guide-Optimal-Classification-Images-Count-Comparison-Test.md)
[![Patreon](https://img.shields.io/badge/Patreon-Support%20Me-F2EB0E?style=for-the-badge&logo=patreon)](https://www.patreon.com/c/SECourses) [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&logoColor=black)](https://www.buymeacoffee.com/DrFurkan) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/Medium-Follow%20Me-800080?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@furkangozukara) [![Codio](https://img.shields.io/static/v1?style=for-the-badge&message=Articles&color=4574E0&logo=Codio&logoColor=FFFFFF&label=CivitAI)](https://civitai.com/user/SECourses/articles) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/DeviantArt-Follow%20Me-990000?style=for-the-badge&logo=deviantart&logoColor=white)](https://www.deviantart.com/monstermmorpg)

[![YouTube Channel](https://img.shields.io/badge/YouTube-SECourses-C50C0C?style=for-the-badge&logo=youtube)](https://www.youtube.com/SECourses)  [![Furkan G√∂z√ºkara LinkedIn](https://img.shields.io/badge/LinkedIn-Follow%20Me-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/furkangozukara/)   [![Udemy](https://img.shields.io/static/v1?style=for-the-badge&message=Stable%20Diffusion%20Course&color=A435F0&logo=Udemy&logoColor=FFFFFF&label=Udemy)](https://www.udemy.com/course/stable-diffusion-dreambooth-lora-zero-to-hero/?referralCode=E327407C9BDF0CEA8156) [![Twitter Follow Furkan G√∂z√ºkara](https://img.shields.io/badge/Twitter-Follow%20Me-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/GozukaraFurkan)


Sign up RunPod: [https://bit.ly/RunPodIO.](https://bit.ly/RunPodIO.) Our Discord : [https://discord.gg/HbqgGaZVmr.](https://discord.gg/HbqgGaZVmr.) New best training settings for DreamBooth training in Automatic1111 Web UI. If I have been of assistance to you and you would like to show your support for my work, please consider becoming a patron on ü•∞ [https://www.patreon.com/SECourses](https://www.patreon.com/SECourses)

Playlist of #StableDiffusion Tutorials, #Automatic1111 and Google Colab Guides, #DreamBooth, Textual Inversion / Embedding, LoRA, AI Upscaling, Pix2Pix, Img2Img:

[https://www.youtube.com/playlist?list=PL_pbwdIyffsmclLl0O144nQRnezKlNdx3](https://www.youtube.com/playlist?list=PL_pbwdIyffsmclLl0O144nQRnezKlNdx3)

Easiest Way to Install & Run Stable Diffusion Web UI on PC by Using Open Source Automatic Installer:

[https://youtu.be/AZg6vzWHOTA](https://youtu.be/AZg6vzWHOTA)

How to use Stable Diffusion V2.1 and Different Models in the Web UI - SD 1.5 vs 2.1 vs Anything V3:

[https://youtu.be/aAyvsX-EpG4](https://youtu.be/aAyvsX-EpG4)

Zero To Hero Stable Diffusion DreamBooth Tutorial By Using Automatic1111 Web UI - Ultra Detailed:

[https://youtu.be/Bdl-jWR3Ukc](https://youtu.be/Bdl-jWR3Ukc)

Sketches into Epic Art with 1 Click: A Guide to Stable Diffusion ControlNet in Automatic1111 Web UI:

[https://youtu.be/vhqqmkTBMlU](https://youtu.be/vhqqmkTBMlU)

Ultimate RunPod Tutorial For Stable Diffusion - Automatic1111 - Data Transfers, Extensions, CivitAI:

[https://youtu.be/QN1vdGhjcRc](https://youtu.be/QN1vdGhjcRc)

8 GB LoRA Training - Fix CUDA & xformers For DreamBooth and Textual Inversion in Automatic1111 SD UI:

[https://youtu.be/O01BrQwOd-Q](https://youtu.be/O01BrQwOd-Q)

2400 Photo Of Man classification images:

[https://drive.google.com/file/d/1qBf8VyUbmPNalKqm076yOsQjE8BrcG7R/view](https://drive.google.com/file/d/1qBf8VyUbmPNalKqm076yOsQjE8BrcG7R/view)

[00:00:00](https://youtu.be/Tb4IYIYm4os?t=0) Introduction to Best Settings of DreamBooth training experiment

[00:00:56](https://youtu.be/Tb4IYIYm4os?t=56) How to close initially started Web UI instance on RunPod Stable Diffusion template

[00:02:20](https://youtu.be/Tb4IYIYm4os?t=140) Which RunPod machine you should pick for DreamBooth training and why

[00:02:48](https://youtu.be/Tb4IYIYm4os?t=168) The used versions in this experiment such as Automatic1111 version, xformers version, DreamBooth version

[00:04:20](https://youtu.be/Tb4IYIYm4os?t=260) Best DreamBooth settings for 0 classification images

[00:04:45](https://youtu.be/Tb4IYIYm4os?t=285) How to continue DreamBooth training from a certain checkpoint

[00:05:12](https://youtu.be/Tb4IYIYm4os?t=312) Used command line arguments for best DreamBooth training

[00:05:20](https://youtu.be/Tb4IYIYm4os?t=320) Used extensions list for best DreamBooth training

[00:05:45](https://youtu.be/Tb4IYIYm4os?t=345) Starting to set parameters for 0 classification images - equal to fine tuning

[00:06:45](https://youtu.be/Tb4IYIYm4os?t=405) Used training dataset and what dataset features you need

[00:07:45](https://youtu.be/Tb4IYIYm4os?t=465) Setting concepts tab of DreamBooth training

[00:08:00](https://youtu.be/Tb4IYIYm4os?t=480) When you should use FileWords and why you should use for fine tuning and how to do fine tuning

[00:10:15](https://youtu.be/Tb4IYIYm4os?t=615) Best training setup parameters for DreamBooth training when using classification images

[00:11:28](https://youtu.be/Tb4IYIYm4os?t=688) How to calculate number of steps for each epoch

[00:13:17](https://youtu.be/Tb4IYIYm4os?t=797) All trainings are completed

[00:13:49](https://youtu.be/Tb4IYIYm4os?t=829) Comparison of sample and sanity sample images generated during training

[00:13:55](https://youtu.be/Tb4IYIYm4os?t=835) Analysis of 0x classification samples

[00:14:41](https://youtu.be/Tb4IYIYm4os?t=881) Analysis of 1x classification samples

[00:15:14](https://youtu.be/Tb4IYIYm4os?t=914) Analysis of 2x classification samples

[00:15:36](https://youtu.be/Tb4IYIYm4os?t=936) Analysis of 5x classification samples

[00:16:12](https://youtu.be/Tb4IYIYm4os?t=972) Analysis of 10x classification samples

[00:16:34](https://youtu.be/Tb4IYIYm4os?t=994) Analysis of 25x classification samples

[00:16:45](https://youtu.be/Tb4IYIYm4os?t=1005) Analysis of 50x classification samples

[00:17:28](https://youtu.be/Tb4IYIYm4os?t=1048) Analysis of 100x classification samples

[00:17:49](https://youtu.be/Tb4IYIYm4os?t=1069) Analysis of 100x classification samples

[00:18:09](https://youtu.be/Tb4IYIYm4os?t=1089) Comparing each checkpoint in all of the trained models

[00:18:46](https://youtu.be/Tb4IYIYm4os?t=1126) How to use x/y/z plot to check different training checkpoints

[00:19:51](https://youtu.be/Tb4IYIYm4os?t=1191) All grids are generated and how did i download them

[00:20:40](https://youtu.be/Tb4IYIYm4os?t=1240) Analysis of 0x classification x/y/z grid images

[00:21:58](https://youtu.be/Tb4IYIYm4os?t=1318) Analysis of 1x classification x/y/z grid images

[00:23:10](https://youtu.be/Tb4IYIYm4os?t=1390) Analysis of 2x classification x/y/z grid images

[00:24:03](https://youtu.be/Tb4IYIYm4os?t=1443) Analysis of 5x classification x/y/z grid images

[00:25:00](https://youtu.be/Tb4IYIYm4os?t=1500) Analysis of 10x classification x/y/z grid images

[00:25:36](https://youtu.be/Tb4IYIYm4os?t=1536) Analysis of 25x classification x/y/z grid images

[00:26:15](https://youtu.be/Tb4IYIYm4os?t=1575) Analysis of 50x classification x/y/z grid images

[00:27:27](https://youtu.be/Tb4IYIYm4os?t=1647) Analysis of 100x classification x/y/z grid images

[00:28:02](https://youtu.be/Tb4IYIYm4os?t=1682) Analysis of 100x classification x/y/z grid images

[00:29:00](https://youtu.be/Tb4IYIYm4os?t=1740) Summary of the experiment

[00:29:40](https://youtu.be/Tb4IYIYm4os?t=1780) Very important speech part

Text-Guided View Synthesis

Our technique can synthesized images with specified viewpoints for a subject cat (left to right: top, bottom, side and back views). Note that the generated poses are different from the input poses, and the background changes in a realistic manner given a pose change. We also highlight the preservation of complex fur patterns on the subject cat's forehead.

Property Modification

We show color modifications in the first row (using prompts ``a [color] [V] car''), and crosses between a specific dog and different animals in the second row (using prompts ``a cross of a [V] dog and a [target species]''). We highlight the fact that our method preserves unique visual features that give the subject its identity or essence, while performing the required property modification.

Accessorization

Outfitting a dog with accessories. The identity of the subject is preserved and many different outfits or accessories can be applied to the dog given a prompt of type "a [V] dog wearing a police/chef/witch outfit''. We observe a realistic interaction between the subject dog and the outfits or accessories, as well as a large variety of possible options.



### Video Transcription


- [00:00:00](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=0) Greetings everyone.

- [00:00:01](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1) In this video I am going to conduct a massive experiment of number of classification images

- [00:00:06](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=6) effect when doing Stable Diffusion DreamBooth training.

- [00:00:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=9) In the community: there are widely varying numbers for how many classification images

- [00:00:13](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=13) to use per training instance image.

- [00:00:15](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=15) In the official paper 200 classification regularization images are used per training image.

- [00:00:20](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=20) So in this video I am going to conduct the experiments written here.

- [00:00:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=23) I will use 9 RunPod instances to do 9 different DreamBooth training.

- [00:00:28](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=28) So all of my instances are running right now.

- [00:00:30](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=30) I have prepared one instance, then cloned all of them between.

- [00:00:34](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=34) To do this I have used runpodctl command.

- [00:00:37](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=37) I have zipped the Stable Diffusion web UI folder, venv folder and the classification

- [00:00:41](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=41) images folder, then sent them between the different RunPods.

- [00:00:46](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=46) Moreover when you start your RunPod with the template, it starts a hidden web UI instance

- [00:00:53](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=53) but you are not able to close it from the jupyter interface.

- [00:00:57](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=57) So for closing it first I have changed the relauncher.py like this.

- [00:01:03](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=63) So I have added a while loop break here.

- [00:01:06](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=66) When it is closed it won't relaunch again and again.

- [00:01:10](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=70) And to close the initial hidden web UI I have used this kill command.

- [00:01:15](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=75) With this command, you are killing the running web UI instance on the port 3000.

- [00:01:21](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=81) Then we will manually launch these web UIs.

- [00:01:26](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=86) So in my My Pods section currently you see: CPU utilization and memory usage are zero.

- [00:01:32](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=92) That means that no instance of Automatic1111 web UI currently running on your pod.

- [00:01:37](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=97) So if you don't know what is Stable Diffusion, what is Automatic1111 web UI I have excellent

- [00:01:42](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=102) tutorials for them.

- [00:01:43](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=103) In this tutorial you can learn how to install and run Automatic1111 web UI on your computer.

- [00:01:49](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=109) In this tutorial, you will learn how to do DreamBooth training from zero to hero.

- [00:01:54](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=114) In this tutorial I am explaining what is new awesome fantastic ControlNet and how to use

- [00:01:59](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=119) it on Automatic1111 web UI.

- [00:02:02](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=122) And this is the ultimate RunPod tutorial.

- [00:02:05](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=125) So if you are interested in this, you can watch this playlist.

- [00:02:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=129) Now I will begin with starting my web UI instances in all of the RunPods.

- [00:02:15](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=135) All Automatic1111 instances are started.

- [00:02:18](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=138) Let me show you the versions and the pods that I am using.

- [00:02:21](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=141) So it is really important to pick your pod correct for DreamBooth training.

- [00:02:26](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=146) I have chosen RTX A4500 pods.

- [00:02:30](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=150) Why?

- [00:02:31](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=151) Because as you can see, these pods have 62GB RAM and 20GB VRAM.

- [00:02:38](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=158) So having more RAM is really important when doing DreamBooth training.

- [00:02:42](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=162) If your pods do not have enough sufficient amount of RAM then you may get gradio killed

- [00:02:49](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=169) error which is extremely annoying.

- [00:02:51](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=171) So the versions I am using are python revision 3.10.9 for this experiment.

- [00:02:57](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=177) DreamBooth revision is this one and the SD Web UI revision is this one.

- [00:03:02](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=182) I am using xformers 0.0.17.dev464.

- [00:03:08](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=188) Why?

- [00:03:10](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=190) Because either you have to use 0.0.14 or 0.0.17 version xformers, otherwise, DreamBooth training

- [00:03:18](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=198) will not work.

- [00:03:20](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=200) This is a very commonly question that I have been getting asked.

- [00:03:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=203) On Windows you should downgrade your xformers to 0.0.14 revision and I am explaining that

- [00:03:30](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=210) in this video.

- [00:03:32](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=212) On Unix you should upgrade your xformers to 0.0.17 development revision and in this video

- [00:03:38](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=218) I am explaining that.

- [00:03:40](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=220) I have pre-prepared 2400 classification images.

- [00:03:45](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=225) To generate these images a simple prompt used which is our classification regularization

- [00:03:51](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=231) prompt.

- [00:03:52](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=232) The prompt used is photo of man and I have used sampling steps as 40 and nothing else

- [00:03:57](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=237) is different.

- [00:03:58](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=238) I have used version 1.5 pruned ckpt file and in the settings and in the Stable Diffusion

- [00:04:05](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=245) settings the default vae used as the newest vae the best vae available as you can see

- [00:04:11](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=251) right now here.

- [00:04:13](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=253) So I will share the link of this classification data set in the description as a zip file.

- [00:04:18](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=258) You can download and use them if you want.

- [00:04:21](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=261) Now I will show you two of the settings.

- [00:04:24](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=264) First one is zero classification and the second one will be 1x classification.

- [00:04:29](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=269) The rest will be same.

- [00:04:30](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=270) First we will begin with generating our training model.

- [00:04:35](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=275) This one will be 0x plus.

- [00:04:38](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=278) I will pick the file from here 1.5 pruned ckpt file as a source checkpoint.

- [00:04:45](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=285) Sometimes I am getting asked that how you can continue your training from certain checkpoint.

- [00:04:51](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=291) Just make a new model and pick your source checkpoint from here.

- [00:04:57](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=297) Like this and it will generate a new training model from that certain checkpoint that you

- [00:05:02](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=302) want to continue.

- [00:05:03](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=303) It is the same thing and I am not touching the other things.

- [00:05:06](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=306) They are default and best settings.

- [00:05:08](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=308) One another thing that I want to mention is that I have only used these command line arguments

- [00:05:14](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=314) as you can see nothing else is special or different and these are the only extensions

- [00:05:19](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=319) I am using right now they are the latest version.

- [00:05:22](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=322) Okay all model files are generated in all of the RunPod instances.

- [00:05:28](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=328) For example, this one is 0x classification images.

- [00:05:30](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=330) This one is 1x classification images 2x, 5x, 10x, 25x, 50x, 100x, and 200x.

- [00:05:41](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=341) So I will show two of the setups first.

- [00:05:44](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=344) Let's begin with no classification images.

- [00:05:47](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=347) This will be basically a fine tuning.

- [00:05:49](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=349) First I will click performance wizard.

- [00:05:51](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=351) Then I'm not going to use any classification images.

- [00:05:53](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=353) I am going to train 200 epochs.

- [00:05:56](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=356) I will save model every 25 epochs.

- [00:05:58](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=358) I will save preview image every five epochs.

- [00:06:00](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=360) I'm not going to change batch size or gradient accumulation steps.

- [00:06:03](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=363) These will affect your training success rate as well.

- [00:06:08](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=368) Because this is mini batches versus higher batches like full batches.

- [00:06:12](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=372) This is a debated topic in the machine learning.

- [00:06:15](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=375) I will use gradient checkpointing.

- [00:06:17](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=377) Why?

- [00:06:18](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=378) Because in this video, I am going to set up the settings as you can do in your computer

- [00:06:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=383) with only 12 gigabyte vram having graphic card.

- [00:06:27](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=387) Therefore, I will use gradient checkpointing.

- [00:06:30](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=390) The learning rate I see that the learning rate is increased when we click performance

- [00:06:35](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=395) wizard or by default I am going to use a lower learning rate like this.

- [00:06:39](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=399) I'm not going to use center crop or apply horizontal flip.

- [00:06:43](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=403) My images are already prepared by me.

- [00:06:46](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=406) This is my training data set.

- [00:06:47](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=407) You see every background is different.

- [00:06:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=410) Clothings are different.

- [00:06:51](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=411) Only face is common in the images so you should make common only the things that you want

- [00:06:57](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=417) to teach to the model.

- [00:06:58](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=418) The sanity sample prompt will be photo of ohwx man by tomer hanuka.

- [00:07:05](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=425) So the ohwx is our rare token.

- [00:07:08](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=428) Man is our class token.

- [00:07:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=429) However, this is zero classification model so we are not going to have any class token

- [00:07:15](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=435) in this particular one.

- [00:07:17](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=437) So for zero classification images, it will be only photo of ohwx by tomer hanuka to see

- [00:07:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=443) how it performs during training.

- [00:07:25](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=445) I'm not going to use ema because this is as I said, 12 gigabyte VRAM having experiment.

- [00:07:32](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=452) I am going to use bf16.

- [00:07:34](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=454) This is supposed to have better precision, but if your graphic card is not supporting

- [00:07:38](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=458) this then you should pick fp16.

- [00:07:40](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=460) We are using xformers and these are the versions I am showing once again.

- [00:07:45](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=465) The other values are will be like this.

- [00:07:47](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=467) In the concepts I will set the data set directory like this.

- [00:07:52](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=472) This is the data set directory that is containing these training images.

- [00:07:56](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=476) We are not setting any classification because this is 0x class.

- [00:08:00](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=480) I'm not using any [FileWords] because [FileWords] is something that you want to use when you

- [00:08:06](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=486) want to do fine tuning.

- [00:08:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=489) That you want to improve quality of lots of tokens like you are teaching castles, rivers,

- [00:08:15](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=495) mountains, and other things and you have beautiful images, then you should caption those images

- [00:08:20](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=500) with the keywords that you want to improve and associate with them.

- [00:08:24](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=504) If I want to show you an example, let's say you want to improve castle images and you

- [00:08:30](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=510) have this image for fine tuning, then you should caption this file as awesome fantastic

- [00:08:36](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=516) castle in a beautiful forest with a awesome river.

- [00:08:41](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=521) And when doing DreamBooth training, all of these tokens will get associated with this

- [00:08:47](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=527) image and they will get improved along with the unet and the text encoder.

- [00:08:51](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=531) This is when it is useful to use [FileWords].

- [00:08:54](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=534) So when you use [FileWords] like this, it will read the caption of that particular training

- [00:09:00](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=540) image and replace here with that caption.

- [00:09:04](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=544) So your instance prompt for that image will become the caption that you have used.

- [00:09:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=549) But for teaching faces I am just only using a rare token and I am not using any captions

- [00:09:14](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=554) or other things, we are not going to use class prompt for this training.

- [00:09:19](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=559) For sample, I will use photo of ohwx and I am not adding class token and you see class

- [00:09:25](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=565) images per instance and other settings.

- [00:09:28](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=568) I think we need to set this one as zero for it to work correctly and in the saving I will

- [00:09:34](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=574) save them into a sub directory.

- [00:09:36](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=576) I will also generate a ckpt file during saving.

- [00:09:39](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=579) So these are my saving settings.

- [00:09:42](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=582) So all settings are ready.

- [00:09:43](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=583) I will click save settings and hit train.

- [00:09:46](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=586) Now in the terminal window we can see the training has started.

- [00:09:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=590) You see number of batches each epoch is 12 because we are not using any classification

- [00:09:56](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=596) regularization images.

- [00:09:57](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=597) Number of epochs 200, text encoder epochs is 150, because we did set ratio of text encoder

- [00:10:05](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=605) training is 75 percent.

- [00:10:07](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=607) The other settings are displayed here.

- [00:10:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=609) It is pretty fast on this graphic card as you can see the loss is looking good.

- [00:10:14](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=614) The used vram is 9.6 gigabytes.

- [00:10:17](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=617) Now I will set up the next one which is 1x classification images.

- [00:10:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=623) This one is simply same.

- [00:10:25](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=625) What is different in this one.

- [00:10:27](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=627) In this one the only different thing is I am adding the class prompt into the sample

- [00:10:33](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=633) prompt and also I am adding classification data set directory like this as you can see

- [00:10:39](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=639) and in the instance prompt I am going to use ohwx man.

- [00:10:42](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=642) Ohwx is our rare token.

- [00:10:44](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=644) Man is our class token and the prompt that I have used to generate classification regularization

- [00:10:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=650) images is photo of man.

- [00:10:53](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=653) So you should write same thing because the reason is that we are wanting to keep the

- [00:10:58](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=658) underlying context of the model as much as possibly same.

- [00:11:02](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=662) We want to have prior-preservation loss, so the sample image prompt will be photo of ohwx

- [00:11:10](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=670) man like this and I will use one class images per instance I am just setting it like this

- [00:11:18](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=678) and since it has already classification images, it won't generate any new one.

- [00:11:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=683) Save settings.

- [00:11:24](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=684) Hit train!

- [00:11:25](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=685) Okay, this is the terminal of 1x classification images.

- [00:11:28](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=688) Now number of batches each epoch is 24 because now it is using bucketing system.

- [00:11:34](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=694) Therefore, it adds in each epoch, classification images and the training images as well.

- [00:11:40](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=700) So it is a double size of the training images that you have.

- [00:11:43](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=703) The other settings are like this, same as the previous one and it started training.

- [00:11:48](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=708) We can see here.

- [00:11:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=710) The rest of the training will be same.

- [00:11:53](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=713) Only the number of classification images per instance will be different.

- [00:11:57](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=717) Okay, all trainings are started.

- [00:11:59](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=719) You see class images 24 used.

- [00:12:02](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=722) This is 2x.

- [00:12:03](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=723) You see class images 60 this is 5x. 120 class images this is 10x. 300 class images this

- [00:12:10](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=730) is 25x. 600 this is 50x.

- [00:12:14](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=734) This is 100x 1200 class images and 2400 class images this is the 200x.

- [00:12:21](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=741) So each one is going on.

- [00:12:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=743) I see that the stylizing capability of the 0x is gone already and in this one the stylizing

- [00:12:31](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=751) in 1x class also gone in epochs 74 in 2x class I see that still stylizing in epoch 39.

- [00:12:40](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=760) In 5x ues, stylizing is still available but learn rate is not very good yet 38 epochs.

- [00:12:47](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=767) This one is just started 10x class.

- [00:12:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=770) Uh, it had error so I had to restart.

- [00:12:53](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=773) Okay with 25x class 36 epochs it looks like the best.

- [00:12:57](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=777) And with 50x class I see that the stylizing is the best so far.

- [00:13:02](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=782) It is really good with 100 class stylizing is good with 200 class stylizing is not very

- [00:13:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=789) good but this is also the epoch 24 yet.

- [00:13:12](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=792) So when you use more classification images, can we say that it is taking more time?

- [00:13:17](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=797) I'm not sure yet.

- [00:13:19](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=799) Okay, now we need to wait all of them to finish and we will then compare.

- [00:13:24](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=804) Okay, all trainings are completed.

- [00:13:26](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=806) Let me show you quickly you see model epoch is 200 200 200 and each one is different.

- [00:13:34](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=814) You see 5x classifications 200.

- [00:13:36](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=816) So all of the trainings are completed without any errors now I am going to download samples

- [00:13:44](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=824) generated during training in all of the RunPods and we will start with comparing them.

- [00:13:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=830) I will analyze and comment on them.

- [00:13:53](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=833) Okay, samples are downloaded here.

- [00:13:55](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=835) So I will begin with the 0x classification images.

- [00:13:59](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=839) When we do not use any classification images, the generated samples are like this: okay,

- [00:14:05](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=845) we see that a good styling at 960 steps here and the face is looking decent as well.

- [00:14:13](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=853) The styling capability of the model is lost after 1260 steps.

- [00:14:19](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=859) So how are we going to calculate the epoch count for this?

- [00:14:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=863) When you divide it by 12, we are going to find the epoch count.

- [00:14:28](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=868) So this is 105 epoch.

- [00:14:31](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=871) So until 100 epochs.

- [00:14:33](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=873) It was able to stylize when we do not use any classification images.

- [00:14:37](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=877) So we can say that with 80 epochs we could get good results when we do not use any classification

- [00:14:42](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=882) images, we will test that.

- [00:14:45](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=885) Let's look at the results.

- [00:14:46](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=886) When we use only single classification image, we see that it started learning pretty fast.

- [00:14:53](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=893) This is a really good styling.

- [00:14:54](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=894) However, the styling capability of the model is lost pretty quickly after 60 epochs and

- [00:15:01](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=901) the generated samples quality is also decreased a lot.

- [00:15:04](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=904) It also looks like memorized the images instead of learning my face.

- [00:15:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=909) So 1x classification doesn't look very good.

- [00:15:13](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=913) We will see results.

- [00:15:14](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=914) Okay, this is 2x classification.

- [00:15:16](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=916) 2x means that we have used 2 multiplied by 12 24 classification images just reminding.

- [00:15:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=923) It is looking decent for 40 epochs and after 60 epochs it becomes very bad actually.

- [00:15:30](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=930) It also looks like memorized the images not learned the face so we will see the results.

- [00:15:36](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=936) Okay, this is 5x classification images and after 60 epochs or 50 epochs it looks like

- [00:15:44](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=944) decent and it loses its styling pretty early also, after 100 epochs.

- [00:15:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=950) Okay, these are the sample images.

- [00:15:52](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=952) By the way, these images are not styled are not beautified.

- [00:15:56](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=956) They are just raw prompts.

- [00:15:58](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=958) Let me show you.

- [00:15:59](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=959) So you see, this is a raw prompt photo of ohwx man and this is the raw sanity prompt

- [00:16:04](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=964) photo of ohwx man by tomer hanuka.

- [00:16:06](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=966) We could add more beautifying tokens to these prompts and also negative prompts to improve

- [00:16:13](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=973) them.

- [00:16:14](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=974) Okay, let's see 10x classification.

- [00:16:16](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=976) In the 10x classification it keeps its styling ability much more as you can see even at the

- [00:16:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=983) 190 epochs, it is still able to stylize my face with tomer hanuka style.

- [00:16:28](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=988) So you see as we increase the number of classification images, we are preventing over training certainly.

- [00:16:33](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=993) Okay, this is 25 classification images and somehow the styling capability is once again

- [00:16:41](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1001) lost quickly after 60 epochs.

- [00:16:44](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1004) Okay, 50x classification images.

- [00:16:46](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1006) When we use 50 classification images per instance, I see that it is the best one that keeps styling.

- [00:16:56](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1016) Also looks like learning the face.

- [00:16:58](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1018) So in this experiment I find that 50x is the sweet spot for my training data set.

- [00:17:06](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1026) I can't say it will be for you, but 50x looks like a good choice for now.

- [00:17:10](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1030) We will test and see it.

- [00:17:12](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1032) Started to learn the face slower than the others.

- [00:17:15](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1035) I think after 60 epochs it becomes somewhat decent and the best one looking like 180 epochs

- [00:17:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1043) like this.

- [00:17:24](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1044) This one also looking decent.

- [00:17:26](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1046) Let's look at the 100 classification images.

- [00:17:28](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1048) Okay, in the 100 classification images, the results are not fascinating.

- [00:17:33](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1053) Actually, the sanity prompt becomes very very irritating.

- [00:17:37](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1057) Also, the sample prompts as well.

- [00:17:39](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1059) I don't know why it is like this, but maybe there is a bug in the code that causes some

- [00:17:45](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1065) errors.

- [00:17:46](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1066) Problems?

- [00:17:47](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1067) it doesn't look good at all.

- [00:17:49](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1069) 200 classification images is same.

- [00:17:52](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1072) After 150 epochs it becomes very bad.

- [00:17:55](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1075) Actually very very bad.

- [00:17:56](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1076) I don't know.

- [00:17:57](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1077) This is very weird.

- [00:17:59](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1079) The prompts are looking correct so this is weird.

- [00:18:02](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1082) I also checked the settings and verified the settings are correct and yes they are correct.

- [00:18:07](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1087) It certainly tried to learn the face, but the results are not very good.

- [00:18:11](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1091) So now what are we going to do is I have prepared a prompt and found a seed that shows my face

- [00:18:19](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1099) in seven of the eight generated images.

- [00:18:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1103) They are all my face.

- [00:18:24](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1104) They are stylized as you can see.

- [00:18:27](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1107) So the aim here is comparing each checkpoint and see how each model will perform.

- [00:18:34](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1114) And how am I going to do that?

- [00:18:35](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1115) I will copy the prompt in the each model.

- [00:18:39](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1119) Then I will copy the seed of this prompt.

- [00:18:43](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1123) Like this.

- [00:18:44](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1124) I will set the batch size as eight.

- [00:18:46](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1126) This is important.

- [00:18:48](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1128) Are we done?

- [00:18:49](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1129) no.

- [00:18:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1130) We are also going to use x/y/z plot and in x/y/z plot we are going to test different

- [00:18:56](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1136) checkpoints.

- [00:18:57](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1137) You see I have checked checkpoint name here.

- [00:18:59](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1139) I click this.

- [00:19:00](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1140) It will fill the checkpoints like this.

- [00:19:02](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1142) I will delete the first one and I am going to test all of the checkpoints.

- [00:19:07](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1147) So how many images this will generate?

- [00:19:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1149) This will generate eight multiplied by eight 64 images.

- [00:19:13](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1153) Because we have eight checkpoints, this is 20 epoch, 40 epoch, 60 epoch this is 25 epoch

- [00:19:21](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1161) 50 epoch 75 epoch 100, epoch 125 epoch 150 epoch 175 epochs and this is 200 epochs.

- [00:19:32](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1172) I am not going to test other values because I want to see what it will generate for the

- [00:19:38](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1178) same settings in the different trained models.

- [00:19:42](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1182) By the way, this is the first zero classification example.

- [00:19:45](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1185) So we don't need man in this one, but in others, we will use man the class prompt and we are

- [00:19:51](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1191) ready to test.

- [00:19:52](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1192) Okay, all grids are generated.

- [00:19:54](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1194) However, they are not displayed on the gradio unfortunately.

- [00:19:58](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1198) So what did I do?

- [00:19:59](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1199) I went to the text to image grids folder and downloaded all of the images one by one and

- [00:20:05](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1205) now they are ready and now time to compare them.

- [00:20:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1209) But before doing that, I will now close my pods since we are done with the pods and time

- [00:20:14](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1214) to evaluate the results.

- [00:20:16](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1216) You see currently, I am using 3.37 dollars per hour time to close them.

- [00:20:22](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1222) After closing all the pods I am using 0.25 dollars per hour because currently I am using

- [00:20:29](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1229) 900 gigabytes volume exited volume you see exited.

- [00:20:35](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1235) Therefore, it is spending 0.25 dollars per hour from my credits.

- [00:20:39](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1239) Okay, let's begin with 0x classification results.

- [00:20:43](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1243) In the 0x classifications there is one thing that you need to be careful.

- [00:20:48](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1248) It starts with 300 steps because we didn't use any classification images.

- [00:20:53](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1253) Therefore, each epoch equals to 12 steps.

- [00:20:58](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1258) The number of training images I have.

- [00:21:00](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1260) When we use classification images they are also included in the bucket.

- [00:21:04](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1264) Therefore, it will be double of this size.

- [00:21:07](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1267) So this is 25 epoch.

- [00:21:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1269) These are the results of 25 epoch, not very like my face.

- [00:21:13](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1273) These are the results of 50 epoch a very low similarity.

- [00:21:17](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1277) These are the results of 75 epoch and yes this one is becoming more similar.

- [00:21:24](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1284) Okay, this one is 100 epochs.

- [00:21:26](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1286) Yes, I see similarity in this one especially.

- [00:21:29](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1289) These are the results of 125 epochs.

- [00:21:33](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1293) As you can see the similarity increases.

- [00:21:35](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1295) However, I wouldn't call them very good results and after 150 epochs it starts to lose stylizing

- [00:21:42](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1302) and also my face.

- [00:21:44](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1304) And these are the results of 175 epochs and these are the results of 200 epochs.

- [00:21:51](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1311) So the best spot for 0x classification is 100 epochs.

- [00:21:56](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1316) The 1200 steps.

- [00:21:58](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1318) Let's look at the 1x classification results.

- [00:22:01](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1321) You see as I said, the 1x classification starts from 600 steps because classification images

- [00:22:07](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1327) are also included in the bucket.

- [00:22:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1329) Therefore, one epoch is 24 steps.

- [00:22:12](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1332) So let me show you each one of them.

- [00:22:15](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1335) This is 50 epochs.

- [00:22:16](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1336) The I see the similarity here but not very similar also.

- [00:22:21](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1341) These are the 75 epochs and the styling ability is becoming lesser as you can see.

- [00:22:27](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1347) Okay, this is the 100 epoch.

- [00:22:29](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1349) As you can see the results are not very good.

- [00:22:33](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1353) These are the 125 epoch.

- [00:22:35](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1355) Even though I said close shot.

- [00:22:37](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1357) You see they are all distant shots.

- [00:22:39](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1359) These are 150 epochs.

- [00:22:41](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1361) These are 175 epochs.

- [00:22:43](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1363) And this is the 200 epochs.

- [00:22:45](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1365) You see it is very much over trained and the quality is very bad as you can see.

- [00:22:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1370) So the best epoch for 1x classification is.

- [00:22:54](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1374) We can say 25 epoch.

- [00:22:56](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1376) After 25 epoch the results are not better.

- [00:23:01](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1381) So when you use 1x classification, 25 epochs is the sweet spot for my training data set.

- [00:23:07](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1387) It may change for you.

- [00:23:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1389) Okay now time to analyze 2x classification images.

- [00:23:13](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1393) In the 25 epoch these are the results as you can see.

- [00:23:16](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1396) This is not at all my face or the other one.

- [00:23:20](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1400) This is the 50 epoch and now it starts to resemble my face much better.

- [00:23:24](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1404) It is stylizing but not the best results.

- [00:23:27](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1407) So this is the 75 epoch and I can see my face in here, but the styling is lost pretty quickly.

- [00:23:35](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1415) This is 100 epochs as you can see.

- [00:23:38](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1418) It almost lost all of its stylizing capability and after 100 epochs you see it is just simply

- [00:23:45](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1425) printing my face without following our prompt.

- [00:23:49](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1429) Also it starts over training.

- [00:23:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1430) Yes, as you can see it is very much over trained at this point.

- [00:23:55](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1435) So the sweet spot for 2x classification looks like 50 epoch as you can see.

- [00:24:00](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1440) Generating a lot of images you may get what you want, but this is still not very good.

- [00:24:04](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1444) Okay now we are at the 5x classification.

- [00:24:07](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1447) In the first image none of the images are like me.

- [00:24:10](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1450) This is the 25 epoch.

- [00:24:12](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1452) This is the 50 epoch and some resemblance starts.

- [00:24:16](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1456) And this is the 75 epoch.

- [00:24:18](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1458) You see as we increase number of classification images, it takes more time to learn our face.

- [00:24:24](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1464) However, it is also able to stylize better.

- [00:24:28](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1468) This is the 75 epoch quality.

- [00:24:30](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1470) This is 100 epoch quality.

- [00:24:33](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1473) This is 125 steps.

- [00:24:34](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1474) Okay, this is 150 epochs as you can see.

- [00:24:38](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1478) Only this one is actually in armor.

- [00:24:41](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1481) So it's already over trained a lot.

- [00:24:44](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1484) So for the 5x classification, the sweet spot we can say is 75 epochs.

- [00:24:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1490) It is almost fully stylized.

- [00:24:52](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1492) All of the images are stylized, and all of the images are similar to me.

- [00:24:57](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1497) So therefore this is the sweet spot.

- [00:25:00](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1500) Okay, now time to see 10x classification.

- [00:25:02](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1502) In the first image the resemblance is good.

- [00:25:05](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1505) This is only 25 epoch.

- [00:25:08](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1508) This is 50 epoch.

- [00:25:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1509) It is stylized but not very much following our prompt.

- [00:25:12](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1512) This is 75 epoch.

- [00:25:15](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1515) Still stylized but not very much like we are targeting.

- [00:25:21](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1521) This is 100 epochs.

- [00:25:23](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1523) This is 125 epochs.

- [00:25:25](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1525) I think it is starting to over training.

- [00:25:27](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1527) This is 150 epochs as you can see the quality is decreased.

- [00:25:31](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1531) There are some problems errors.

- [00:25:34](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1534) This is 175 epoch and this is 200 epoch.

- [00:25:37](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1537) Very bad quality.

- [00:25:38](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1538) Okay, this is 25 classification images.

- [00:25:42](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1542) The first one is not at all like me.

- [00:25:44](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1544) This is 25 epoch.

- [00:25:46](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1546) This is 50 epoch.

- [00:25:47](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1547) It is stylized but not very similar to me.

- [00:25:49](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1549) This one looks like me.

- [00:25:51](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1551) But not very good.

- [00:25:52](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1552) This is 75 epoch.

- [00:25:53](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1553) I can say this is better than 50 epoch and this is 100 epoch.

- [00:25:59](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1559) It already looks like over trained.

- [00:26:02](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1562) Some major problems in the images and this is 125 epoch.

- [00:26:07](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1567) Already very much over trained and the rest is also.

- [00:26:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1569) You see it has memorized it, even the elevator or the backgrounds.

- [00:26:14](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1574) Okay, this is 60 classification images.

- [00:26:17](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1577) In the first image I can see the resemblance.

- [00:26:19](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1579) Some very good styling as well.

- [00:26:21](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1581) This is the 50 epoch and the results are really good actually if you ask me, with 50 epoch

- [00:26:28](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1588) and 50 classification images probably I can get whatever I want in a stylized manner.

- [00:26:34](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1594) The distance shot is also decent.

- [00:26:36](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1596) When you want to have distance shot then you need to upscale this image maybe with high

- [00:26:43](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1603) res fix and you can then in paint your face.

- [00:26:46](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1606) Then you can obtain very good images with that approach.

- [00:26:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1610) So this is uh so this is 75 epoch, still very much stylized.

- [00:26:55](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1615) I can see a very decent quality.

- [00:26:58](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1618) You see this looks pretty good one.

- [00:27:00](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1620) This is 100 epochs.

- [00:27:02](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1622) It is starting to lose styling capability.

- [00:27:05](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1625) This is 125 epochs and I can see it is memorized and producing bad quality images.

- [00:27:13](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1633) After 125 it starts over training so you can alternatively reduce the training speed by

- [00:27:18](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1638) halving it and it may help you to maybe obtain better ones.

- [00:27:24](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1644) And this is very bad.

- [00:27:25](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1645) You see totally over trained.

- [00:27:26](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1646) Okay, now time to see 100 classification images.

- [00:27:30](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1650) In the first one, there is almost no resembling.

- [00:27:33](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1653) This is the 25 epoch.

- [00:27:34](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1654) This is 50 epoch and I can see resemblance.

- [00:27:37](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1657) Actually, these results are also pretty decent for 50 epochs, but this is not like me.

- [00:27:42](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1662) In the 75 epoch, we see these are the generated images.

- [00:27:46](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1666) Yes, it is stylizing, but it is not very well and this is 100 epochs and the quality is

- [00:27:53](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1673) very bad and these are the rest.

- [00:27:56](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1676) It starts over training after 100 epochs, even for 100 classification images.

- [00:28:01](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1681) Okay, now we are at the 200 classification images.

- [00:28:05](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1685) In the 25 epoch version, there is some resemblance, but not very good.

- [00:28:10](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1690) In the 50 epoch, there are some more resembles but not very good either.

- [00:28:15](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1695) By the way, even though we are using same seed, that doesn't mean they are equal between

- [00:28:21](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1701) different trainings.

- [00:28:22](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1702) The same seed should produce similar results in the same training.

- [00:28:26](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1706) But in the different training, then we can say it will be same.

- [00:28:30](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1710) Same seed basically means that it will start from the same noise and then generate the

- [00:28:35](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1715) image with denoising.

- [00:28:36](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1716) Okay, this is 75 epochs.

- [00:28:39](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1719) The results are decent.

- [00:28:41](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1721) For 75.

- [00:28:42](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1722) Actually, I see stylizing some good decent results.

- [00:28:46](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1726) For 200 images, this is 100 epoch.

- [00:28:49](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1729) As you can see, uh, the styling ability is starting to lose once again and this is 125

- [00:28:55](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1735) epoch.

- [00:28:56](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1736) Okay, this is 150 epoch and it is already over trained.

- [00:29:00](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1740) So what is the the summary of this experiment?

- [00:29:05](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1745) As you increase the number of classification images, it doesn't mean you will get better

- [00:29:10](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1750) results.

- [00:29:11](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1751) From this experiment I can say that 50 classification images yielded best results for me.

- [00:29:17](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1757) That is my sweet spot for 12 training images.

- [00:29:21](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1761) I can't say it will be same for you, but 50 images looks like a sweet spot.

- [00:29:26](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1766) Also, you should take more checkpoints and compare them as I did and find your best checkpoint.

- [00:29:32](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1772) This is really important because difference between different checkpoints are huge.

- [00:29:38](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1778) You should find the best checkpoint that will work best for you.

- [00:29:42](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1782) Okay, this is all for today.

- [00:29:43](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1783) I hope you have enjoyed.

- [00:29:44](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1784) Please like, subscribe, leave a comment.

- [00:29:47](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1787) I will put the used prompt in the comment section.

- [00:29:50](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1790) Please also support us on patreon.

- [00:29:52](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1792) This is really important.

- [00:29:54](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1794) The patreon link will be in the description and also in the comment section.

- [00:29:58](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1798) You see so far we have 25 patrons.

- [00:30:00](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1800) I am hoping that you will also become our patreon.

- [00:30:03](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1803) I am hoping that you can also be for sharing, liking, making a comment and becoming our

- [00:30:09](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1809) patron.

- [00:30:10](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1810) Also, you can make a comment and tell me what you want to see next.

- [00:30:14](https://www.youtube.com/watch?v=Tb4IYIYm4os&t=1814) Hopefully see you in better more awesome videos.
