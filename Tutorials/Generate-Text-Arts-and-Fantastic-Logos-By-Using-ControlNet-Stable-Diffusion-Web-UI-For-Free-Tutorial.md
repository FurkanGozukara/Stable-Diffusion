# Generate Text Arts & Fantastic Logos By Using ControlNet Stable Diffusion Web UI For Free   Tutorial

## Full tutorial link > https://www.youtube.com/watch?v=C_mJI4U23nQ

[![Generate Text Arts & Fantastic Logos By Using ControlNet Stable Diffusion Web UI For Free   Tutorial](https://img.youtube.com/vi/C_mJI4U23nQ/sddefault.jpg)](https://www.youtube.com/watch?v=C_mJI4U23nQ "Generate Text Arts & Fantastic Logos By Using ControlNet Stable Diffusion Web UI For Free   Tutorial")

[![image](https://img.shields.io/discord/772774097734074388?label=Discord&logo=discord)](https://discord.com/servers/software-engineering-courses-secourses-772774097734074388) [![Hits](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/Generate-Text-Arts-and-Fantastic-Logos-By-Using-ControlNet-Stable-Diffusion-Web-UI-For-Free-Tutorial.md.svg?style=plastic&label=Hits%20Since%2025.08.27&labelColor=007ec6&logo=SECourses)](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/Generate-Text-Arts-and-Fantastic-Logos-By-Using-ControlNet-Stable-Diffusion-Web-UI-For-Free-Tutorial.md)
[![Patreon](https://img.shields.io/badge/Patreon-Support%20Me-F2EB0E?style=for-the-badge&logo=patreon)](https://www.patreon.com/c/SECourses) [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&logoColor=black)](https://www.buymeacoffee.com/DrFurkan) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/Medium-Follow%20Me-800080?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@furkangozukara) [![Codio](https://img.shields.io/static/v1?style=for-the-badge&message=Articles&color=4574E0&logo=Codio&logoColor=FFFFFF&label=CivitAI)](https://civitai.com/user/SECourses/articles) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/DeviantArt-Follow%20Me-990000?style=for-the-badge&logo=deviantart&logoColor=white)](https://www.deviantart.com/monstermmorpg)

[![YouTube Channel](https://img.shields.io/badge/YouTube-SECourses-C50C0C?style=for-the-badge&logo=youtube)](https://www.youtube.com/SECourses)  [![Furkan G√∂z√ºkara LinkedIn](https://img.shields.io/badge/LinkedIn-Follow%20Me-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/furkangozukara/)   [![Udemy](https://img.shields.io/static/v1?style=for-the-badge&message=Stable%20Diffusion%20Course&color=A435F0&logo=Udemy&logoColor=FFFFFF&label=Udemy)](https://www.udemy.com/course/stable-diffusion-dreambooth-lora-zero-to-hero/?referralCode=E327407C9BDF0CEA8156) [![Twitter Follow Furkan G√∂z√ºkara](https://img.shields.io/badge/Twitter-Follow%20Me-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/GozukaraFurkan)


Discord : [https://bit.ly/SECoursesDiscord.](https://bit.ly/SECoursesDiscord.) How to use #ControlNet to generate cinematic text arts and logos for free. If I have been of assistance to you and you would like to show your support for my work, please consider becoming a patron on ü•∞ [https://www.patreon.com/SECourses](https://www.patreon.com/SECourses)

Playlist of #StableDiffusion Tutorials, #Automatic1111 and Google Colab Guides, DreamBooth, Textual Inversion / Embedding, LoRA, AI Upscaling, Pix2Pix, Img2Img:

[https://www.youtube.com/playlist?list=PL_pbwdIyffsmclLl0O144nQRnezKlNdx3](https://www.youtube.com/playlist?list=PL_pbwdIyffsmclLl0O144nQRnezKlNdx3)

Easiest Way to Install & Run Stable Diffusion Web UI on PC by Using Open Source Automatic Installer:

[https://youtu.be/AZg6vzWHOTA](https://youtu.be/AZg6vzWHOTA)

How to use Stable Diffusion V2.1 and Different Models in the Web UI - SD 1.5 vs 2.1 vs Anything V3:

[https://youtu.be/aAyvsX-EpG4](https://youtu.be/aAyvsX-EpG4)

Sketches into Epic Art with 1 Click: A Guide to Stable Diffusion ControlNet in Automatic1111 Web UI:

[https://youtu.be/vhqqmkTBMlU](https://youtu.be/vhqqmkTBMlU)

New Style Transfer Extension, ControlNet of Automatic1111 Stable Diffusion T2I-Adapter Color Control:

[https://youtu.be/tXaQAkOgezQ](https://youtu.be/tXaQAkOgezQ)

Free photoshop like website: [https://www.photopea.com/](https://www.photopea.com/)

AI Upscalers dataset: [https://upscale.wiki/wiki/Model_Database](https://upscale.wiki/wiki/Model_Database)

Prompts (used model deliberate):

3d burning lava fire text

3D cinematic lightning blue shiny diamond text

3D cinematic lightning shiny metallic text

3D cinematic lightning amazing texture having text

3D cinematic lightning shiny metallic text

3D cinematic lightning shiny neon text

[00:00:00](https://youtu.be/C_mJI4U23nQ?t=0) How to generate cinematic text for free

[00:00:50](https://youtu.be/C_mJI4U23nQ?t=50) How to update Automatic1111 Web UI extensions

[00:01:05](https://youtu.be/C_mJI4U23nQ?t=65) How to enable Multi ControlNet and how to speed up ControlNet

[00:01:40](https://youtu.be/C_mJI4U23nQ?t=100) How to run Automatic1111 Web UI and ControlNet on weak GPU such as 4GB cards

[00:01:55](https://youtu.be/C_mJI4U23nQ?t=115) What are the key points of generating beautiful text artworks with ControlNet

[00:02:03](https://youtu.be/C_mJI4U23nQ?t=123) How to generate template base text image easily

[00:02:34](https://youtu.be/C_mJI4U23nQ?t=154) How to generate clean background having epic text via ControlNet

[00:03:28](https://youtu.be/C_mJI4U23nQ?t=208) Which ControlNet models are working best for txt2img tab cinematic text artwork generation

[00:04:44](https://youtu.be/C_mJI4U23nQ?t=284) Second part - How to use img2img tab when generating epic text

[00:05:22](https://youtu.be/C_mJI4U23nQ?t=322) Best ControlNet models when using img2img tab

[00:06:11](https://youtu.be/C_mJI4U23nQ?t=371) How to generate epic logo by using Stable Diffusion Web UI ControlNet

[00:08:20](https://youtu.be/C_mJI4U23nQ?t=500) Few of the best AI upscalers examples comparison

[00:09:14](https://youtu.be/C_mJI4U23nQ?t=554) Where to download and how to use different AI upscales for free

[00:09:45](https://youtu.be/C_mJI4U23nQ?t=585) Dramatic different effect of using higher target resolution

Generating cinematic text is a great way to enhance the visual appeal of any artwork or video project. With the help of Artificial Intelligence (AI), it is now possible to generate stunning text effects quickly and easily. In this article, we will cover everything you need to know about generating cinematic text with ControlNet, a state-of-the-art AI text generator, and how to use it for free.

Generating Cinematic Text for Free

ControlNet is a powerful AI-based text generator that can be used to generate cinematic text for free. The best part about using ControlNet is that you do not need any special skills or knowledge to use it. All you need is an internet connection, and you can access ControlNet through a web browser.

Updating Automatic1111 Web UI Extensions

Before you start using ControlNet, it is essential to update Automatic1111 Web UI extensions. This will ensure that you have access to all the latest features and improvements, making your experience with ControlNet more seamless.

Enabling Multi ControlNet and Speeding Up ControlNet

Enabling Multi ControlNet and speeding up ControlNet can significantly improve the speed and efficiency of generating cinematic text. Multi ControlNet allows you to use multiple GPUs simultaneously, increasing the speed of the text generation process. Additionally, optimizing the ControlNet settings can speed up the process even further.

Running Automatic1111 Web UI and ControlNet on Weak GPU

If you have a weak GPU, such as a 4GB card, you can still run Automatic1111 Web UI and ControlNet. However, it is recommended that you optimize the ControlNet settings to ensure that the process runs smoothly.

Key Points for Generating Beautiful Text Artworks with ControlNet

Generating beautiful text artworks with ControlNet is relatively simple. The key is to start with a high-quality template base text image and then generate a clean background with epic text using ControlNet. Additionally, using the right ControlNet models can significantly improve the quality of the generated text.

Generating Template-Based Text Image Easily

To generate a template-based text image, you first need to select a high-quality image and add text to it using a tool such as Adobe Photoshop. Once you have your template image, you can use ControlNet to generate additional text effects.

Generating Clean Background with Epic Text via ControlNet

Generating a clean background with epic text via ControlNet is a straightforward process. First, you need to select a background image and then use ControlNet to generate text effects on top of it.



### Video Transcription


- [00:00:00](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=0) Greetings everyone.

- [00:00:01](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=1) In this video, I will show you how you can generate epic text by using ControlNet.

- [00:00:07](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=7) There are several key issues when using ControlNet to generate text or logos for your companies,

- [00:00:14](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=14) for your job, for your professional business, and I will explain all of them.

- [00:00:18](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=18) In this video, I will not show how to install ControlNet or Automatic1111 Web UI because

- [00:00:23](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=23) I already have excellent tutorials for these.

- [00:00:27](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=27) For example, in this tutorial, I have explained how to install and run Automatic1111 Web UI.

- [00:00:33](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=33) In this tutorial, I have explained how to install and use ControlNet.

- [00:00:38](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=38) And this is my newest tutorial where I explained how to install and use ControlNet from scratch.

- [00:00:44](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=44) The only thing that I want to mention is that currently, extension update mechanism of the

- [00:00:49](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=49) Automatic1111 Web UI is broken.

- [00:00:51](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=51) Therefore, update your extension through git pull.

- [00:00:55](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=55) So enter inside the folder of your extension, then type git pull.

- [00:00:59](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=59) That's it.

- [00:01:00](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=60) So to achieve good high quality text, we are going to use Multi ControlNet.

- [00:01:05](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=65) To enable Multi ControlNet, go to the Settings tab in your Automatic1111 Web UI.

- [00:01:10](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=70) Go to the ControlNet tab in here.

- [00:01:12](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=72) Set Multi ControlNet max models amount like 4. And model cache size.

- [00:01:18](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=78) This model cache size will speed up your image generation whenever you generate new images.

- [00:01:24](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=84) This is really really important.

- [00:01:25](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=85) If you set this one and if you use 2 ControlNet, each time it will load models.

- [00:01:30](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=90) Therefore, it will slow you down significantly.

- [00:01:33](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=93) However, if you get out of memory error, of course, you need to reduce this to 1.

- [00:01:37](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=97) And one final thing, if you are still getting out of memory error, start your Automatic1111

- [00:01:43](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=103) Web UI with ‚Äìmedvram command line argument.

- [00:01:46](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=106) This will significantly lower the required VRAM amount.

- [00:01:50](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=110) However, it will slow you down significantly as well.

- [00:01:53](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=113) So what is the key point of generating epic text with ControlNet?

- [00:01:57](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=117) First of all, you need to generate your text template, the base image.

- [00:02:02](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=122) For this, I have used photopea.com.

- [00:02:04](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=124) This is a free Photoshop like service that you can use.

- [00:02:07](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=127) So I have generated a very simple image like this.

- [00:02:10](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=130) White background and over it, I have a black text.

- [00:02:13](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=133) Once you generated it, just click Export as PNG.

- [00:02:16](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=136) Then in our Web UI, go to the Text to Image tab, define your prompt.

- [00:02:21](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=141) This is really important because what you are going to get will be 100% based on your

- [00:02:26](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=146) prompt and the base model that you are using.

- [00:02:29](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=149) So if you use Text to Image tab, you are going to get clean background having text like this.

- [00:02:34](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=154) As you can see, this is a pretty simple prompt and this is the output of it.

- [00:02:38](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=158) So in the bottom, you will see ControlNet in here.

- [00:02:41](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=161) As a control model 0, load the image from here.

- [00:02:45](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=165) Let me demonstrate it again like this.

- [00:02:47](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=167) And as a preprocessor, I am picking the first preprocessor as Canny and I am selecting the

- [00:02:52](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=172) model as Canny from here.

- [00:02:54](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=174) Click Enable.

- [00:02:55](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=175) Annotator resolution is also important.

- [00:02:57](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=177) If your input image is over 512, then you should also increase this resolution according

- [00:03:04](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=184) to that.

- [00:03:05](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=185) And when you click Preview Annotator Result, it will display you this Annotator Result here.

- [00:03:09](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=189) So you can see how it is affecting.

- [00:03:12](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=192) And if you get out of memory error, you can check this low VRAM. In the control model 1.

- [00:03:17](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=197) Because we are going to use two control models to achieve this high success rate.

- [00:03:22](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=202) Again, we are uploading our same base image.

- [00:03:25](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=205) Then for this one, I am selecting Scribble.

- [00:03:28](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=208) When you are using Text-to-Image tab to generate such text images, Scribble and plus Canny

- [00:03:34](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=214) is working best.

- [00:03:35](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=215) So select your preprocessor as Scribble.

- [00:03:38](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=218) Select your model as Scribble.

- [00:03:40](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=220) Make sure that you have right Annotator Resolution.

- [00:03:43](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=223) You can also click Preview Annotator Result to see the preview and you are ready.

- [00:03:47](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=227) Then just hit Generate button and you will get the images that you want to and then you

- [00:03:53](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=233) will start getting images.

- [00:03:54](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=234) Of course, as in the nature of Stable Diffusion, you have to generate hundreds of images or

- [00:03:59](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=239) try different prompts to get what you want.

- [00:04:01](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=241) For example, I have changed blue to gold and these are the results I got.

- [00:04:06](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=246) These are not cherry-picked.

- [00:04:07](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=247) These are the first results I have got.

- [00:04:10](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=250) You see, with just changing prompts, you can get awesome different results and you may

- [00:04:15](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=255) get the best one you are looking for.

- [00:04:17](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=257) And these are the maps used.

- [00:04:19](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=259) Using two ControlNets is the key here.

- [00:04:22](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=262) Otherwise, you are not going to get this quality of text.

- [00:04:25](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=265) And the very nice thing of this is that you can use any resolution.

- [00:04:30](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=270) So if you make your base image 1024, just change the output to 1024 and you are going

- [00:04:35](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=275) to get awesome 1024 images.

- [00:04:38](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=278) Also, you can play with CFG value to see what kind of results you are getting.

- [00:04:42](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=282) Now the second part.

- [00:04:44](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=284) First, generate your background image with a certain prompt you like.

- [00:04:48](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=288) For example, I have generated this image by using epic cinematic blue, space station,

- [00:04:53](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=293) masterpiece, art station.

- [00:04:55](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=295) This time we are going to use image to image tab.

- [00:04:57](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=297) This is important.

- [00:04:59](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=299) Upload your base image.

- [00:05:00](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=300) So here my base image, just drag and drop.

- [00:05:02](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=302) Then set your denoising strength.

- [00:05:05](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=305) This is really important.

- [00:05:06](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=306) This is actually the most important thing that will determine your output.

- [00:05:10](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=310) If you increase this too much, then you are going to get a very different background.

- [00:05:14](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=314) It will completely become almost the text.

- [00:05:17](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=317) If you make this very low, then you are not going to get good text.

- [00:05:21](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=321) I will demonstrate that.

- [00:05:22](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=322) And this time we are going to use again, canny preprocessor and canny model in the first control

- [00:05:28](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=328) net model.

- [00:05:29](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=329) But in the second control net model, this time we are going to use hed preprocessor

- [00:05:33](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=333) and hed model.

- [00:05:34](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=334) This is working better than scribble when you are generating your text over an image.

- [00:05:40](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=340) When you are using image to image tab.

- [00:05:42](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=342) This is really important.

- [00:05:44](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=344) So for example, let's use the same seed and change the denoising strength to see what

- [00:05:49](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=349) kind of effect we are going to get.

- [00:05:51](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=351) Okay, on the left we see 65% denoising strength and on the right we see 90%.

- [00:05:57](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=357) So when you increase your denoising strength, then the parts where your map is applied becomes

- [00:06:04](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=364) much more simpler.

- [00:06:06](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=366) Like in the first case, because our map is overriding the original image.

- [00:06:11](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=371) And as a third, I will show you how you can generate fantastic logos by using the same

- [00:06:17](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=377) principle.

- [00:06:18](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=378) For example, this is the base logo of C sharp.

- [00:06:21](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=381) I have downloaded this, added a black background to it.

- [00:06:25](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=385) Then I have used photo P to write a simple text on it like this.

- [00:06:31](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=391) Then again, in image to image tab, I have used a base image like this.

- [00:06:35](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=395) This is the prompt that I have used to generate this base image.

- [00:06:38](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=398) And in the bottom I have uploaded my target image.

- [00:06:41](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=401) Then I have chosen preprocessor as canny model as canny.

- [00:06:45](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=405) I did set my annotator resolution same as my input image.

- [00:06:49](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=409) And again, I am using hed model as hed and the annotator resolution is set like this.

- [00:06:53](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=413) Then this time I have used 75 denoising strength.

- [00:06:57](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=417) And for example, this is one of the images I have got.

- [00:07:00](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=420) So it is trying to use the colors context in here and generating the output image in

- [00:07:07](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=427) here.

- [00:07:08](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=428) By using this strategy, you can turn any logo into an epic artwork.

- [00:07:13](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=433) For example, I have generated the thumbnail image of our discord server channel by using

- [00:07:18](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=438) this same strategy.

- [00:07:20](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=440) You see it is keeping the C sharp logo as in here.

- [00:07:23](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=443) But it is generating a perfect artwork by using our prompt and the control net inputs

- [00:07:30](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=450) and the base image to image.

- [00:07:31](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=451) By following this strategy, you can generate awesome fantastic images.

- [00:07:36](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=456) There are limitless number of things that you can generate.

- [00:07:39](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=459) For example, these are the results of another batch generation.

- [00:07:43](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=463) These are not cherry-picked.

- [00:07:45](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=465) I am able to generate batch size as 8 and cache 3 models on the RAM.

- [00:07:51](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=471) And my graphic card is only RTX 3060 which has 12 GB VRAM.

- [00:07:57](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=477) So it is working very well.

- [00:07:59](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=479) And you see during the batch generation, it was using almost all of them.

- [00:08:02](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=482) But I have generated 8 images at once.

- [00:08:05](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=485) And these are the results of the generation.

- [00:08:07](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=487) All I need to do is generate more images and get the best one that I like.

- [00:08:11](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=491) And once you get the image that you are looking for, you can upscale it.

- [00:08:16](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=496) For upscaling, I will compare several of them and show you now.

- [00:08:20](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=500) To upscale this image, I will click send to extras.

- [00:08:23](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=503) Also, you can upload it in here.

- [00:08:25](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=505) For example, let me show.

- [00:08:27](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=507) Let's upload this one.

- [00:08:28](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=508) It is the same principle.

- [00:08:29](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=509) And I find that 4X UltraSharp is really good for upscaling images.

- [00:08:34](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=514) Okay, this is the upscaled version of the image with 4X UltraSharp.

- [00:08:38](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=518) And this is the original image.

- [00:08:40](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=520) Let's compare them.

- [00:08:41](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=521) Here is the comparison.

- [00:08:42](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=522) This is the base image.

- [00:08:43](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=523) And this is the upscaled version.

- [00:08:45](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=525) Alternatively, of course, you can always generate 1024x1024.

- [00:08:50](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=530) That takes more time, but you may get better results.

- [00:08:53](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=533) And let me compare it with other upscalers that I have.

- [00:08:57](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=537) So these are the upscalers I have tested so far.

- [00:08:59](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=539) Let's try with Lollypop.

- [00:09:01](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=541) Okay, here we see the result of Lollypop on the right and the 4X UltraSharp on the left.

- [00:09:07](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=547) So it is up to you to choose which one that you like.

- [00:09:11](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=551) And you may wonder where you can download these models.

- [00:09:14](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=554) There is a model database wiki for upscalers.

- [00:09:18](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=558) This is a link of it.

- [00:09:19](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=559) From here, just download them and put them inside the target folder based on their architecture.

- [00:09:25](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=565) For example, you see these are based on ESRGAN.

- [00:09:28](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=568) Therefore, you need to put them inside this folder.

- [00:09:32](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=572) Inside your Automatic1111 installation, inside StableDiffusion WebUI models, ESRGAN.

- [00:09:36](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=576) You see.

- [00:09:38](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=578) So when you put them inside here and after you restart your WebUI, you will be able to

- [00:09:43](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=583) use these upscaler models.

- [00:09:45](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=585) And one other thing is when you change the output resolution bigger than the original

- [00:09:50](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=590) resolution, you are going to get a very different image.

- [00:09:54](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=594) You see, currently I am using denoising strength as 1.

- [00:09:58](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=598) 100%.

- [00:09:59](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=599) Let's make the width and height the double of the original resolution.

- [00:10:03](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=603) 1024, 1024.

- [00:10:04](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=604) And let's hit generate.

- [00:10:07](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=607) And this is the output we get with 1024.

- [00:10:09](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=609) It is completely different output.

- [00:10:12](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=612) So it is up to you which one you prefer.

- [00:10:15](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=615) If you keep the original resolution, you are going to get images like this.

- [00:10:19](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=619) But if you prefer a bigger resolution, you are going to get images like this.

- [00:10:23](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=623) Actually this one looks fantastic.

- [00:10:25](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=625) But it is like only keeping the shallow structure of our input images.

- [00:10:30](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=630) So it is 100% stylized.

- [00:10:32](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=632) It looks epic, but not as clear as this one.

- [00:10:38](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=638) So it is up to you to get whichever you want.

- [00:10:41](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=641) This is all for today.

- [00:10:42](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=642) Please like, subscribe, join and support us on Patreon if possible.

- [00:10:47](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=647) You will see the link of our Discord in the description.

- [00:10:51](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=651) Also in the comment section of the video.

- [00:10:53](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=653) You will also find our Patreon link there as well.

- [00:10:56](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=656) Thank you very much for watching.

- [00:10:57](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=657) Leaving a comment, liking, sharing.

- [00:11:00](https://www.youtube.com/watch?v=C_mJI4U23nQ&t=660) Hopefully see you in another awesome video.
