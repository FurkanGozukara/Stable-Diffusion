# How To Do Stable Diffusion Textual Inversion (TI) / Text Embeddings By Automatic1111 Web UI Tutorial

## Full tutorial link > https://www.youtube.com/watch?v=dNOpWt-epdQ

[![How To Do Stable Diffusion Textual Inversion (TI) / Text Embeddings By Automatic1111 Web UI Tutorial](https://img.youtube.com/vi/dNOpWt-epdQ/sddefault.jpg)](https://www.youtube.com/watch?v=dNOpWt-epdQ "How To Do Stable Diffusion Textual Inversion (TI) / Text Embeddings By Automatic1111 Web UI Tutorial")

[![image](https://img.shields.io/discord/772774097734074388?label=Discord&logo=discord)](https://discord.com/servers/software-engineering-courses-secourses-772774097734074388) [![Hits](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/How-To-Do-Stable-Diffusion-Textual-Inversion-TI-Text-Embeddings-By-Automatic1111-Web-UI-Tutorial.md.svg?style=plastic&label=Hits%20Since%2025.08.27&labelColor=007ec6&logo=SECourses)](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/How-To-Do-Stable-Diffusion-Textual-Inversion-TI-Text-Embeddings-By-Automatic1111-Web-UI-Tutorial.md)
[![Patreon](https://img.shields.io/badge/Patreon-Support%20Me-F2EB0E?style=for-the-badge&logo=patreon)](https://www.patreon.com/c/SECourses) [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&logoColor=black)](https://www.buymeacoffee.com/DrFurkan) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/Medium-Follow%20Me-800080?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@furkangozukara) [![Codio](https://img.shields.io/static/v1?style=for-the-badge&message=Articles&color=4574E0&logo=Codio&logoColor=FFFFFF&label=CivitAI)](https://civitai.com/user/SECourses/articles) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/DeviantArt-Follow%20Me-990000?style=for-the-badge&logo=deviantart&logoColor=white)](https://www.deviantart.com/monstermmorpg)

[![YouTube Channel](https://img.shields.io/badge/YouTube-SECourses-C50C0C?style=for-the-badge&logo=youtube)](https://www.youtube.com/SECourses)  [![Furkan G√∂z√ºkara LinkedIn](https://img.shields.io/badge/LinkedIn-Follow%20Me-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/furkangozukara/)   [![Udemy](https://img.shields.io/static/v1?style=for-the-badge&message=Stable%20Diffusion%20Course&color=A435F0&logo=Udemy&logoColor=FFFFFF&label=Udemy)](https://www.udemy.com/course/stable-diffusion-dreambooth-lora-zero-to-hero/?referralCode=E327407C9BDF0CEA8156) [![Twitter Follow Furkan G√∂z√ºkara](https://img.shields.io/badge/Twitter-Follow%20Me-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/GozukaraFurkan)


Our Discord : [https://discord.gg/HbqgGaZVmr.](https://discord.gg/HbqgGaZVmr.) Grand Master tutorial for Textual Inversion / Text Embeddings. If I have been of assistance to you and you would like to show your support for my work, please consider becoming a patron on ü•∞ [https://www.patreon.com/SECourses](https://www.patreon.com/SECourses)

Playlist of Stable Diffusion Tutorials, Automatic1111 and Google Colab Guides, DreamBooth, Textual Inversion / Embedding, LoRA, AI Upscaling, Pix2Pix, Img2Img:

[https://www.youtube.com/playlist?list=PL_pbwdIyffsmclLl0O144nQRnezKlNdx3](https://www.youtube.com/playlist?list=PL_pbwdIyffsmclLl0O144nQRnezKlNdx3)

In this video, I am explaining almost every aspect of  Stable Diffusion Textual Inversion (TI) / Text Embeddings. I am demonstrating a live example of how to train a person face with all of the best settings including technical details.

TI Academic Paper: [https://arxiv.org/pdf/2208.01618.pdf](https://arxiv.org/pdf/2208.01618.pdf)

Automatic1111 Repo: [https://github.com/AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)

Easiest Way to Install & Run Stable Diffusion Web UI on PC

[https://youtu.be/AZg6vzWHOTA](https://youtu.be/AZg6vzWHOTA)

How to use Stable Diffusion V2.1 and Different Models in the Web UI

[https://youtu.be/aAyvsX-EpG4](https://youtu.be/aAyvsX-EpG4)

Automatic1111 Used Commit : d8f8bcb821fa62e943eb95ee05b8a949317326fe

Git Bash : [https://git-scm.com/downloads](https://git-scm.com/downloads)

Automatic1111 Command Line Arguments List: [https://bit.ly/StartArguments](https://bit.ly/StartArguments)

S.D. 1.5 CKPT: [https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main)

Latest Best S.D. VAE File: [https://huggingface.co/stabilityai/sd-vae-ft-mse-original/tree/main](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/tree/main)

VAE File Explanation: [https://bit.ly/WhatIsVAE](https://bit.ly/WhatIsVAE)

Cross attention optimizations bug: [https://bit.ly/CrosOptBug](https://bit.ly/CrosOptBug)

Vector Pull Request: [https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/6667](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/6667)

All of the tokens list in Stable Diffusion: [https://huggingface.co/openai/clip-vit-large-patch14/tree/main](https://huggingface.co/openai/clip-vit-large-patch14/tree/main)

Example training dataset used in the video:

[https://drive.google.com/file/d/1Hom2XbILub0hQc-zmLizRcwFrKwHYGcc/view?usp=sharing](https://drive.google.com/file/d/1Hom2XbILub0hQc-zmLizRcwFrKwHYGcc/view?usp=sharing)

Inspect-Embedding-Training Script repo:

[https://github.com/Zyin055/Inspect-Embedding-Training](https://github.com/Zyin055/Inspect-Embedding-Training)

How to Inject Your Trained Subject: [https://youtu.be/s25hcW4zq4M](https://youtu.be/s25hcW4zq4M)

Comparison of training techniques: [https://bit.ly/TechnicComparison](https://bit.ly/TechnicComparison)

Embedding file name list generator script:

[https://jsfiddle.net/MonsterMMORPG/Lg0swc1b/10/](https://jsfiddle.net/MonsterMMORPG/Lg0swc1b/10/)

[00:00:00](https://youtu.be/dNOpWt-epdQ?t=0) Introduction to #StableDiffusion #TextualInversion Embeddings

[00:01:00](https://youtu.be/dNOpWt-epdQ?t=60) Which commit of the #Automatic1111 Web UI we are using and how to checkout / switch to specific commit of any Git project

[00:04:07](https://youtu.be/dNOpWt-epdQ?t=247) Used command line arguments of Automatic1111 webui-user.bat file

[00:04:35](https://youtu.be/dNOpWt-epdQ?t=275) Automatic1111 command line arguments

[00:05:31](https://youtu.be/dNOpWt-epdQ?t=331) How to and where to put Stable Diffusion models and VAE files in Automatic1111 installation

[00:06:05](https://youtu.be/dNOpWt-epdQ?t=365) Why do we use latest VAE file and what does VAE file do

[00:08:24](https://youtu.be/dNOpWt-epdQ?t=504) Training settings of Automatic1111

[00:10:38](https://youtu.be/dNOpWt-epdQ?t=638) All about names of text embeddings

[00:11:00](https://youtu.be/dNOpWt-epdQ?t=660) What is initialization text of textual inversion training

[00:11:32](https://youtu.be/dNOpWt-epdQ?t=692) Embedding inspector extension of Automatic1111

[00:14:25](https://youtu.be/dNOpWt-epdQ?t=865) How to set number of vectors per token when doing Textual Inversion training

[00:11:52](https://youtu.be/dNOpWt-epdQ?t=712) Technical and detailed explanation of tokens and their numerical weights vectors in Stable Diffusion

[00:16:00](https://youtu.be/dNOpWt-epdQ?t=960) How the prompts getting tokenized - turned into tokens - by using tokenizer extension

[00:18:58](https://youtu.be/dNOpWt-epdQ?t=1138) Setting number of training vectors

[00:20:24](https://youtu.be/dNOpWt-epdQ?t=1224) Where embedding files are saved in automatic1111 installation

[00:20:38](https://youtu.be/dNOpWt-epdQ?t=1238) All about preprocess images before TI training

[00:23:06](https://youtu.be/dNOpWt-epdQ?t=1386) Training tab of textual inversion

[00:23:18](https://youtu.be/dNOpWt-epdQ?t=1398) What to and how to set embedding learning rate

[00:23:40](https://youtu.be/dNOpWt-epdQ?t=1420) What are the Batch size and Gradient accumulation steps and how to set them

[00:24:40](https://youtu.be/dNOpWt-epdQ?t=1480) How to set training learning rate according to Batch size and Gradient accumulation steps

[00:26:21](https://youtu.be/dNOpWt-epdQ?t=1581) What are prompt templates, what are they used for, how to set and use them in textual inversion training

[00:29:06](https://youtu.be/dNOpWt-epdQ?t=1746) What are filewords and how they are used in training in automatic1111 web ui

[00:29:35](https://youtu.be/dNOpWt-epdQ?t=1775) How to edit image captions when doing textual inversion training

[00:31:07](https://youtu.be/dNOpWt-epdQ?t=1867) From training images pool, how and why did i choose some of them and not all of them

[00:31:54](https://youtu.be/dNOpWt-epdQ?t=1914) Why I did add noise to the backgrounds of some training dataset images

[00:32:07](https://youtu.be/dNOpWt-epdQ?t=1927) How should be your training dataset. What is a good training dataset

[00:34:48](https://youtu.be/dNOpWt-epdQ?t=2088) Save TI training checkpoints

[00:36:31](https://youtu.be/dNOpWt-epdQ?t=2191) Which latent sampling method is best

[00:39:59](https://youtu.be/dNOpWt-epdQ?t=2399) Training started

[00:38:08](https://youtu.be/dNOpWt-epdQ?t=2288) Overclock GPU to get 10% training speed up

[00:38:32](https://youtu.be/dNOpWt-epdQ?t=2312) Where to find TI training preview images

[00:39:15](https://youtu.be/dNOpWt-epdQ?t=2355) Where to see used final prompts during training

[00:41:34](https://youtu.be/dNOpWt-epdQ?t=2494) How to use inspect_embedding_training script to determine overtraining of textual inversion

[00:42:31](https://youtu.be/dNOpWt-epdQ?t=2551) What is training loss

[00:48:23](https://youtu.be/dNOpWt-epdQ?t=2903) Technical difference of Textual Inversion, DreamBooth, LoRA and HyperNetworks training

[00:52:17](https://youtu.be/dNOpWt-epdQ?t=3137) Over 200 epochs and already got very good sample preview images

[00:54:28](https://youtu.be/dNOpWt-epdQ?t=3268) How to set newest VAE file as default in the settings of automatic1111 web ui

[00:55:06](https://youtu.be/dNOpWt-epdQ?t=3306) How to use generated embeddings checkpoint files

[00:58:31](https://youtu.be/dNOpWt-epdQ?t=3511) How to test different checkpoints via X/Y plot and embedding files name generator script

[01:07:27](https://youtu.be/dNOpWt-epdQ?t=4047) How to upscale image by using AI

[01:08:42](https://youtu.be/dNOpWt-epdQ?t=4122) How to use multiple embeddings in a prompt



### Video Transcription


- [00:00:01](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1) Greetings everyone. Welcome to the most&nbsp; comprehensive, technical, detailed and yet&nbsp;&nbsp;

- [00:00:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=7) still beginner-friendly Stable Diffusion Text&nbsp; Embeddings, also known as Textual Inversion&nbsp;&nbsp;

- [00:00:12](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=12) training tutorial. In this video I am going to&nbsp; cover all of the topics that you see here and&nbsp;&nbsp;

- [00:00:18](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=18) more. Currently I am hovering my mouse over there.&nbsp; You can pause the video and check them out if you&nbsp;&nbsp;

- [00:00:25](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=25) wish. Also, you see here the training dataset we&nbsp; used and here the textual embedding used results.&nbsp;&nbsp;

- [00:00:32](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=32) Let's start by quickly introducing what is textual&nbsp; inversion and its officially released academic&nbsp;&nbsp;

- [00:00:39](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=39) paper. If you are interested in reading this&nbsp; article, so you can open the link and read it,&nbsp;&nbsp;

- [00:00:47](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=47) I am also going to show some of the important&nbsp; parts of this article when we are going to use&nbsp;&nbsp;

- [00:00:56](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=56) them. I will explain through the article.&nbsp; So to do training, we are going to use&nbsp;&nbsp;

- [00:01:02](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=62) Automatic1111 web UI. If you don't know how to&nbsp; install and set up the Automatic1111 web UI,&nbsp;&nbsp;

- [00:01:10](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=70) I already have a video for that on my channel:&nbsp; Easiest Way to Install &amp; Run Stable Diffusion&nbsp;&nbsp;

- [00:01:16](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=76) Web UI. Also, I have another video How to use&nbsp; Stable Diffusion V2.1 and Different Models.&nbsp;&nbsp;

- [00:01:24](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=84) So I am going to use specific version of&nbsp; the Automatic1111 web UI. It is constantly&nbsp;&nbsp;

- [00:01:30](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=90) getting updated and therefore it is constantly&nbsp; getting broken and you are asking me that:&nbsp;&nbsp;

- [00:01:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=97) which version did you use? I am going to use this&nbsp; specific version, this commit, because after bump&nbsp;&nbsp;

- [00:01:44](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=104) gradio to 3.16, it has given me a lot of errors.&nbsp; So how am I going to use this specific version?&nbsp;&nbsp;

- [00:01:55](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=115) To use specific version, I am going to clone&nbsp; it with Git Bash. If you didn't still install&nbsp;&nbsp;

- [00:02:00](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=120) Git Bash, you can install it by using&nbsp; Google. Just type Git Bash to Google.&nbsp;&nbsp;

- [00:02:06](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=126) You can download from this website and&nbsp; install it. It is so easy to install.&nbsp;&nbsp;

- [00:02:10](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=130) First I am going to select the folder where I want&nbsp; to clone my Automatic1111 web UI. I am entering my&nbsp;&nbsp;

- [00:02:18](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=138) F drive and in here I am generating a new folder&nbsp; with right click new folder. Let's give it a name&nbsp;&nbsp;

- [00:02:25](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=145) as tutorial web UI. OK, then we will move inside&nbsp; this folder in our Git Bash window to do that.&nbsp;&nbsp;

- [00:02:36](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=156) Type CD F: Now we are in the F drive, then CD,&nbsp; put a quotation mark like this and hit enter.&nbsp;&nbsp;

- [00:02:45](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=165) Now we are inside this folder. Now we can clone&nbsp; Automatic1111 with git clone and copy the URL from&nbsp;&nbsp;

- [00:02:55](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=175) here like this and paste it into here. Right&nbsp; click, paste and it will clone it. OK, it is&nbsp;&nbsp;

- [00:03:03](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=183) cloned inside this folder. So I will enter there&nbsp; CD "s" tab and it will be automatically completed&nbsp;&nbsp;

- [00:03:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=191) like this and hit enter. Now we will check out to&nbsp; certain version from here. Let me show you again.&nbsp;&nbsp;

- [00:03:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=201) Click the commits from here, and here I am moving&nbsp; to the commit that I want: enable progress bar&nbsp;&nbsp;

- [00:03:27](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=207) without gallery. This is the commit ID. I will&nbsp; also put this into the description of the video.&nbsp;&nbsp;

- [00:03:32](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=212) Then we are going to do Git checkout like this.&nbsp; And right click paste. Now we are into that commit&nbsp;&nbsp;

- [00:03:44](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=224) and we are using the that specific version inside&nbsp; our folder. So before starting setup, I am copy&nbsp;&nbsp;

- [00:03:53](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=233) pasting this web UI user bat. But because I am&nbsp; going to add my command line arguments to here.&nbsp;&nbsp;

- [00:04:01](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=241) OK, right click the copy and edit and let me zoom&nbsp; in copy paste. So I am going to use xformers,&nbsp;&nbsp;

- [00:04:09](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=249) no-half and disable-safe-unpickle. So how did&nbsp; I come up with these command line arguments?&nbsp;&nbsp;

- [00:04:15](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=255) xformers going to increase your speed&nbsp; significantly and reduce the VRAM usage&nbsp;&nbsp;

- [00:04:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=261) of your graphic card. No-half is necessary for&nbsp; xformers to work correctly when you are using&nbsp;&nbsp;

- [00:04:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=269) SD 1.5 or 2.1 versions and disable-safe-unpickle.&nbsp; According to the web UI documentation you see the&nbsp;&nbsp;

- [00:04:38](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=278) URL here. Command line arguments and settings:&nbsp; disable checking pytorch models for malicious&nbsp;&nbsp;

- [00:04:44](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=284) code. Why I am using this? Because if you train&nbsp; your model on a Google Colab, sometimes it is not&nbsp;&nbsp;

- [00:04:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=290) working. It is not necessary, but I am just&nbsp; using it and I am not downloading any model&nbsp;&nbsp;

- [00:04:54](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=294) without knowing it. OK, then we save and run,&nbsp; then we are going to get our fresh installation.&nbsp;&nbsp;

- [00:05:04](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=304) OK, like this, it will install all necessary&nbsp; things. And you see, Let me zoom in It is&nbsp;&nbsp;

- [00:05:12](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=312) using Python 3.10.8 version. By the way, you&nbsp; have to have installed Python correctly for&nbsp;&nbsp;

- [00:05:23](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=323) this to install. It is also showing the&nbsp; commit hash that I am using like this.&nbsp;&nbsp;

- [00:05:30](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=330) I also need to put my Stable Diffusion models&nbsp; into the models folder. So let's open it. Open&nbsp;&nbsp;

- [00:05:36](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=336) the State Diffusion folder, copy paste from my&nbsp; previous download. And one another thing is I&nbsp;&nbsp;

- [00:05:43](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=343) am going to use the latest VAE file that I have&nbsp; downloaded from the Internet, which I am going to&nbsp;&nbsp;

- [00:05:49](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=349) show you right now. So where do we put this VAE&nbsp; file? Go to the stable diffusion web UI and in&nbsp;&nbsp;

- [00:05:54](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=354) here you will see VAE files folder. It is not&nbsp; generated. It is inside the models folder and&nbsp;&nbsp;

- [00:06:02](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=362) inside here VAE, and this is the VAE file. Why we&nbsp; are using this VAE file? Because it is improving&nbsp;&nbsp;

- [00:06:10](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=370) generation of person images. And now let me show&nbsp; the link. OK, this is the link of the VAE file.&nbsp;&nbsp;

- [00:06:17](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=377) This is the latest version of VAE file. Just click&nbsp; the CKPT file from here and click the download&nbsp;&nbsp;

- [00:06:24](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=384) button. I will also put the link of this into the&nbsp; description. So if you are wondering the technical&nbsp;&nbsp;

- [00:06:30](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=390) description, technical details of the VAE files,&nbsp; there is a long explanation in here in this&nbsp;&nbsp;

- [00:06:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=397) thread. I will also put the link of this thread&nbsp; into this description of the video and there is a&nbsp;&nbsp;

- [00:06:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=402) shorter description which I liked. Each generation&nbsp; is done in a compressed representation. The VAE&nbsp;&nbsp;

- [00:06:48](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=408) takes the compressed results and turn them into&nbsp; full sized images. SD comes with a VAE already,&nbsp;&nbsp;

- [00:06:54](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=414) but certain models may supply a custom VAE that&nbsp; works better for that model and SD 1.5 version&nbsp;&nbsp;

- [00:07:03](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=423) model is not using the latest VAE file. Therefore,&nbsp; we are downloading this and putting that into our&nbsp;&nbsp;

- [00:07:10](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=430) folder. SD 2.1 version is using the latest VAE&nbsp; file. And which SD 1.5 version model I am using?&nbsp;&nbsp;

- [00:07:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=440) I am using the 1.5 pruned ckpt. And where did&nbsp; I download it? I have downloaded it from this&nbsp;&nbsp;

- [00:07:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=449) URL and we are using the pruned ckpt because it&nbsp; is better for training than emaonly file which,&nbsp;&nbsp;

- [00:07:36](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=456) you see, is lesser in size. By the way, the&nbsp; things I am going to show in this video can&nbsp;&nbsp;

- [00:07:43](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=463) be applied to any model, such as Protogen&nbsp; or SD 2.1 version. Actually, I have made&nbsp;&nbsp;

- [00:07:51](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=471) experiments on Protogen training as well, and&nbsp; I will show the results of that too to you.&nbsp;&nbsp;

- [00:07:59](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=479) Okay, the fresh installation has been completed.&nbsp; No errors, and these are the messages displayed,&nbsp;&nbsp;

- [00:08:04](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=484) and it has started on this URL and I have&nbsp; already opened it. You can copy and paste&nbsp;&nbsp;

- [00:08:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=491) this URL in my browser. So currently it has&nbsp; selected by default the Protogen and I am going&nbsp;&nbsp;

- [00:08:18](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=498) to make this tutorial on version 1.5 pruned, the&nbsp; official version. Okay, before starting training,&nbsp;&nbsp;

- [00:08:26](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=506) I am going to first settings. First going to&nbsp; show you the settings that we need. Go to the&nbsp;&nbsp;

- [00:08:32](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=512) training tab in here and check this checkbox,&nbsp; Move VAE and CLIP to RAM when training. This&nbsp;&nbsp;

- [00:08:40](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=520) requires a lot of RAM actually. I have 64 GB and&nbsp; if you have checked this, it will reduce to VRAM,&nbsp;&nbsp;

- [00:08:48](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=528) which is the GPU RAM, which is our more&nbsp; limited RAM. Then you can also, on check this,&nbsp;&nbsp;

- [00:08:56](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=536) Turn on pin_memory for DataLoader. This makes&nbsp; training slightly faster, but increase memory&nbsp;&nbsp;

- [00:09:01](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=541) usage. I think this is increasing the RAM&nbsp; usage, not VRAM usage. So you can test this.&nbsp;&nbsp;

- [00:09:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=547) In other videos. You will see that. Check this&nbsp; checkbox. Use cross attention optimizations while&nbsp;&nbsp;

- [00:09:13](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=553) training. This will significantly increase your&nbsp; training speed and reduce the VRAM usage. However,&nbsp;&nbsp;

- [00:09:19](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=559) it is also significantly reducing your training&nbsp; success. So, if your graphic card can do training&nbsp;&nbsp;

- [00:09:27](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=567) without checking this out, do not check this,&nbsp; because it will reduce your training success and&nbsp;&nbsp;

- [00:09:33](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=573) it will reduce your learning rate. How do I know&nbsp; this? According to the vladmandic from Github,&nbsp;&nbsp;

- [00:09:41](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=581) this is causing a lot of problems. He&nbsp; has opened a bug topic on the Stable&nbsp;&nbsp;

- [00:09:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=590) Diffusion web UI issues and he says that this&nbsp; is causing a lot of problems. Let me show you.&nbsp;&nbsp;

- [00:09:59](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=599) He says that when he disabled cross attention&nbsp; for training and rerun exactly the same settings,&nbsp;&nbsp;

- [00:10:04](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=604) the results are perfect and I can verify this.&nbsp; So do not check this if your graphic card can&nbsp;&nbsp;

- [00:10:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=611) run it. There is also one more settings that we&nbsp; are going to set: Save an CSV containing the loss&nbsp;&nbsp;

- [00:10:18](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=618) to log directory every N steps. So I am going to&nbsp; make this 1. Why? Because I will show you how we&nbsp;&nbsp;

- [00:10:24](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=624) are going to use this during the training. Then&nbsp; go to the apply settings. Okay. Then reload UI.&nbsp;&nbsp;

- [00:10:31](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=631) Okay, settings were saved and UI is reloaded.&nbsp; Now go to the train tab. Okay. First of all,&nbsp;&nbsp;

- [00:10:39](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=639) we are going to give a name to our&nbsp; embedding. The name is not important at all,&nbsp;&nbsp;

- [00:10:46](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=646) so you can give it any name. This will&nbsp; be used to activate our embedding. Okay,&nbsp;&nbsp;

- [00:10:54](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=654) so I am going to give it a name as training&nbsp; example. It can be any character length,&nbsp;&nbsp;

- [00:11:00](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=660) It won't affect your results or token count.&nbsp; Initialization text. Now what does this mean?&nbsp;&nbsp;

- [00:11:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=667) For example, you are teaching a face and you&nbsp; want it to be similar to Brad Pitt. Then you&nbsp;&nbsp;

- [00:11:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=674) can type Brad Pitt. So what does this mean?&nbsp; Actually, to show you that first we are going&nbsp;&nbsp;

- [00:11:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=680) to install an extension, go to the available load&nbsp; from and in here, type embed into your search bar&nbsp;&nbsp;

- [00:11:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=689) and you will see embedding inspector. This is an&nbsp; extremely useful extension and let's install it.&nbsp;&nbsp;

- [00:11:39](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=699) Okay, the extension has been installed, so let's&nbsp; just restart with this. Okay, now we can see the&nbsp;&nbsp;

- [00:11:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=710) embedding inspector. So everything in the Stable&nbsp; Diffusion is composed by tokens. What does that&nbsp;&nbsp;

- [00:12:00](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=720) mean? You can think tokens as keywords, but not&nbsp; exactly like that. For example, when we type cat&nbsp;&nbsp;

- [00:12:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=727) and click inspect, the cat is a single token&nbsp; and it has an embedding ID and it has weights.&nbsp;&nbsp;

- [00:12:16](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=736) So every token has numerical weights, like&nbsp; this. And when we do training with embeddings,&nbsp;&nbsp;

- [00:12:27](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=747) actually we are going to generate a new vector&nbsp; that doesn't exist in the stable diffusion. We&nbsp;&nbsp;

- [00:12:34](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=754) are going to do training on that. So when you&nbsp; set initialization text like this, by the way,&nbsp;&nbsp;

- [00:12:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=762) it is going to generate a vector with the weights&nbsp; of this. However, this is a two token. How do I&nbsp;&nbsp;

- [00:12:51](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=771) know, go to go to the embedding inspector and&nbsp; type Brad. So you see, Brad is a single token.&nbsp;&nbsp;

- [00:12:57](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=777) It has weights. And let's type Pitt, and Pitt&nbsp; is also another token and it has also vector.&nbsp;&nbsp;

- [00:13:05](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=785) So these weights will be assigned initially to&nbsp; our new vector. However, we have to use at least&nbsp;&nbsp;

- [00:13:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=794) two vectors, otherwise we wouldn't be able to get&nbsp; two vectors. So if we start our training with Brad&nbsp;&nbsp;

- [00:13:24](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=804) Pitt, our first initial weights will be according&nbsp; to the Brad Pitt and our model will learn upon&nbsp;&nbsp;

- [00:13:32](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=812) that. Is this good? If your face is very similar&nbsp; to Brad Pitt, yes, but if it is not, no. So&nbsp;&nbsp;

- [00:13:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=822) Shondoit from the automatic community has made&nbsp; an extensive experimentation and he found that&nbsp;&nbsp;

- [00:13:54](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=834) starting with zero initialization text as empty.&nbsp; So we will start with zeroed vectors is performing&nbsp;&nbsp;

- [00:14:04](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=844) better than starting with, for example, *. Because&nbsp; * is also another token and you can see it from&nbsp;&nbsp;

- [00:14:13](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=853) here. Just type * here. It is just some vectors&nbsp; like this. So starting with empty vectors is&nbsp;&nbsp;

- [00:14:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=861) better. And now the number of vectors per token.&nbsp; So everything, every token has a vector in the&nbsp;&nbsp;

- [00:14:30](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=870) stable diffusion, and you may wonder how many&nbsp; tokens there are To find out that we are going to&nbsp;&nbsp;

- [00:14:38](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=878) check out the clip vit large patch. So in here you&nbsp; will see the tokenizer json. Yes, inside this json&nbsp;&nbsp;

- [00:14:45](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=885) file all of the tokens are listed. So you see, let&nbsp; me show, there is word IDs and words themselves,&nbsp;&nbsp;

- [00:14:56](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=896) like here: you see yes. So the list is starting&nbsp; from here. So each one of these are tokens and&nbsp;&nbsp;

- [00:15:04](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=904) it goes to the bottom like this: For example,&nbsp; sickle, whos, lamo, etour, finity. So these are&nbsp;&nbsp;

- [00:15:12](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=912) all of the tokens, all of the embeddings that the&nbsp; stable diffusion contains. If you wonder how many&nbsp;&nbsp;

- [00:15:19](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=919) there are exactly, there are exactly 49408 tokens&nbsp; and each contain one vector. For SD 1.x versions,&nbsp;&nbsp;

- [00:15:33](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=933) it is 762 vector size and for SD 2 version,&nbsp; it is 1024. So when we do embedding inspector,&nbsp;&nbsp;

- [00:15:45](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=945) you see it is showing the vector. So&nbsp; everything is composed by numerical weights&nbsp;&nbsp;

- [00:15:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=950) and they are being used by the machine learning&nbsp; algorithms to do inference and so also every&nbsp;&nbsp;

- [00:16:00](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=960) prompt we type is getting into tokenized and&nbsp; I will also show that tokenization right now.&nbsp;&nbsp;

- [00:16:06](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=966) Before we start to do that, go to the available&nbsp; tab load here and search for token and you will&nbsp;&nbsp;

- [00:16:13](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=973) see there is tokenizer, like tokenizer extension.&nbsp; Just install it, restart the UI and now you will&nbsp;&nbsp;

- [00:16:22](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=982) see tokenizer. So type your prompt here and see&nbsp; how it is getting tokenized. So let's say I am&nbsp;&nbsp;

- [00:16:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=989) going to use this kind of prompt. It is showing&nbsp; in the web UI that fifty eight tokens are being&nbsp;&nbsp;

- [00:16:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=997) used and we are limited to seventy five tokens.&nbsp; But we are not using fifty eight words here.&nbsp;&nbsp;

- [00:16:44](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1004) If you count the number of words it is not fifty&nbsp; eight. So let's copy this and go to the tokenizer,&nbsp;&nbsp;

- [00:16:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1010) paste it and tokenize, and now it is showing all&nbsp; of the tokenization. So the face is a single token&nbsp;&nbsp;

- [00:16:57](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1017) with ID of 1810. Photo is single token of a single&nbsp; token and let's see: OK, So the artstation is two&nbsp;&nbsp;

- [00:17:06](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1026) tokens. It is art and station. Comas are also one&nbsp; tokens, as you can see, and let's see if there is&nbsp;&nbsp;

- [00:17:13](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1033) any other being tokenized into some tokens.&nbsp; Or photorealistic. Photorealistic is also two&nbsp;&nbsp;

- [00:17:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1041) tokens and artstation is two tokens. So this is&nbsp; how tokenization works. Each of these tokens have&nbsp;&nbsp;

- [00:17:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1049) their own vectors and you can see their weights&nbsp; from embedding inspector. However, it is not very&nbsp;&nbsp;

- [00:17:35](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1055) useful because these numbers doesn't mean anything&nbsp; individually, but in the bigger scheme they&nbsp;&nbsp;

- [00:17:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1062) are working very well with the machine learning&nbsp; algorithms. Machine learning is all about weights.&nbsp;&nbsp;

- [00:17:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1070) Also, in the official paper of textual inversion,&nbsp; on the page four, you see they are showing a photo&nbsp;&nbsp;

- [00:17:58](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1078) of a star, which is our embedding name. So you see&nbsp; there is a tokenizer and token IDs and they have&nbsp;&nbsp;

- [00:18:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1087) vectors like this. So it is all about vectors&nbsp; and their weights. OK. Now we can return to&nbsp;&nbsp;

- [00:18:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1094) train tab. Now we have idea of our tokenization.&nbsp; So let's give a name as tutorial training. You&nbsp;&nbsp;

- [00:18:23](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1103) can give this any name. This will be activation.&nbsp; Initialization text. I am just leaving it empty to&nbsp;&nbsp;

- [00:18:30](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1110) obtain best results. So our vectors with will&nbsp; start with zero. Let's say you are training&nbsp;&nbsp;

- [00:18:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1117) a bulldog image, so you can start with bulldog&nbsp; weights. So it will make your, it may make your&nbsp;&nbsp;

- [00:18:46](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1126) training better. However, for faces since we are&nbsp; training a new face that the database has no idea,&nbsp;&nbsp;

- [00:18:54](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1134) I think leaving it is. Leaving it as empty is&nbsp; better. So, number of vectors: now you know that&nbsp;&nbsp;

- [00:19:01](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1141) each token has one vector, which means that when&nbsp; we type Brad Pitt, only two vectors are used for&nbsp;&nbsp;

- [00:19:10](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1150) that. So all of the Brad Pitt images are saved in&nbsp; the stable diffusion model with just two vectors,&nbsp;&nbsp;

- [00:19:18](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1158) which means that two vectors is a good number&nbsp; of , is a good number for our face training or&nbsp;&nbsp;

- [00:19:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1169) for our subjects training. I also have made a&nbsp; lot of experiments with one vector, two vector,&nbsp;&nbsp;

- [00:19:35](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1175) three, vector four vector, and I have found that&nbsp; two vectors are working best. However, this is&nbsp;&nbsp;

- [00:19:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1182) based on my training data set. You can also try&nbsp; one, two, three, four, five and you will see that&nbsp;&nbsp;

- [00:19:49](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1189) the quality is decreasing as you increase the&nbsp; number of vectors. Also, in the official papers&nbsp;&nbsp;

- [00:19:55](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1195) the researchers have used up to three vectors. You&nbsp; see extended latent spaces. This is the number of&nbsp;&nbsp;

- [00:20:01](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1201) vector count that is derived from the official&nbsp; paper and they have used up to three. You see&nbsp;&nbsp;

- [00:20:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1207) detonated two words and three words, but it is&nbsp; up to you to do experimentation and I am going to&nbsp;&nbsp;

- [00:20:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1214) use two. If you write overwrite old embedding, it&nbsp; will override if there is embedding like this. So&nbsp;&nbsp;

- [00:20:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1220) let's click create embedded and it is created. So&nbsp; where it is, saved. Go to your folder installation&nbsp;&nbsp;

- [00:20:30](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1230) folder and in here you will see embeddings and in&nbsp; here we can see already our embedding is composed.&nbsp;&nbsp;

- [00:20:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1237) Then let's go to the preprocess images. So this is&nbsp; a generic tab of web UI. It provides you to crop&nbsp;&nbsp;

- [00:20:48](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1248) images, create flipped copies, split oversized&nbsp; images. Auto focal point crop, use BLIP for&nbsp;&nbsp;

- [00:20:53](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1253) caption, use deepbooru for for captioning. There&nbsp; is source directory and destination directory.&nbsp;&nbsp;

- [00:21:00](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1260) So I have a folder like this for experimentation&nbsp; and showing I am copying its address like this&nbsp;&nbsp;

- [00:21:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1267) and pasting it in here as source and I am going&nbsp; to give it a destination directory like a1. They&nbsp;&nbsp;

- [00:21:15](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1275) are going to be auto-resized and cropped. So let's&nbsp; check. Let's check this checkbox. Create flipped&nbsp;&nbsp;

- [00:21:22](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1282) copies. By the way, for faces, I am not suggesting&nbsp; to use this. It is not improving quality. You can&nbsp;&nbsp;

- [00:21:28](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1288) also split oversized images, but this doesn't&nbsp; make sense for faces. Autofocal point: yes,&nbsp;&nbsp;

- [00:21:34](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1294) let's just also click that. Use BLIP for&nbsp; caption. So it will use BLIP algorithm for&nbsp;&nbsp;

- [00:21:40](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1300) captioning. This is better for real images and&nbsp; deepbooru is better for i think anime images. OK,&nbsp;&nbsp;

- [00:21:48](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1308) and then just let's click preprocess. By the way,&nbsp; why we are doing 512 and 512? Because version 1.5,&nbsp;&nbsp;

- [00:21:58](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1318) Stable Diffusion version 1.5 is based on 512&nbsp; and 512 pixels. If you use version 2.1 Stable&nbsp;&nbsp;

- [00:22:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1327) Diffusion. Then it has both 512 pixels and 768&nbsp; pixels. So you need to process images based on&nbsp;&nbsp;

- [00:22:19](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1339) the model native resolution. Based on the model&nbsp; that you are going to do training. In the training&nbsp;&nbsp;

- [00:22:26](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1346) tab it will use the selected model here. So&nbsp; be careful with that. And when the first time&nbsp;&nbsp;

- [00:22:32](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1352) when you do preprocessing, it is downloading the&nbsp; necessary files as usual. OK, the processing has&nbsp;&nbsp;

- [00:22:38](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1358) been finished. Let's open the processed folder&nbsp; from pictures and a2 folder, a1 folder. And now&nbsp;&nbsp;

- [00:22:45](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1365) you see there are flipped copies and they were&nbsp; automatically cropped to 512 and 512 pixels. And&nbsp;&nbsp;

- [00:22:52](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1372) there are also descriptions generated by the BLIP.&nbsp; When you open the descriptions, you will see like&nbsp;&nbsp;

- [00:22:58](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1378) this: a man standing in front of a metal door in&nbsp; a building with a blue shirt on and black pants.&nbsp;&nbsp;

- [00:23:05](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1385) So now we are ready with the preprocess images,&nbsp; we can go to the training tab. In here we are&nbsp;&nbsp;

- [00:23:13](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1393) selecting the embedding that we are going to&nbsp; train, embedding learning rate. There are various,&nbsp;&nbsp;

- [00:23:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1400) let's say, discussions on this learning rate, but&nbsp; in the official paper, 0.005 is used. Therefore,&nbsp;&nbsp;

- [00:23:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1409) I believe that this is the best learning rate.&nbsp; The gradient clipping is related to hyper network&nbsp;&nbsp;

- [00:23:36](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1416) learning rate, a hyper network training, so just&nbsp; don't touch it. So the batch size and gradient&nbsp;&nbsp;

- [00:23:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1422) accumulation size. This is also explained in&nbsp; the official paper. The batch size and gradient&nbsp;&nbsp;

- [00:23:49](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1429) accumulation steps will just increase your&nbsp; training speed if you have sufficient amount&nbsp;&nbsp;

- [00:23:54](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1434) of RAM, VRAM memory. However, make sure that&nbsp; the number of training images can be divided&nbsp;&nbsp;

- [00:24:00](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1440) the multiplication of these both numbers. So let's&nbsp; say you have 10 training images, then you can set&nbsp;&nbsp;

- [00:24:10](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1450) these as 2 batch size and 5 gradient accumulation,&nbsp; which is two multiplied by five, is equal to 10,&nbsp;&nbsp;

- [00:24:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1460) or they can be. Or let's say you have 40 training&nbsp; images, then you can set it as 20 or 10 or 5,&nbsp;&nbsp;

- [00:24:28](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1468) it is up to you. However, this will increase,&nbsp; significantly, increase your VRAM usage. And&nbsp;&nbsp;

- [00:24:35](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1475) let's say the multiplication of these two numbers&nbsp; is equal to 10. Then you should also multiply&nbsp;&nbsp;

- [00:24:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1482) learning rate with 10. Why? Because this requires&nbsp; learning rate to be increased. How do I know that?&nbsp;&nbsp;

- [00:24:51](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1491) In the official paper, in&nbsp; the implementation details,&nbsp;&nbsp;

- [00:24:55](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1495) they say that they are using two graphic cards&nbsp; with batch size of four. Then they are changing&nbsp;&nbsp;

- [00:25:03](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1503) the base learning rate with multiplying by&nbsp; by eight. Why? Because two graphic cards,&nbsp;&nbsp;

- [00:25:08](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1508) batch size for four multiplied by two is eight,&nbsp; and when you multiply 0.005 with 8, then we obtain&nbsp;&nbsp;

- [00:25:17](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1517) 0.04. So be careful with that. If you increase&nbsp; this batch size and gradient accumulation steps,&nbsp;&nbsp;

- [00:25:25](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1525) just also make sure that you are increasing the&nbsp; learning rate as well. However, for this tutorial&nbsp;&nbsp;

- [00:25:31](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1531) I am going to use batch size one and gradient&nbsp; accumulation steps as one. Actually, until you&nbsp;&nbsp;

- [00:25:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1537) obtain good initial results, don't change them,&nbsp; I suggest you. Then you can change them. Then you&nbsp;&nbsp;

- [00:25:44](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1544) need to set your training data set directory.&nbsp; So let's say I am going to use these images,&nbsp;&nbsp;

- [00:25:51](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1551) then I am going to set them. Also, there is log&nbsp; directory, so the training logs will be logged&nbsp;&nbsp;

- [00:25:59](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1559) in this directory where it is. When we open our&nbsp; installation folder, we will see that there is a&nbsp;&nbsp;

- [00:26:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1571) textual inversion. However, since we still didn't&nbsp; start yet, it is not generated. So when the first&nbsp;&nbsp;

- [00:26:18](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1578) time we start, it will be generated. I&nbsp; suggest you to not change this. Okay,&nbsp;&nbsp;

- [00:26:23](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1583) prompt template. So what are prompts templates?&nbsp; Why are they used? Actually, there is not a clear&nbsp;&nbsp;

- [00:26:31](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1591) explanation of this in the official paper.&nbsp; When you go to the very bottom, you will see&nbsp;&nbsp;

- [00:26:38](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1598) training prompt templates. So these templates are&nbsp; actually derived from here. From my experience,&nbsp;&nbsp;

- [00:26:46](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1606) I have a theory that these prompts are used like&nbsp; this. So let's say you are teaching a photo of a&nbsp;&nbsp;

- [00:26:55](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1615) person, then this, the vectors of these tokens&nbsp; are also used to obtain your target image. So&nbsp;&nbsp;

- [00:27:04](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1624) they are helping to reach your target image. This&nbsp; is my theory. So it is using the vector of photo,&nbsp;&nbsp;

- [00:27:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1634) it is using a vector of a of, or you are teaching&nbsp; a style, then it is using that. So these templates&nbsp;&nbsp;

- [00:27:23](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1643) are actually these ones. When you open the prompt&nbsp; template folder which is in here, let's go to the&nbsp;&nbsp;

- [00:27:32](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1652) textual inversion temples and you will see the&nbsp; template file files like this. So let's say, when&nbsp;&nbsp;

- [00:27:38](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1658) you open subject file words, it will, you will get&nbsp; a list of, like this: a photo of name and file,&nbsp;&nbsp;

- [00:27:44](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1664) or so. The name is the activation name that we&nbsp; have given. It will be treated specially. It will&nbsp;&nbsp;

- [00:27:52](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1672) not get turned into a regular token. For example,&nbsp; tutorial training would be tokenized like this if&nbsp;&nbsp;

- [00:28:00](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1680) it was not an embedding tutorial: training. Let's&nbsp; click tokenize. You see the tutorial training.&nbsp;&nbsp;

- [00:28:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1687) Actually three tokens. Tutorial is a tokenized,&nbsp; like tutor, ial and training. However, since&nbsp;&nbsp;

- [00:28:16](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1696) it will be our special embedding name, therefore&nbsp; it will be treated as with the number of special&nbsp;&nbsp;

- [00:28:25](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1705) tokens and it will be based on the number of&nbsp; vectors per token. We decided, if we decide this,&nbsp;&nbsp;

- [00:28:33](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1713) to take 10, then it will use 10 token space from&nbsp; our prompting, so it will take 10 space in here.&nbsp;&nbsp;

- [00:28:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1722) However, it will now take only two instead of&nbsp; three because it will be specially treated. Okay,&nbsp;&nbsp;

- [00:28:51](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1731) let's go back to the train tab. So? So this name&nbsp; is the. Sorry about that. This name is the name of&nbsp;&nbsp;

- [00:29:01](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1741) our embedding name and the filewords. So the&nbsp; filewords is the description generated here.&nbsp;&nbsp;

- [00:29:08](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1748) So, basically, the prompt for training will&nbsp; become, tutorial training, and the file words,&nbsp;&nbsp;

- [00:29:16](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1756) let's say it is training for this particular&nbsp; image. It will just get this and append it&nbsp;&nbsp;

- [00:29:22](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1762) here and this will become the final prompt&nbsp; for that image when doing training. So, what&nbsp;&nbsp;

- [00:29:30](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1770) should we? How should we edit this description?&nbsp; You should define the parts that you don't want&nbsp;&nbsp;

- [00:29:39](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1779) model to learn. Which parts i don't want model to&nbsp; learn? I don't want model to learn this clothing,&nbsp;&nbsp;

- [00:29:45](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1785) these walls, for example, or the this item here.&nbsp; So i have to define them as much as possibly.&nbsp;&nbsp;

- [00:29:53](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1793) So if i want model to learn glasses,&nbsp; then i need to remove glasses, okay,&nbsp;&nbsp;

- [00:29:58](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1798) and for example, if i want model to learn my&nbsp; smile, i should just remove it. Okay, i want my,&nbsp;&nbsp;

- [00:30:06](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1806) i want model to learn my face. Therefore, i can,&nbsp; i can just remove it, and this is so on. However,&nbsp;&nbsp;

- [00:30:15](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1815) i am not going to use file words in this&nbsp; training, because i have found that if you&nbsp;&nbsp;

- [00:30:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1821) pick your training data set carefully, you don't&nbsp; need to use filewords. So, how am i going to do&nbsp;&nbsp;

- [00:30:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1829) training in this case? I am just going to generate&nbsp; a new text file here and i will say: my, special,&nbsp;&nbsp;

- [00:30:41](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1841) okay, let's just open it. And here i am just going&nbsp; to type [name]. You have to use at least name,&nbsp;&nbsp;

- [00:30:47](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1847) otherwise it won't work. It will throw an error,&nbsp; and i am not going to use, filewords. Also,&nbsp;&nbsp;

- [00:30:55](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1855) i am not going to use myself in this training.&nbsp; I am going to use one of my followers. He had&nbsp;&nbsp;

- [00:31:03](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1863) he had sent me his pictures. Let me show you the&nbsp; original pictures he had sent me. Okay, this is&nbsp;&nbsp;

- [00:31:10](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1870) the images he had sent me. However, i didn't use&nbsp; all of them. You see the images right now. There&nbsp;&nbsp;

- [00:31:17](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1877) are different angles and, different backgrounds.&nbsp; When you are doing textual inversion, you should&nbsp;&nbsp;

- [00:31:25](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1885) only teach one subject at a time, but if you want&nbsp; to combine multiple subjects, then you can train&nbsp;&nbsp;

- [00:31:32](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1892) multiple embeddings and you can combine all of&nbsp; them when using, when do, when generating images.&nbsp;&nbsp;

- [00:31:38](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1898) So which ones i did pick, let me show you. I have&nbsp; picked these ones, okay, and now you will notice&nbsp;&nbsp;

- [00:31:46](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1906) something here. You see, the background is here,&nbsp; is like this. You see green and some noise. Why?&nbsp;&nbsp;

- [00:31:55](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1915) Because i don't want model to learn background.&nbsp; So if multiple images containing same background,&nbsp;&nbsp;

- [00:32:01](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1921) i am just noising out those backgrounds. And why&nbsp; I did not noise out to other backgrounds? Because&nbsp;&nbsp;

- [00:32:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1927) other backgrounds are different. So you see, in&nbsp; your training data set, only the subject should&nbsp;&nbsp;

- [00:32:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1934) be same and all other things need to be different,&nbsp; like backgrounds, like clothing and other things.&nbsp;&nbsp;

- [00:32:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1941) So the training will learn only your subject,&nbsp; in this case the face. It will not learn the&nbsp;&nbsp;

- [00:32:28](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1948) background or the clothing. Okay, so let me show&nbsp; the original one. So in the original one you see&nbsp;&nbsp;

- [00:32:34](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1954) this image, this image, this image and these two&nbsp; images have same backgrounds. So i have edited&nbsp;&nbsp;

- [00:32:40](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1960) those same backgrounds with paint .NET, which is&nbsp; a free editing software. You can also edit with&nbsp;&nbsp;

- [00:32:47](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1967) paint. How did i edit it? It is so actually simple&nbsp; and amateur, you may say. So let's set a brush&nbsp;&nbsp;

- [00:32:55](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1975) size here and just, for example, change the color&nbsp; like this. Then i did added some noise: select it&nbsp;&nbsp;

- [00:33:04](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1984) with a selection tool, set the tolerance from&nbsp; here and go to the effects, adjust effects and&nbsp;&nbsp;

- [00:33:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=1991) in here you will see distort and frosted glass and&nbsp; when you click it it will change the appearance.&nbsp;&nbsp;

- [00:33:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2000) You can also try other distortion. By the&nbsp; way, i am providing these images to you for&nbsp;&nbsp;

- [00:33:27](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2007) testing. Let me show you the link. So i have&nbsp; uploaded images into a google drive folder&nbsp;&nbsp;

- [00:33:33](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2013) and i am going to put the link of this into&nbsp; the description so you can download this data&nbsp;&nbsp;

- [00:33:38](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2018) set and do training and see how it performs.&nbsp; Are you able to obtain good results, as me?&nbsp;&nbsp;

- [00:33:45](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2025) Okay, so i am going to change my training data&nbsp; set folder from pictures and i am going to use&nbsp;&nbsp;

- [00:33:55](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2035) example training set folder. I am going to set it&nbsp; in my training here. Okay, and i am going to use&nbsp;&nbsp;

- [00:34:05](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2045) my prompt template. Just refresh it and go to the&nbsp; my special. So what was my special? My special was&nbsp;&nbsp;

- [00:34:12](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2052) just only containing [name]. It is not containing&nbsp; any file descriptions. I have found that this is&nbsp;&nbsp;

- [00:34:18](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2058) working great if you optimize your training data&nbsp; set, as, like me, you can try both of them. You&nbsp;&nbsp;

- [00:34:26](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2066) can try with [filewords] and you can try without&nbsp; [filewords]. [filewords] and you can see how it&nbsp;&nbsp;

- [00:34:33](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2073) is working. Okay, do not resize images, because&nbsp; our images are already 512 pixels max steps. Now,&nbsp;&nbsp;

- [00:34:40](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2080) this can be set anything. I will show you a way&nbsp; to understand whether you started over training&nbsp;&nbsp;

- [00:34:48](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2088) or not, so this can be stay like this. In how&nbsp; many steps we want to save? Okay, this is rather&nbsp;&nbsp;

- [00:34:56](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2096) different than epochs in the DreamBooth, if you&nbsp; have watched my DreamBooth videos. So each image&nbsp;&nbsp;

- [00:35:03](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2103) is one step and there is no epoch saving here.&nbsp; It is step saving. How many training images i&nbsp;&nbsp;

- [00:35:10](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2110) have. I have total 10 images, therefore, okay, so&nbsp; for 10 epochs we need to set this 100, actually,&nbsp;&nbsp;

- [00:35:18](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2118) okay. So the formula is like this: one epoch&nbsp; equal to number of training images. 10 epoch&nbsp;&nbsp;

- [00:35:24](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2124) for 10 training images is 10 multiplied by 10&nbsp; is 100, so it will be saved every 10 epoch.&nbsp;&nbsp;

- [00:35:31](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2131) Save images with embedding in png chunks. This&nbsp; will save embed. This will save embedding info&nbsp;&nbsp;

- [00:35:41](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2141) in the generated preview images. I will show&nbsp; you. Read parameters from text to image tab&nbsp;&nbsp;

- [00:35:48](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2148) when generating preview images. I don't want that,&nbsp; so it will just use the regular prompts, that is,&nbsp;&nbsp;

- [00:35:57](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2157) that we will see in here. Shuffle tags by comma,&nbsp; which which means that if you use file words,&nbsp;&nbsp;

- [00:36:05](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2165) the words in there will be shuffled when doing&nbsp; training. This can be useful. You can test it&nbsp;&nbsp;

- [00:36:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2171) out and drop out tags when creating prompts.&nbsp; This means that it will randomly drop the&nbsp;&nbsp;

- [00:36:18](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2178) file descriptions, file captions,&nbsp; that you have used. This is, i think,&nbsp;&nbsp;

- [00:36:23](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2183) percentage based. So if you set it 0.1, it will&nbsp; drop out randomly the 10 percent, and i am not&nbsp;&nbsp;

- [00:36:30](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2190) going to use file words. Therefore, this will have&nbsp; zero effect. Okay, choose latent sampling methods.&nbsp;&nbsp;

- [00:36:36](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2196) I also have searched this. In the official&nbsp; paper. Random is used. However, one of the&nbsp;&nbsp;

- [00:36:43](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2203) community developer proposed deterministic and&nbsp; he found that deterministic is working best. So&nbsp;&nbsp;

- [00:36:49](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2209) choose deterministic. And now we are ready so we&nbsp; can start training. So i am going to click train&nbsp;&nbsp;

- [00:36:57](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2217) embedding. Okay, training has started, as you can&nbsp; see, and we are. It is displaying the number of&nbsp;&nbsp;

- [00:37:05](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2225) epochs and number of steps. It is displaying the&nbsp; iteration, so currently it is 1.30 seconds for&nbsp;&nbsp;

- [00:37:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2234) per iteration. Why? Because i am recording and&nbsp; it is taking already a lot of gpu power. I have&nbsp;&nbsp;

- [00:37:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2241) RTX 3060. It has 12 gigabyte of memory. Let me&nbsp; also show you what is taking the memory usage.&nbsp;&nbsp;

- [00:37:32](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2252) You see, OBS studio is already using a lot of gpu&nbsp; memory and also training uses. But since they are&nbsp;&nbsp;

- [00:37:39](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2259) using different parts of the gpu, i think it&nbsp; is working fine. When we open the performance,&nbsp;&nbsp;

- [00:37:43](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2263) we can see that the training is using the 3d part&nbsp; of the gpu and obs is using the video encode part&nbsp;&nbsp;

- [00:37:52](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2272) of the gpu. That is how i am still able to record,&nbsp; but sometimes it is dropping out out my voice. I&nbsp;&nbsp;

- [00:37:59](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2279) hope that it is currently recording very well.&nbsp; Okay, and i also did some overclocking to my gpu&nbsp;&nbsp;

- [00:38:10](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2290) by using MSI Afterburner. I have increased&nbsp; the core clock by 175 and i have increased&nbsp;&nbsp;

- [00:38:18](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2298) memory clock by 900, so this boosted my training&nbsp; speed like 10%. You can also do that if you want.&nbsp;&nbsp;

- [00:38:26](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2306) I didn't do any core voltage increasing. So it&nbsp; has already generated two preview images where&nbsp;&nbsp;

- [00:38:34](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2314) we are going to find them. Let me show you. Now&nbsp; it will be inside textual inversion folder. You&nbsp;&nbsp;

- [00:38:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2322) see it has just arrived and when you open it you&nbsp; will see the date, the date of the training, and&nbsp;&nbsp;

- [00:38:51](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2331) you will see the name of the embed we are going&nbsp; training and in here you will see embeddings. This&nbsp;&nbsp;

- [00:38:56](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2336) is the checkpoint. So you can use any checkpoint&nbsp; to do to generate images and, this is the images&nbsp;&nbsp;

- [00:39:04](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2344) that it has generated. So this is the first&nbsp; image and also in image embeddings. This image&nbsp;&nbsp;

- [00:39:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2351) embedding contains the embedding info. Why this&nbsp; is generated? Because we did check this checkbox.&nbsp;&nbsp;

- [00:39:19](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2359) You will see the used prompts here. Since i&nbsp; didn't use any file words and i just used name,&nbsp;&nbsp;

- [00:39:25](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2365) it is only using this name as a prompt. And what&nbsp; does that mean? That means that it is only using&nbsp;&nbsp;

- [00:39:34](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2374) the vectors we have generated in the beginning&nbsp; to learn our subject, to learn the details of&nbsp;&nbsp;

- [00:39:40](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2380) our subject, which is the face, and we have&nbsp; two vectors to learn, and also Brad Pitt is&nbsp;&nbsp;

- [00:39:48](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2388) based on the two vectors, so why not? We can&nbsp; be also taught to the model with two vectors.&nbsp;&nbsp;

- [00:39:56](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2396) Okay, in the just in the 20th epoch,&nbsp; we already getting some similarity.&nbsp;&nbsp;

- [00:40:03](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2403) Actually, i already did the same training,&nbsp; so i already have the trained data.&nbsp;&nbsp;

- [00:40:12](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2412) But i am doing recording while training for&nbsp; you again, for to explain to you better.&nbsp;&nbsp;

- [00:40:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2421) It also shows here the estimated time, for&nbsp; training to be completed. This time is based on&nbsp;&nbsp;

- [00:40:28](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2428) 100 000 steps, but we are not going to train that&nbsp; much. Actually, i have found that around three&nbsp;&nbsp;

- [00:40:35](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2435) thousand steps. We are getting very good results,&nbsp; with the training data set i have. It will totally&nbsp;&nbsp;

- [00:40:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2442) depend on the training data set you have with how&nbsp; many number of steps you can teach your subject.&nbsp;&nbsp;

- [00:40:48](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2448) I will show you the way how to determine which one&nbsp; is best, which checkpoint is best, which number of&nbsp;&nbsp;

- [00:40:56](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2456) steps is best. Okay, with 30 epoch we already got&nbsp; a very much similar image. You see, with just 30&nbsp;&nbsp;

- [00:41:06](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2466) epoch we are starting to get very similar images.&nbsp; It is starting to learn our subject very well&nbsp;&nbsp;

- [00:41:12](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2472) with just 30 epoch and when we get over 100&nbsp; epoch, we will get much better quality images.&nbsp;&nbsp;

- [00:41:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2480) Okay, it has been over 60 steps,&nbsp; 600 steps and over 60 epochs,&nbsp;&nbsp;

- [00:41:26](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2486) and we got six preview images. Since we are&nbsp; generating preview images and checkpoints,&nbsp;&nbsp;

- [00:41:32](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2492) for every 10 epoch. Now i am going to show you how&nbsp; you can determine whether you are overtraining or&nbsp;&nbsp;

- [00:41:40](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2500) not with a community developed script. So the&nbsp; script name is: inspect embedding training.&nbsp;&nbsp;

- [00:41:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2510) It is hosted on github. It's a public project.&nbsp; I will put the link of this project to the&nbsp;&nbsp;

- [00:41:55](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2515) description as well. Everything, every link,&nbsp; will be put to the description. So check out&nbsp;&nbsp;

- [00:41:59](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2519) the video description and in here, just click&nbsp; code and download as zip. Okay, it is downloaded.&nbsp;&nbsp;

- [00:42:06](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2526) When you open it you will see inspect embedding&nbsp; training part. Extract this file into your textual&nbsp;&nbsp;

- [00:42:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2534) inversion and tutorial training, as i have shown.&nbsp; So you will see these files there. To extract it,&nbsp;&nbsp;

- [00:42:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2541) just drag and drop. Why we are extracting it in&nbsp; here? Because we are going to analyze the loss.&nbsp;&nbsp;

- [00:42:28](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2548) And so the loss, what is loss? You are always&nbsp; seeing the loss here. The number value is here:&nbsp;&nbsp;

- [00:42:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2557) loss is the penalty for a bad prediction. That&nbsp; is, that is loss is a number indicating how bad&nbsp;&nbsp;

- [00:42:43](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2563) the model's prediction was on a single example.&nbsp; If the model's prediction is perfect, the loss is&nbsp;&nbsp;

- [00:42:49](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2569) zero. Otherwise the loss is greater. In our case&nbsp; we can think that as the model generated image,&nbsp;&nbsp;

- [00:42:56](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2576) how likely, how close to our training subjects,&nbsp; training images. So if you get a zero loss,&nbsp;&nbsp;

- [00:43:05](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2585) that means that model is learning very good,&nbsp; okay. If your loss is too high, that means that&nbsp;&nbsp;

- [00:43:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2591) your model is not learning. Now, with this script&nbsp; we have extracted here, we are going to see the&nbsp;&nbsp;

- [00:43:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2600) loss. And how are we going to use this script?&nbsp; This script requires torch installation and&nbsp;&nbsp;

- [00:43:28](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2608) the torch is already installed in our web ui&nbsp; folder, inside venv folder, virtual environment,&nbsp;&nbsp;

- [00:43:36](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2616) and inside here scripts. So we are going to use&nbsp; the python exe here to do that. First copy the&nbsp;&nbsp;

- [00:43:43](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2623) path of this. Open a notepad file like this: okay,&nbsp; put quotation marks and just type python exe like&nbsp;&nbsp;

- [00:43:54](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2634) this: okay, then we are going to get the path&nbsp; of the file. Let me show you. The script file&nbsp;&nbsp;

- [00:44:02](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2642) is in this folder. So, with quotation marks,&nbsp; just copy and paste it in here and type the&nbsp;&nbsp;

- [00:44:12](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2652) script file name like this: then open&nbsp; a new cmd window by typing like this:&nbsp;&nbsp;

- [00:44:19](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2659) okay, let me some zoom in, copy and paste the&nbsp; path like this, the code, and just hit enter&nbsp;&nbsp;

- [00:44:28](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2668) and you will see it has generated some&nbsp; info for us: learning rate at step at,&nbsp;&nbsp;

- [00:44:34](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2674) loss jpg, vector, vector jpg and the average&nbsp; vector strength. So let's open our folder in&nbsp;&nbsp;

- [00:44:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2682) here and we will see the files. When we open the&nbsp; loss file we are going to see a graph like this:&nbsp;&nbsp;

- [00:44:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2690) an average loss is below 0.2, which means it is&nbsp; learning very well. Why, as close as it is to 0,&nbsp;&nbsp;

- [00:44:58](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2698) it is better, so as close as it is to 1,&nbsp; it is worse. So currently we are able to&nbsp;&nbsp;

- [00:45:04](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2704) learn very well. Now i will show you how&nbsp; to determine the over training or not.&nbsp;&nbsp;

- [00:45:13](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2713) To do that, we are going to add a parameter&nbsp; here, --folder, and just give the folder of&nbsp;&nbsp;

- [00:45:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2721) embedding files here. Just copy paste it again&nbsp; do not forget quotation marks and open a new&nbsp;&nbsp;

- [00:45:28](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2728) cmd window. Just copy and paste it, hit enter.&nbsp; It will calculate the average strength of the&nbsp;&nbsp;

- [00:45:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2737) vectors and when this strength is over 0.2, that&nbsp; usually means that you started over training. How&nbsp;&nbsp;

- [00:45:46](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2746) do we know? According to the developer of this,&nbsp; this script, if the strength of the average&nbsp;&nbsp;

- [00:45:54](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2754) strength of the all vectors is greater than 0.2,&nbsp; the embedding starts to become inflexible. That&nbsp;&nbsp;

- [00:46:02](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2762) means over training. So you will not be able&nbsp; to stylize your trained subject. So you won't&nbsp;&nbsp;

- [00:46:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2774) be able to get good images like this if you&nbsp; do over training, if you're, if the strength&nbsp;&nbsp;

- [00:46:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2780) of the vectors becomes too weak. And what was the&nbsp; vector strength? It was so easy. When we opened,&nbsp;&nbsp;

- [00:46:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2789) the embedding tab, the embedding inspector tab,&nbsp; we were able to see the values of vectors. So this&nbsp;&nbsp;

- [00:46:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2797) strength means that the average of these values,&nbsp; when they, when the average of these values is&nbsp;&nbsp;

- [00:46:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2802) over 0.2 that means that you are starting to&nbsp; do over training. You need to check this out&nbsp;&nbsp;

- [00:46:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2810) to determine that. By the way, it is said&nbsp; that the DreamBooth is best to teach faces,&nbsp;&nbsp;

- [00:46:58](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2818) and in the official paper of embedding, the&nbsp; textual inversion, the authors, the researchers,&nbsp;&nbsp;

- [00:47:04](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2824) have used all you see objects like this, or they&nbsp; have used training on style, let me show you like&nbsp;&nbsp;

- [00:47:13](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2833) here. However, as i have just shown, as i have&nbsp; just demonstrated you this textual embeddings are&nbsp;&nbsp;

- [00:47:22](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2842) also very good, very successful, for teaching&nbsp; faces as well, and for objects, of course,&nbsp;&nbsp;

- [00:47:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2849) it is working very well as well. And for styles. I&nbsp; think the textual inversion, the text embeddings,&nbsp;&nbsp;

- [00:47:36](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2856) is much better than DreamBooth. So if you want&nbsp; to teach objects or styles, then i suggest you&nbsp;&nbsp;

- [00:47:44](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2864) to use textual inversion. Actually for faces, i&nbsp; think textual inversion of the automatic1111 is&nbsp;&nbsp;

- [00:47:51](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2871) working very well as well. And for DreamBooth to&nbsp; obtain very good results, you need to merge your&nbsp;&nbsp;

- [00:47:59](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2879) learned subject into a new model, which i have&nbsp; shown in my video. So if you use dream boot,&nbsp;&nbsp;

- [00:48:05](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2885) you should inject your trained subject into a&nbsp; good custom model to obtain very good images.&nbsp;&nbsp;

- [00:48:10](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2890) But on textual inversion, you can already obtain&nbsp; very good images. Okay, we are over 170 epoch and&nbsp;&nbsp;

- [00:48:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2900) meanwhile training is going on. I will show you&nbsp; the difference of DreamBooth, textual inversion,&nbsp;&nbsp;

- [00:48:27](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2907) LoRA and hypernetworks. One of the community&nbsp; member of reddit, use_excalidraw, prepared&nbsp;&nbsp;

- [00:48:36](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2916) an infographic like this and this is very useful.&nbsp; So in DreamBooth, we are modifying the weights of&nbsp;&nbsp;

- [00:48:43](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2923) the model itself. So all of the prompt words&nbsp; we use you know. You already know by now that&nbsp;&nbsp;

- [00:48:52](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2932) they all, they all have vectors of them, each of&nbsp; them, and these vectors are getting modified in&nbsp;&nbsp;

- [00:48:59](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2939) DreamBooth, all of them. The token we selected&nbsp; for DreamBooth is also getting modified and in&nbsp;&nbsp;

- [00:49:06](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2946) DreamBooth we are not able to add a new vector.&nbsp; We already have to use one of the existing vectors&nbsp;&nbsp;

- [00:49:15](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2955) of the model. Therefore, we are selecting&nbsp; one of the existing tokens in the models,&nbsp;&nbsp;

- [00:49:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2960) such as sks or ohwx. So in DreamBooth we are&nbsp; basically modifying, altering the model itself.&nbsp;&nbsp;

- [00:49:32](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2972) Okay, in Textual Inversion we are adding a new&nbsp; token. Actually, this is displayed incorrectly&nbsp;&nbsp;

- [00:49:39](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2979) because it is generating a unique new vector which&nbsp; does not exist in the model, and we are modifying&nbsp;&nbsp;

- [00:49:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2990) the weights of these new vectors. So when we set&nbsp; the vector count as two, it is actually using two&nbsp;&nbsp;

- [00:49:58](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=2998) unique new tokens. So it is modifying two vectors.&nbsp; If we set the vector count 10, it is using 10&nbsp;&nbsp;

- [00:50:06](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3006) unique tokens. It is being specially treated, it&nbsp; is adding new 10 vectors and it is not modifying&nbsp;&nbsp;

- [00:50:15](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3015) any of the existing vector of the model. So if we&nbsp; set the vector count to 10, actually when we do,&nbsp;&nbsp;

- [00:50:25](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3025) when we generate an image in here, it will&nbsp; use 10 vectors. It will use 10 tokens out&nbsp;&nbsp;

- [00:50:31](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3031) of 75 tokens we have. We have 75 tokens limit. So&nbsp; this is how it works. Also, if you use 10 vector,&nbsp;&nbsp;

- [00:50:41](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3041) you will see that you are getting very bad&nbsp; results for face. I have made tests. Tests. Okay,&nbsp;&nbsp;

- [00:50:47](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3047) in LoRA it is. This is very similar to the&nbsp; DreamBooth. It is modifying the existing vectors&nbsp;&nbsp;

- [00:50:55](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3055) of the model. It is. I have found that the LoRA is&nbsp; inferior to the DreamBooth, but it is just using&nbsp;&nbsp;

- [00:51:05](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3065) lesser VRAM and it is faster. Therefore, people is&nbsp; choosing that. However, for quality, DreamBooth is&nbsp;&nbsp;

- [00:51:12](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3072) better in as shown here, and the hypernetworks.&nbsp; Hypernetworks doesn't have an official academic&nbsp;&nbsp;

- [00:51:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3081) paper. I think it is made upon a leaked code&nbsp; and this is the least successful method. It&nbsp;&nbsp;

- [00:51:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3089) is the worst quality, so just don't waste time&nbsp; with it. It is so i don't suggest to use it.&nbsp;&nbsp;

- [00:51:38](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3098) So in hypernetworks, the original weights,&nbsp; original vectors of the model is not modified,&nbsp;&nbsp;

- [00:51:44](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3104) but at the inference time. Inference means that&nbsp; when you generate an image from text to image,&nbsp;&nbsp;

- [00:51:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3110) it is inference. They are just getting swapped&nbsp; in. So you see there are some images which are&nbsp;&nbsp;

- [00:51:58](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3118) training sample, apply noise compare and&nbsp; there is loss. So this is how the model&nbsp;&nbsp;

- [00:52:06](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3126) is learning. Basically, of course, there are a&nbsp; lot of details if you are interested in them,&nbsp;&nbsp;

- [00:52:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3131) you can just read the official paper, but it is&nbsp; very hard to understand and complex thing things.&nbsp;&nbsp;

- [00:52:19](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3139) Okay, we are over 200 epochs, so we have 20&nbsp; example images and the last one is extremely&nbsp;&nbsp;

- [00:52:27](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3147) similar to our official training set, as you&nbsp; can see. So let's also check out our strength&nbsp;&nbsp;

- [00:52:34](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3154) of the training vector. So i am just typing,&nbsp; hitting the up arrow in my keyboard, and it is&nbsp;&nbsp;

- [00:52:43](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3163) retyping the last executed command and hit enter.&nbsp; Okay, so our strength, average strength, is 0.13,&nbsp;&nbsp;

- [00:52:53](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3173) actually almost 0.14. We are getting close to 0.2.&nbsp; After 0.2, we can assume we started over training.&nbsp;&nbsp;

- [00:53:02](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3182) Of course, this would depend on your training&nbsp; data set, but it is an indication according&nbsp;&nbsp;

- [00:53:08](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3188) to the experience of the this developer. It also&nbsp; makes sense because as the strength of the vector&nbsp;&nbsp;

- [00:53:16](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3196) increases, it will override the other vectors. You&nbsp; see, since they are all floating point numbers,&nbsp;&nbsp;

- [00:53:27](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3207) numeric numbers, the bigger numeric number is&nbsp; usually making ineffective the lower numeric&nbsp;&nbsp;

- [00:53:36](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3216) numbers. This is how machine learning usually&nbsp; works, according to the chosen algorithms. They&nbsp;&nbsp;

- [00:53:41](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3221) are extremely complex stuff, but this is one of&nbsp; the, let's say, common principles that in the many&nbsp;&nbsp;

- [00:53:49](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3229) of the numerical weights based machine learning&nbsp; algorithms. Therefore, it also makes sense.&nbsp;&nbsp;

- [00:53:57](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3237) Okay, we are over 500 epochs at the moment. So&nbsp; let me show you the generated sample images. These&nbsp;&nbsp;

- [00:54:05](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3245) are the sample images. We are already very similar&nbsp; and the latest one is, you see, looks like getting&nbsp;&nbsp;

- [00:54:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3251) over trained. So let's check out with the script&nbsp; we have. Just hit the up arrow and hit enter&nbsp;&nbsp;

- [00:54:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3260) and you see, we are now over 0.2 strength.&nbsp; Therefore, I am going to cancel the training and&nbsp;&nbsp;

- [00:54:28](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3268) now I will show you how to use these embeddings.&nbsp; But before doing that, first let's set the newest&nbsp;&nbsp;

- [00:54:36](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3276) VAE file to generate better quality images.&nbsp; To do that, let's go to the let me find it&nbsp;&nbsp;

- [00:54:49](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3289) okay go to the Stable Diffusion tab&nbsp; in the settings and in here, you see,&nbsp;&nbsp;

- [00:54:54](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3294) SD VAE is automatic. I am going to select the&nbsp; one we did put. Let's apply settings, okay,&nbsp;&nbsp;

- [00:55:02](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3302) and then we will reload to UI. Okay, settings&nbsp; applied and UI is reloaded. So how are we going to&nbsp;&nbsp;

- [00:55:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3311) use these generated embeddings? It is easy. First&nbsp; let's enter to our textual inversion directory and&nbsp;&nbsp;

- [00:55:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3321) inside here let's go to the embeddings folder.&nbsp; Let me show you what kind of path it is. I know&nbsp;&nbsp;

- [00:55:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3329) that it is looking small, so this is where I&nbsp; have installed my automatic1111. This is the&nbsp;&nbsp;

- [00:55:40](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3340) main folder: textual inversion. This is the date&nbsp; of the training, when it was started. This is the&nbsp;&nbsp;

- [00:55:46](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3346) embedding name that I have given and this is the&nbsp; folder where the embedding checkpoints are saved.&nbsp;&nbsp;

- [00:55:53](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3353) When we analyze the weights, we see the&nbsp; change it has. So I am going to pick 20&nbsp;&nbsp;

- [00:56:01](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3361) of them to compare. How am I gonna do that?&nbsp; I will pick with 200 per epoch, like this:&nbsp;&nbsp;

- [00:56:10](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3370) okay, I have selected 24. Right click copy. By&nbsp; the way, for selecting each one of them, I have&nbsp;&nbsp;

- [00:56:18](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3378) used control button. You can select all of them.&nbsp; It is just fine. Then move to the main folder,&nbsp;&nbsp;

- [00:56:23](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3383) installation, and in here you will see embeddings&nbsp; folder. Go there, I'm just going to delete&nbsp;&nbsp;

- [00:56:28](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3388) the original one and I am pasting the ones as&nbsp; checkpoints. So how we are going to use them, just&nbsp;&nbsp;

- [00:56:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3397) type their name like this: this is equal to OHWX&nbsp; in the, in the DreamBooth tutorials that we have&nbsp;&nbsp;

- [00:56:45](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3405) and let's see. Currently it says that it is&nbsp; using seven prompts, but this is not correct&nbsp;&nbsp;

- [00:56:53](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3413) actually. It should be using just two. Okay,&nbsp; maybe it didn't refresh. Let's do a generation.&nbsp;&nbsp;

- [00:57:03](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3423) Okay, we got our picture. I think this is taking&nbsp; seven length because it is also using the okay,&nbsp;&nbsp;

- [00:57:15](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3435) yeah, so okay, now fixed. Now you see it is using&nbsp; only two tokens. Why? Because now it has loaded&nbsp;&nbsp;

- [00:57:25](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3445) the embedding file name and our embedding&nbsp; was composed with two vectors. Therefore,&nbsp;&nbsp;

- [00:57:34](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3454) it is using two vectors. However, if this was&nbsp; not our embedding name, it, if it was, was just a&nbsp;&nbsp;

- [00:57:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3462) regular prompt. If we go to the tokenizer we can&nbsp; see it was going to take. Let me show you one,&nbsp;&nbsp;

- [00:57:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3470) two, three, four, five, six, seven, eight tokens.&nbsp; You see each number is a token. This is a token.&nbsp;&nbsp;

- [00:57:58](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3478) So it was going to use eight tokens, but since it&nbsp; is an embedding name and the embedding is only two&nbsp;&nbsp;

- [00:58:05](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3485) vectors, it is using only two tokens because&nbsp; in the background, in the technical details,&nbsp;&nbsp;

- [00:58:13](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3493) it has composed of two unique tokens, since we&nbsp; did set the vector count 2. So for each vector a&nbsp;&nbsp;

- [00:58:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3500) token is generated and with a textual embedding&nbsp; we are able to insert, we are able to generate&nbsp;&nbsp;

- [00:58:28](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3508) new tokens, unlike DreamBooth. DreamBooth can only&nbsp; use the existing tokens. Okay, so now we are going&nbsp;&nbsp;

- [00:58:35](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3515) to generate a test case with using X/Y plot. I&nbsp; have tested CFG values and the prompt strength.&nbsp;&nbsp;

- [00:58:45](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3525) So from prompt strength i mean that prompt&nbsp; attention emphasis and it is explained in the wiki&nbsp;&nbsp;

- [00:58:51](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3531) of the Stable Diffusion of automatic1111 web ui.&nbsp; So you see, when you use parentheses like this,&nbsp;&nbsp;

- [00:58:57](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3537) it increases the attention toward by factor of&nbsp; 1.1. You can also set directly the attention like&nbsp;&nbsp;

- [00:59:03](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3543) this. So i have tested with embeddings, the prompt&nbsp; attention and it. It always resulted bad quality&nbsp;&nbsp;

- [00:59:12](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3552) for me, but still you can test with them. I also&nbsp; played with the CFG higher values. They were also&nbsp;&nbsp;

- [00:59:18](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3558) not very good, but now i will show you how to&nbsp; test each one of the embeddings. So instead of&nbsp;&nbsp;

- [00:59:26](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3566) typing manually, manually each one of the name,&nbsp; i have prepared a public cs fiddle script. I will&nbsp;&nbsp;

- [00:59:34](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3574) also share the link of this script so you will be&nbsp; also able to use it. So the starting checkpoint:&nbsp;&nbsp;

- [00:59:41](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3581) the starting checkpoint is 400, so let's set it&nbsp; as 400. Our increment is 200. We have selected&nbsp;&nbsp;

- [00:59:48](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3588) and our embedding name is tutorial name. Okay, so&nbsp; let's just type it in here. Then just click run&nbsp;&nbsp;

- [00:59:58](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3598) and you see it has generated me all of the names.&nbsp; I have names up to 5000. I copy them with a ctrl&nbsp;&nbsp;

- [01:00:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3607) c or copy. Then we are going to paste it in here&nbsp; in the X/Y plot and in here select the prompt sr.&nbsp;&nbsp;

- [01:00:15](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3615) Then we need to set a keyword. Okay, let's set a&nbsp; keyword as kw. Okay, test, it is not important,&nbsp;&nbsp;

- [01:00:26](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3626) you can simply set, set anything here. Now i&nbsp; will copy and paste some good prompts. To do&nbsp;&nbsp;

- [01:00:33](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3633) that i will use png info, drag and drop. Okay,&nbsp; i have lots of experimentation. As you can see,&nbsp;&nbsp;

- [01:00:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3642) these experimentation are from protogen&nbsp; training with textual embeddings. It was&nbsp;&nbsp;

- [01:00:47](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3647) also extremely successful for my face. Okay, let's&nbsp; pick from my today, experimentation which is under&nbsp;&nbsp;

- [01:00:59](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3659) okay, under here. Let's just pick one of&nbsp; them. Okay, now i am going to copy paste&nbsp;&nbsp;

- [01:01:06](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3666) this into text to image tab. You see,&nbsp; when you use png info, it shows all of&nbsp;&nbsp;

- [01:01:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3671) the parameters of the selected picture, if they&nbsp; are select, if they are generated by the web,&nbsp;&nbsp;

- [01:01:19](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3679) ui by default. Okay, so you see. Face&nbsp; photo of. Let me zoom in like this:&nbsp;&nbsp;

- [01:01:27](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3687) testp2400. This is from my previous embedding,&nbsp; so it is currently 60 tokens. Now i am going to&nbsp;&nbsp;

- [01:01:34](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3694) replace these with my test keyword. They will&nbsp; be replaced with these all of the tutorial&nbsp;&nbsp;

- [01:01:43](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3703) training text which are my embedding names, and&nbsp; prompt sr. Okay, as a second parameter. You see,&nbsp;&nbsp;

- [01:01:51](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3711) now it is reduced to 55. You can try CFG values&nbsp; actually, if you want. Or you can try the prompt&nbsp;&nbsp;

- [01:02:01](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3721) strength, prompt emphasis. To do that just at&nbsp; another keyword here as another kw. Okay, and&nbsp;&nbsp;

- [01:02:11](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3731) let's put it in as a prompt strength, actually,&nbsp; not more strength, but prompt sr, okay, and&nbsp;&nbsp;

- [01:02:19](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3739) replace it with 1.0 and 1.1, for example. So you&nbsp; will see the results of different prompt emphasis,&nbsp;&nbsp;

- [01:02:30](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3750) attention emphasis, as explained here. You can&nbsp; test them. You can also test the CFG values.&nbsp;&nbsp;

- [01:02:35](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3755) It is totally up to you. Do not check this&nbsp; box, because you will. You want to see the&nbsp;&nbsp;

- [01:02:42](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3762) same seed images. Actually, since these are&nbsp; different checkpoints, you are not going to&nbsp;&nbsp;

- [01:02:47](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3767) get the same image. By the way, when we use the&nbsp; command argument in here, let me show you. When&nbsp;&nbsp;

- [01:02:58](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3778) we use xformers, even if you use the same seed,&nbsp; you, you will not get the same image because,&nbsp;&nbsp;

- [01:03:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3787) since this is doing a lot of optimization, it&nbsp; will not allow you to get the exactly same image,&nbsp;&nbsp;

- [01:03:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3794) even if you use the same model and the same&nbsp; seed.. And also, there is one more thing:&nbsp;&nbsp;

- [01:03:21](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3801) actually there are two more options. If you&nbsp; have low vram. Let me show you. So in command&nbsp;&nbsp;

- [01:03:29](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3809) line arguments of the wiki, if we search for&nbsp; VRAM, let's see like this: you will see there&nbsp;&nbsp;

- [01:03:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3817) is medvram, medium vram and low vram. So if you&nbsp; also add these parameters to your command line&nbsp;&nbsp;

- [01:03:45](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3825) arguments like this. Let me show, okay, medvram&nbsp; and lowvram. It will allow you to run the web&nbsp;&nbsp;

- [01:03:55](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3835) ui on a lower vram gpu. and with lowvram and&nbsp; medvram you can still generate images with very&nbsp;&nbsp;

- [01:04:04](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3844) low amount of gpu. However, when you use low vram,&nbsp; it will not allow you to do training. So you can&nbsp;&nbsp;

- [01:04:13](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3853) add medvram to your command line argument and this&nbsp; will allow you to do training, textual embedding,&nbsp;&nbsp;

- [01:04:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3860) textual inversion training on a lower vram having&nbsp; gpu. Okay, okay, now we are ready. I'm not going&nbsp;&nbsp;

- [01:04:30](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3870) to test the strength, so i'm only going to test,&nbsp; the different embedding checkpoints. Okay, draw&nbsp;&nbsp;

- [01:04:39](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3879) legend, include separate images. Keep, keep minus&nbsp; one. Okay, we are ready. I'm not going to apply&nbsp;&nbsp;

- [01:04:45](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3885) restore faces or tiling or high resolution fix&nbsp; and okay, so let's just click and see the results.&nbsp;&nbsp;

- [01:04:56](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3896) Oh, by the way, to get a better idea, i am&nbsp; setting the batch size eight. So in each&nbsp;&nbsp;

- [01:05:03](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3903) in each generation, it will generate eight&nbsp; images for each one of the embedding checkpoint.&nbsp;&nbsp;

- [01:05:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3914) Okay, let me also show you the speed. So it&nbsp; is going to generate 25 grids because we have&nbsp;&nbsp;

- [01:05:20](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3920) selected 25 checkpoints and each one will be eight&nbsp; images. Therefore, it will generate 200 images.&nbsp;&nbsp;

- [01:05:27](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3927) Currently it shows speed as 5.73 second&nbsp; per iteration. Actually, per iteration is&nbsp;&nbsp;

- [01:05:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3937) currently eight images, eight steps, because we&nbsp; are generating eight images parallely as a batch.&nbsp;&nbsp;

- [01:05:46](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3946) Therefore, it is actually eight times faster&nbsp; than the regular one single image generation.&nbsp;&nbsp;

- [01:05:54](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3954) Okay, since it was going to take one hour and&nbsp; it's already 3 am and i want to finish this&nbsp;&nbsp;

- [01:06:00](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3960) video today, i am going to show the results of&nbsp; my previous training with exactly same data set&nbsp;&nbsp;

- [01:06:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3967) and exactly same settings and you are going to&nbsp; get this kind of output after generating grid&nbsp;&nbsp;

- [01:06:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3974) images. It is actually, let me see, 90 megabytes.&nbsp; So you see, these are the different checkpoints,&nbsp;&nbsp;

- [01:06:24](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3984) as you can see, and from these images you need to&nbsp; decide which one looks best. For example, i have&nbsp;&nbsp;

- [01:06:33](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=3993) picked in this example: testp-2400 steps count,&nbsp; which means from 10 training images, 240 epochs,&nbsp;&nbsp;

- [01:06:47](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4007) and i have generated a lot of images from this&nbsp; epoch and actually they are the ones that i have&nbsp;&nbsp;

- [01:06:54](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4014) shown in the beginning of the video, these ones.&nbsp; So these ones were generated from the testp-2400&nbsp;&nbsp;

- [01:07:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4027) steps, as you can see. Also, the name is&nbsp; written on the images description. And&nbsp;&nbsp;

- [01:07:12](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4032) show me one of the example and see how good it&nbsp; is. It is a 3d rendering of the person. We did&nbsp;&nbsp;

- [01:07:22](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4042) trained and you see the quality. This is the raw&nbsp; quality. I didn't upscale it or did anything and&nbsp;&nbsp;

- [01:07:28](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4048) it is just amazing. Let's just upscale it and&nbsp; see how it looks, in the bigger resolution.&nbsp;&nbsp;

- [01:07:34](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4054) Okay, to do that, let's go to the extras tab&nbsp; and in here i will drag and drop it one moment.&nbsp;&nbsp;

- [01:07:45](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4065) Okay, this image, okay, and then i am going to use&nbsp; R-ESRGAN 4x+ I find this the best one. Actually,&nbsp;&nbsp;

- [01:07:57](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4077) you can try also anime for this one,&nbsp; and let's just upscale it four times.&nbsp;&nbsp;

- [01:08:05](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4085) Okay, upscale is done. And look at the quality.&nbsp; It is just amazingly stylized quality and these&nbsp;&nbsp;

- [01:08:13](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4093) are the original images. You see how good&nbsp; it is. It is exactly the same person and&nbsp;&nbsp;

- [01:08:19](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4099) hundred percent stylized as we wanted. If&nbsp; you wanted some artist to draw this it,&nbsp;&nbsp;

- [01:08:25](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4105) i think the artist would draw as good as only like&nbsp; this, and i also didn't generate too much images&nbsp;&nbsp;

- [01:08:32](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4112) because i had little time. I have been doing&nbsp; a lot of research, experimentation to explain&nbsp;&nbsp;

- [01:08:37](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4117) to you everything in this video with as&nbsp; much as possible details. Now, how you can&nbsp;&nbsp;

- [01:08:45](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4125) combine multiple embeddings in the single query.&nbsp; Let's say you have trained on multiple person,&nbsp;&nbsp;

- [01:08:51](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4131) multiple object and you want to use them. Or you&nbsp; have trained multiple styles and you want to apply&nbsp;&nbsp;

- [01:08:57](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4137) them in the same query. It is just so easy. All&nbsp; you need to do is just type the names of them. So&nbsp;&nbsp;

- [01:09:08](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4148) if you add here like this, and you, if you had,&nbsp; if you add this one, they will be used both,&nbsp;&nbsp;

- [01:09:16](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4156) since these two are using the same tokens, their&nbsp; strength will be applied, both of their strength&nbsp;&nbsp;

- [01:09:25](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4165) will be applied, both of their weights and vectors&nbsp; will be applied. But if they were different,&nbsp;&nbsp;

- [01:09:32](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4172) embedding file, both of them would be&nbsp; applied. So this is how you use embeddings:&nbsp;&nbsp;

- [01:09:41](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4181) in the text to image tab. Hopefully, i plan i plan&nbsp; to work on an experiment on teaching a style and&nbsp;&nbsp;

- [01:09:50](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4190) object and make another video about them, but,&nbsp; the principles are same. It may just require to&nbsp;&nbsp;

- [01:09:57](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4197) select the prepare the good training data set. You&nbsp; see, this training data set is not even good. The&nbsp;&nbsp;

- [01:10:04](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4204) images are blurry, not high quality. The&nbsp; lightning is not very good. As you can see,&nbsp;&nbsp;

- [01:10:10](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4210) this is a blurry image actually, and this is also&nbsp; a blurry image and you will get the link of this&nbsp;&nbsp;

- [01:10:16](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4216) data set to see on your computer as well. However,&nbsp; even though these are not very good. The results&nbsp;&nbsp;

- [01:10:24](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4224) are just amazing. As you can see, the textual&nbsp; embeddings are very strong to teach faces as well,&nbsp;&nbsp;

- [01:10:31](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4231) and you can train do you can do training&nbsp; on official pruned or you can do training&nbsp;&nbsp;

- [01:10:38](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4238) on protogen, like a protogen, a custom, very&nbsp; good model or SD 2.1. And the one good side of&nbsp;&nbsp;

- [01:10:47](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4247) textual inversion than the DreamBooth is that, for&nbsp; example, i did DreamBooth training on protogen and&nbsp;&nbsp;

- [01:10:53](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4253) it was a failure. However, it was a great&nbsp; success for textual inversion. By the way,&nbsp;&nbsp;

- [01:11:00](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4260) the grid images will be saved under the outputs&nbsp; folder inside text to image grids like this. When&nbsp;&nbsp;

- [01:11:07](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4267) you do X/Y plot generation and regular outputs&nbsp; are saved in the text to image stuff like this.&nbsp;&nbsp;

- [01:11:14](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4274) And this is all for today. I hope you have&nbsp; enjoyed it. I have worked a lot for preparing&nbsp;&nbsp;

- [01:11:24](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4284) this tutorial. I have read a lot of technical&nbsp; documents. I have done a lot of research&nbsp;&nbsp;

- [01:11:30](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4290) and experimentation and please subscribe. If you&nbsp; join and support us, i appreciate it. Like the&nbsp;&nbsp;

- [01:11:38](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4298) video, share it and if you have any questions,&nbsp; just join our discord channel. To do that, go to&nbsp;&nbsp;

- [01:11:44](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4304) our about tab and in here you will see official&nbsp; discord channel. Just click it. And if you support&nbsp;&nbsp;

- [01:11:49](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4309) us on patreon. I would appreciate that very much.&nbsp; So far, we have 10 patrons and i thank them a&nbsp;&nbsp;

- [01:11:57](https://www.youtube.com/watch?v=dNOpWt-epdQ&t=4317) lot. They are keeping me to prepare more, better&nbsp; videos and, hopefully see you in another video.
