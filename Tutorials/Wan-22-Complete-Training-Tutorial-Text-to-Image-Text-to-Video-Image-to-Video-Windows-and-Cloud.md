# Wan 2.2 Complete Training Tutorial - Text to Image, Text to Video, Image to Video, Windows & Cloud

## Full tutorial link > https://www.youtube.com/watch?v=ocEkhAsPOs4

[![Wan 2.2 Complete Training Tutorial - Text to Image, Text to Video, Image to Video, Windows & Cloud](https://img.youtube.com/vi/ocEkhAsPOs4/sddefault.jpg)](https://www.youtube.com/watch?v=ocEkhAsPOs4 "Wan 2.2 Complete Training Tutorial - Text to Image, Text to Video, Image to Video, Windows & Cloud")

[![image](https://img.shields.io/discord/772774097734074388?label=Discord&logo=discord)](https://discord.com/servers/software-engineering-courses-secourses-772774097734074388) [![Hits](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/Wan-22-Complete-Training-Tutorial-Text-to-Image-Text-to-Video-Image-to-Video-Windows-and-Cloud.md.svg?style=plastic&label=Hits%20Since%2025.08.27&labelColor=007ec6&logo=SECourses)](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/Wan-22-Complete-Training-Tutorial-Text-to-Image-Text-to-Video-Image-to-Video-Windows-and-Cloud.md)
[![Patreon](https://img.shields.io/badge/Patreon-Support%20Me-F2EB0E?style=for-the-badge&logo=patreon)](https://www.patreon.com/c/SECourses) [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&logoColor=black)](https://www.buymeacoffee.com/DrFurkan) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/Medium-Follow%20Me-800080?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@furkangozukara) [![Codio](https://img.shields.io/static/v1?style=for-the-badge&message=Articles&color=4574E0&logo=Codio&logoColor=FFFFFF&label=CivitAI)](https://civitai.com/user/SECourses/articles) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/DeviantArt-Follow%20Me-990000?style=for-the-badge&logo=deviantart&logoColor=white)](https://www.deviantart.com/monstermmorpg)

[![YouTube Channel](https://img.shields.io/badge/YouTube-SECourses-C50C0C?style=for-the-badge&logo=youtube)](https://www.youtube.com/SECourses)  [![Furkan G√∂z√ºkara LinkedIn](https://img.shields.io/badge/LinkedIn-Follow%20Me-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/furkangozukara/)   [![Udemy](https://img.shields.io/static/v1?style=for-the-badge&message=Stable%20Diffusion%20Course&color=A435F0&logo=Udemy&logoColor=FFFFFF&label=Udemy)](https://www.udemy.com/course/stable-diffusion-dreambooth-lora-zero-to-hero/?referralCode=E327407C9BDF0CEA8156) [![Twitter Follow Furkan G√∂z√ºkara](https://img.shields.io/badge/Twitter-Follow%20Me-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/GozukaraFurkan)


Wan 2.2 training is now so easy. I have done over 64 different unique Wan 2.2 trainings to prepare the very best working training configurations for you. The configurations are fully working locally with as low as 6 GB GPUs. So you will be able to train your awesome Wan 2.2 image or video generation LoRAs on your Windows computer with easiness. Moreover, I have shown how to train on cloud platforms RunPod and Massed Compute so even if you have no GPU or you want faster training, you can train on cloud for very cheap prices fully privately.

üìÇ Resources & Links:

Download the One-Click Installer & Configs: [ [https://www.patreon.com/posts/Musubi-Tuner-Trainer-App-Configs-137551634](https://www.patreon.com/posts/Musubi-Tuner-Trainer-App-Configs-137551634) ]

Qwen Image Model Training Tutorial (Prerequisite): [ [https://youtu.be/DPX3eBTuO_Y](https://youtu.be/DPX3eBTuO_Y) ]

SwarmUI & ComfyUI Setup Guide for Windows: [ [https://youtu.be/c3gEoAyL2IE](https://youtu.be/c3gEoAyL2IE) ]

SwarmUI Installer and Model Downloader : [ [https://www.patreon.com/posts/SwarmUI-Install-Download-Models-114517862](https://www.patreon.com/posts/SwarmUI-Install-Download-Models-114517862) ]

ComfyUI Installer : [ [https://www.patreon.com/posts/ComfyUI-Installers-105023709](https://www.patreon.com/posts/ComfyUI-Installers-105023709) ]

SwarmUI & ComfyUI Setup Guide for RunPod & Massed Compute: [ [https://youtu.be/bBxgtVD3ek4](https://youtu.be/bBxgtVD3ek4) ]

Upload / Download Big Files Guide for RunPod & Massed Compute: [ [https://youtu.be/X5WVZ0NMaTg](https://youtu.be/X5WVZ0NMaTg) ]

‚è±Ô∏è Video Chapters:

[00:00:00](https://youtu.be/ocEkhAsPOs4?t=0) Introduction to Wan 2.2 Training & Capabilities

[00:00:56](https://youtu.be/ocEkhAsPOs4?t=56) Installing & Updating Musubi Tuner Locally

[00:02:20](https://youtu.be/ocEkhAsPOs4?t=140) Explanation of Optimized Presets & Research Logic

[00:04:00](https://youtu.be/ocEkhAsPOs4?t=240) Differences Between T2I, T2V, and I2V Configs

[00:05:36](https://youtu.be/ocEkhAsPOs4?t=336) Extracting Files & Running Update Batch File

[00:06:14](https://youtu.be/ocEkhAsPOs4?t=374) Downloading Wan 2.2 Training Models via Script

[00:07:30](https://youtu.be/ocEkhAsPOs4?t=450) Loading Configs: Selecting GPU & VRAM Options

[00:09:33](https://youtu.be/ocEkhAsPOs4?t=573) Using nvitop to Monitor RAM & VRAM Usage

[00:10:28](https://youtu.be/ocEkhAsPOs4?t=628) Preparing Image Dataset & Trigger Words

[00:11:17](https://youtu.be/ocEkhAsPOs4?t=677) Generating Dataset Config & Resolution Logic

[00:12:55](https://youtu.be/ocEkhAsPOs4?t=775) Calculating Epochs & Checkpoint Save Frequency

[00:13:40](https://youtu.be/ocEkhAsPOs4?t=820) Troubleshooting: Fixing Missing VAE Path Error

[00:15:12](https://youtu.be/ocEkhAsPOs4?t=912) VRAM Cache Behavior & Training Speed Analysis

[00:15:51](https://youtu.be/ocEkhAsPOs4?t=951) Trade-offs: Learning Rate vs Resolution vs Epochs

[00:16:29](https://youtu.be/ocEkhAsPOs4?t=989) Installing SwarmUI & Updating ComfyUI Backend

[00:18:13](https://youtu.be/ocEkhAsPOs4?t=1093) Importing Latest Presets into SwarmUI

[00:19:25](https://youtu.be/ocEkhAsPOs4?t=1165) Downloading Inference Models via Script

[00:20:33](https://youtu.be/ocEkhAsPOs4?t=1233) Generating Images with Trained Low Noise LoRA

[00:22:22](https://youtu.be/ocEkhAsPOs4?t=1342) Upscaling Workflow for High-Fidelity Results

[00:24:15](https://youtu.be/ocEkhAsPOs4?t=1455) Increasing Base Resolution to 1280x1280

[00:27:26](https://youtu.be/ocEkhAsPOs4?t=1646) Text-to-Video Generation with Lightning LoRA

[00:30:12](https://youtu.be/ocEkhAsPOs4?t=1812) Image-to-Video Generation Workflow & Settings

[00:31:35](https://youtu.be/ocEkhAsPOs4?t=1895) Restarting Backend to Clear VRAM for Model Switching

[00:33:45](https://youtu.be/ocEkhAsPOs4?t=2025) Fixing RAM Crashes with Cache-None Argument

[00:35:13](https://youtu.be/ocEkhAsPOs4?t=2113) Dual Model (High & Low Noise) Training Setup

[00:36:54](https://youtu.be/ocEkhAsPOs4?t=2214) Preparing Hybrid Datasets (Images + Videos)

[00:37:40](https://youtu.be/ocEkhAsPOs4?t=2260) Manually Editing Dataset TOML for Resolution Control

[00:39:53](https://youtu.be/ocEkhAsPOs4?t=2393) Setting High Noise Model Paths for Dual Training

[00:41:50](https://youtu.be/ocEkhAsPOs4?t=2510) Optimization: Block Swap vs CPU Offload

[00:43:10](https://youtu.be/ocEkhAsPOs4?t=2590) Generating Video with Dual-Model Trained LoRA

[00:45:35](https://youtu.be/ocEkhAsPOs4?t=2735) Massed Compute: Server Setup & Coupon Code

[00:47:00](https://youtu.be/ocEkhAsPOs4?t=2820) Connecting via ThinLinc & File Transfer Methods

[00:49:12](https://youtu.be/ocEkhAsPOs4?t=2952) Massed Compute: Fast UV Installation & Downloads

[00:50:27](https://youtu.be/ocEkhAsPOs4?t=3027) Loading Configurations on Massed Compute

[00:52:18](https://youtu.be/ocEkhAsPOs4?t=3138) Troubleshooting: Fixing Config Version Error

[00:53:20](https://youtu.be/ocEkhAsPOs4?t=3200) Dual Model Training Speed Analysis on Cloud

[00:55:40](https://youtu.be/ocEkhAsPOs4?t=3340) RunPod: Selecting the Correct Template & GPU

[00:57:45](https://youtu.be/ocEkhAsPOs4?t=3465) RunPod: Uploading Files & Extracting Archive

[00:58:38](https://youtu.be/ocEkhAsPOs4?t=3518) RunPod: Terminal Installation & Model Downloads

[01:00:26](https://youtu.be/ocEkhAsPOs4?t=3626) RunPod: Correct Pathing Syntax & Backslash Fix

[01:01:28](https://youtu.be/ocEkhAsPOs4?t=3688) Setting Dataset Paths on RunPod

[01:03:34](https://youtu.be/ocEkhAsPOs4?t=3814) Installing nvitop on RunPod Terminal

[01:03:54](https://youtu.be/ocEkhAsPOs4?t=3834) Speed Hack: Disabling Numpy Memory Mapping

[01:06:00](https://youtu.be/ocEkhAsPOs4?t=3960) Terminating Instances & Final Remarks

Greetings everyone! Today I am presenting an epic tutorial on how to train the Wan 2.2 model to generate extremely high-quality, realistic images and videos. This is currently the most advanced model for generating life-like textures and details.

In this comprehensive guide, I cover everything you need to know to train Wan 2.2 on your local Windows computer, as well as on cloud platforms like RunPod and Massed Compute. We utilize the SECourses Musubi Tuner with fully optimized, 1-click presets designed for every GPU range (from 6GB to 192GB VRAM).

üöÄ What You Will Learn in This Tutorial:

Wan 2.2 Text-to-Image Training: How to train the Low Noise model for massive detail and realism.

Wan 2.2 Text-to-Video Training: Mastering Dual Model training (Low Noise + High Noise) for superior video consistency.

Image-to-Video Workflow: How to use your trained LoRAs to animate static images.

Cloud Training: Step-by-step guides for Massed Compute (ultra-fast disk speeds) and RunPod.

Performance Optimization: Using FP8 scaling, Block Swapping, and CPU offloading to train on consumer GPUs.

Inference & Upscaling: Using SwarmUI and ComfyUI to generate and upscale content to 4K resolution.

üí° Key Features of Our Workflow:

Auto-Resume & Speed: New UV package installers for lightning-fast setup.

Presets for All GPUs: Configurations included for 6GB, 12GB, 24GB, 48GB, and 80GB+ cards.

Dataset Automation: Auto-resizing and captioning for both image and video datasets.



### Video Transcription


- [00:00:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=0) Greetings everyone. Today I am going to show&nbsp; you Wan 2.2 trainings. This will be an epic&nbsp;&nbsp;

- [00:00:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=8) tutorial. I will cover so many topics, so check&nbsp; out the video descriptions to learn all. So,&nbsp;&nbsp;

- [00:00:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=14) what I am going to show you today? I will show&nbsp; you how to train Wan 2.2 model to generate&nbsp;&nbsp;

- [00:00:22](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=22) images like these ones. You see, these are&nbsp; extremely high quality and extremely realistic,&nbsp;&nbsp;

- [00:00:28](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=28) with massive amount of details, textures,&nbsp; amazing quality, and amazing realism. The&nbsp;&nbsp;

- [00:00:35](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=35) Wan 2.2 is currently the most realistic plus&nbsp; most advanced model for generating images.

- [00:00:44](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=44) But I will not show only how to train images.&nbsp; I will show also how to generate videos,&nbsp;&nbsp;

- [00:00:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=50) how to train video model, so many things. I&nbsp; will show how to train on your local computer&nbsp;&nbsp;

- [00:00:56](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=56) with our SECourses Musubi Tuner application.&nbsp; I have prepared configurations for every GPU,&nbsp;&nbsp;

- [00:01:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=64) starting from 6 GB GPUs to 192 GB GPUs. The&nbsp; configurations are all set. They are separated&nbsp;&nbsp;

- [00:01:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=72) into qualities, so you can pick your configuration&nbsp; according to your GPU and start training right&nbsp;&nbsp;

- [00:01:19](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=79) away and get the most amazing results without&nbsp; doing all the research that I did for you.

- [00:01:25](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=85) I have literally done over 64 separate&nbsp; trainings to find out the best workflow,&nbsp;&nbsp;

- [00:01:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=93) find out the best parameters, and prepare these&nbsp; presets. I have used a cloud machine with 8 B200&nbsp;&nbsp;

- [00:01:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=101) GPUs to complete these trainings, analyze them.&nbsp; Moreover, I have developed this Gradio based&nbsp;&nbsp;

- [00:01:46](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=106) application that you will be able to load the&nbsp; configuration and start training right away.&nbsp;&nbsp;

- [00:01:52](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=112) Furthermore, I will show how to do training on&nbsp; RunPod. So if you don't have a powerful GPU,&nbsp;&nbsp;

- [00:01:57](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=117) you will be able to train on RunPod. And I will&nbsp; show how to do training on Massed Compute. Again,&nbsp;&nbsp;

- [00:02:02](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=122) if you don't have a powerful GPU, you will be&nbsp; able to train on Massed Compute. For example,&nbsp;&nbsp;

- [00:02:07](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=127) you see with this GPU we are able to train with&nbsp; like 70 minutes with only 2 dollars in 1 hour.

- [00:02:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=134) Everything is ready. Some people were asking&nbsp; me how to train on RunPod and Massed Compute&nbsp;&nbsp;

- [00:02:19](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=139) the Qwen image models. This tutorial&nbsp; covers the Wan 2.2 training. However,&nbsp;&nbsp;

- [00:02:25](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=145) all of the models that we support in SECourses&nbsp; Musubi Tuner are exactly same. So you can train&nbsp;&nbsp;

- [00:02:31](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=151) the Qwen image models, Wan models, or the future&nbsp; models hopefully that will get added like Flux 2,&nbsp;&nbsp;

- [00:02:37](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=157) like Z Image base model, will be&nbsp; exactly same as in this tutorial.

- [00:02:42](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=162) Moreover, I have updated all of our presets&nbsp; to the highest quality with lowest amount of&nbsp;&nbsp;

- [00:02:49](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=169) time to generate highest quality images and&nbsp; videos. So all the image generation and video&nbsp;&nbsp;

- [00:02:56](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=176) generation presets are updated and ready&nbsp; for you to use. To update these presets,&nbsp;&nbsp;

- [00:03:01](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=181) I have done so many different tests. I have&nbsp; compared all the results of these tests to&nbsp;&nbsp;

- [00:03:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=188) generate videos or images. So many tests have&nbsp; been made. This tutorial is a product of a massive&nbsp;&nbsp;

- [00:03:15](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=195) research. So many different parameter testing, so&nbsp; many different workflow testing, and all is ready&nbsp;&nbsp;

- [00:03:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=200) for you to use right away with 1 click installers.&nbsp; So you see from bad videos to good videos.

- [00:03:26](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=206) This tutorial covers text to image&nbsp; training with only from images,&nbsp;&nbsp;

- [00:03:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=210) text to video training with only images or&nbsp; plus videos, but videos are not necessary,&nbsp;&nbsp;

- [00:03:36](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=216) or image to video training. I have done so many&nbsp; testing and training only text to video yields the&nbsp;&nbsp;

- [00:03:43](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=223) best results for text to video generation or image&nbsp; to video generation. The presets are separated,&nbsp;&nbsp;

- [00:03:49](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=229) so if you are not interested in video generation,&nbsp; you can only use the text to image generation&nbsp;&nbsp;

- [00:03:55](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=235) workflow. Still, it is working on text to video&nbsp; generation or image to video generation as well.

- [00:04:01](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=241) So all these configs are interchangeable. There&nbsp;&nbsp;

- [00:04:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=244) are several tricks. If you want to&nbsp; generate highest quality images,&nbsp;&nbsp;

- [00:04:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=248) you use the text to image configs. If you&nbsp; want to generate highest quality videos,&nbsp;&nbsp;

- [00:04:13](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=253) you use Wan 2.2 text to video configs. If you want&nbsp; to generate image to video, still I recommend to&nbsp;&nbsp;

- [00:04:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=260) use text to video configs because this is working&nbsp; better than image to video. It is weird, I know,&nbsp;&nbsp;

- [00:04:25](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=265) but this is my findings. How do I know? Because&nbsp; I have done so many different testings as you&nbsp;&nbsp;

- [00:04:32](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=272) are seeing. This is a research of a week doing&nbsp; a lot of comparisons, doing a lot of testing,&nbsp;&nbsp;

- [00:04:38](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=278) experimentation, and all the presets are ready for&nbsp; you to use right away with the highest quality.

- [00:04:45](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=285) So I will show everything: how to install, how&nbsp; to set up, how to start training both Windows,&nbsp;&nbsp;

- [00:04:51](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=291) RunPod, Massed Compute. And everything is&nbsp; literally with 1 click install. Let's begin.&nbsp;&nbsp;

- [00:04:56](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=296) So as usual, I have prepared an amazing post&nbsp; where you will find everything to follow this&nbsp;&nbsp;

- [00:05:02](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=302) tutorial. However, before starting this tutorial,&nbsp; you need to watch this main tutorial. So open&nbsp;&nbsp;

- [00:05:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=308) this tutorial. This tutorial is our Qwen image&nbsp; models training tutorial. This is a masterpiece.&nbsp;&nbsp;

- [00:05:16](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=316) This tutorial covers so many different topics&nbsp; as you are seeing right now. So I recommend&nbsp;&nbsp;

- [00:05:22](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=322) you to watch this tutorial and learn how to use&nbsp; SECourses Kohya Musubi Tuner premium application.

- [00:05:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=330) So I will download the latest zip file. This&nbsp; zip file includes our latest installers and&nbsp;&nbsp;

- [00:05:36](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=336) configurations. I will move it into my existing&nbsp; installation folder. Then I will extract the&nbsp;&nbsp;

- [00:05:43](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=343) files here and I will overwrite existing&nbsp; files. Don't forget that you can also do a&nbsp;&nbsp;

- [00:05:48](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=348) fresh installation. After extraction, double&nbsp; click Windows install and update.bat file.&nbsp;&nbsp;

- [00:05:55](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=355) This will update the application to the latest&nbsp; version. Moreover, now we are using uv package&nbsp;&nbsp;

- [00:06:01](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=361) installer along with pip, therefore it is ultra&nbsp; fast. So you see my update already completed.

- [00:06:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=368) Then double click Windows download training&nbsp; models.bat file. This file will ask you download&nbsp;&nbsp;

- [00:06:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=374) which training models. For this tutorial, we need&nbsp; Wan 2.2 text to video training. If you want to&nbsp;&nbsp;

- [00:06:21](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=381) train Wan 2.2 image to video, then you need to&nbsp; download it with option 5. Since I already have&nbsp;&nbsp;

- [00:06:27](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=387) downloaded the models, it will just verify their&nbsp; hash values, and once the verification completed,&nbsp;&nbsp;

- [00:06:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=393) we will be ready. This way we are ensuring&nbsp; that our models are 100 percentage accurately&nbsp;&nbsp;

- [00:06:39](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=399) downloaded, so we will never have any weird bugs,&nbsp; issues, or problems. If the files were missing,&nbsp;&nbsp;

- [00:06:46](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=406) it would have downloaded them with the maximum&nbsp; speed with your network connection support.

- [00:06:51](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=411) So now we are ready to begin training. I will&nbsp; double click Windows start up.bat file. This&nbsp;&nbsp;

- [00:06:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=418) will start the latest version of the SECourses&nbsp; Kohya Musubi Tuner premium application. You see&nbsp;&nbsp;

- [00:07:05](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=425) currently we are at the version 24. This is&nbsp; where you see the versioning. Then go to Wan&nbsp;&nbsp;

- [00:07:11](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=431) models training. So we are going to use this&nbsp; tab. Whenever you are going to reload a model,&nbsp;&nbsp;

- [00:07:16](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=436) I recommend to refresh, go to the accurate tab,&nbsp; and then begin loading. Then click this icon&nbsp;&nbsp;

- [00:07:23](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=443) and go back to your installation folder, enter&nbsp; inside Wan 2.2 training configs. Now there are&nbsp;&nbsp;

- [00:07:29](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=449) 3 configuration folders: Wan 2.2 image to video,&nbsp; Wan 2.2 text to image, and Wan 2.2 text to video.

- [00:07:37](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=457) So what are the differences between these 3? From&nbsp; the naming as you can see, Wan 2.2 image to video&nbsp;&nbsp;

- [00:07:44](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=464) trains image to video model. However, there&nbsp; is only single configuration because if you&nbsp;&nbsp;

- [00:07:49](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=469) want to only generate images, it is not the&nbsp; model that you need. This model is for only&nbsp;&nbsp;

- [00:07:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=474) generating videos. However, I don't recommend to&nbsp; use this model for training even if you are going&nbsp;&nbsp;

- [00:07:59](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=479) to use image to video models because training&nbsp; with Wan 2.2 text to video works better than&nbsp;&nbsp;

- [00:08:06](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=486) training with Wan 2.2 image to video, even&nbsp; for image to video presets. Even for image&nbsp;&nbsp;

- [00:08:13](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=493) to video generation. So for video generation, I&nbsp; recommend to always use Wan 2.2 text to video,&nbsp;&nbsp;

- [00:08:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=500) and for only image generation, I&nbsp; recommend to use Wan 2.2 text to image.

- [00:08:24](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=504) So let's begin with Wan 2.2 text to image. Now&nbsp; you will see bunch of configurations like this.&nbsp;&nbsp;

- [00:08:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=510) The first numbers are for GPU&nbsp; VRAM. So if you have 6 GB of GPU,&nbsp;&nbsp;

- [00:08:36](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=516) you need to pick it. If you have 24 GB of GPU,&nbsp; then you need to pick it. It also shows the&nbsp;&nbsp;

- [00:08:42](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=522) quality. The quality 1 is the maximum quality.&nbsp; Then the quality as the number increases the&nbsp;&nbsp;

- [00:08:48](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=528) quality get worse. So quality 1 is better than&nbsp; quality 2, quality 2 better than quality 3,&nbsp;&nbsp;

- [00:08:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=534) and so on. Since I have RTX 5090, I am going&nbsp; to use 32 GB VRAM configuration. However,&nbsp;&nbsp;

- [00:09:02](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=542) you will see that there are 2 versions: 32&nbsp; GB of FP8 scaled and no FP8 scaled. The FP8&nbsp;&nbsp;

- [00:09:09](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=549) scaled has a big advantage. It is usually faster&nbsp; because it uses lesser VRAM. Moreover, it uses&nbsp;&nbsp;

- [00:09:16](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=556) much lesser RAM memory. So if your RAM memory is&nbsp; limited, then always go with FP8 scaled version.

- [00:09:23](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=563) So let's open nvitop to see our GPU and RAM&nbsp; memory. pip install nvitop. nvitop. And this is&nbsp;&nbsp;

- [00:09:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=573) my current usages. So I have 60 GB of empty RAM&nbsp; memory. My first GPU is fully empty. Therefore,&nbsp;&nbsp;

- [00:09:42](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=582) I will pick the very best configuration inside&nbsp; Wan 2.2 trainings. Wan 2.2 text to image 32 GB.&nbsp;&nbsp;

- [00:09:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=590) And it is loaded. You can also click this icon&nbsp; to be sure it is loaded. Then it is same as in&nbsp;&nbsp;

- [00:09:57](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=597) our Qwen image training tutorial. What I am&nbsp; going to change here is the training dataset.

- [00:10:02](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=602) For text to image training, you only need to use&nbsp; the images. For text to video training, if you&nbsp;&nbsp;

- [00:10:09](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=609) only use images, it is still working perfect as&nbsp; I have shown in the beginning of the tutorial.&nbsp;&nbsp;

- [00:10:15](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=615) But you can also include videos. I will show&nbsp; that later. So I will pick my training dataset,&nbsp;&nbsp;

- [00:10:21](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=621) but first let's prepare it. This is exactly same&nbsp; logic as in the Qwen images, so this is my folder.&nbsp;&nbsp;

- [00:10:28](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=628) I will delete the other files to show you what&nbsp; happens as a quick rewinding. But watch the Qwen&nbsp;&nbsp;

- [00:10:35](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=635) image training tutorial. So this is my trigger&nbsp; word ohwx and this is my parent path. Don't worry,&nbsp;&nbsp;

- [00:10:42](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=642) it will automatically resize them. So this is&nbsp; it. I copy pasted, I can also pick from here.

- [00:10:48](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=648) The best working resolution is 960 to 960.&nbsp; How do I know? I have literally tested so&nbsp;&nbsp;

- [00:10:56](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=656) many different configurations. So when you&nbsp; see these configurations 1024, 1280, 1328,&nbsp;&nbsp;

- [00:11:05](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=665) these are the resolutions that I have tested.&nbsp; And the best yielding resolution is 960 to 960.&nbsp;&nbsp;

- [00:11:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=672) Therefore, I recommend that. Then don't forget&nbsp; to click generate dataset configuration and&nbsp;&nbsp;

- [00:11:17](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=677) it is ready. So when you return back to your&nbsp; images folder, you should see the txt files.&nbsp;&nbsp;

- [00:11:23](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=683) These are the captions and when you open&nbsp; one of them you will see only the folder&nbsp;&nbsp;

- [00:11:27](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=687) name is used. If you want to use custom&nbsp; captions, you need to change these files.

- [00:11:32](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=692) Okay, our training dataset is ready. Now we need&nbsp; to set our output and other stuff same as in the&nbsp;&nbsp;

- [00:11:38](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=698) Qwen image training tutorial. So I will save&nbsp; them into my test trainings folder like this&nbsp;&nbsp;

- [00:11:44](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=704) where the models will be saved. This is the model&nbsp; LoRA models file names. Currently fine tuning is&nbsp;&nbsp;

- [00:11:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=710) not supported so we can only train LoRA. Okay,&nbsp; then the model settings. You need to pick the&nbsp;&nbsp;

- [00:11:55](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=715) files for text to image training, we are only use&nbsp; the low noise. There are 2 versions of Wan 2.2:&nbsp;&nbsp;

- [00:12:02](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=722) high noise and low noise. High noise is the&nbsp; initial generation which roughly generates&nbsp;&nbsp;

- [00:12:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=728) the base of the image or video. The low noise&nbsp; is the details. For generating only images,&nbsp;&nbsp;

- [00:12:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=734) we are only using low noise. This is the higher&nbsp; quality. So I will pick the accurate models from&nbsp;&nbsp;

- [00:12:21](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=741) downloaded models training models Wan. So&nbsp; this is the low noise text to video model.&nbsp;&nbsp;

- [00:12:27](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=747) Like this. Then since other files are there, I&nbsp; will just copy paste the folder paths like this,&nbsp;&nbsp;

- [00:12:34](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=754) like this. But you can individually pick them.&nbsp; We are not going to use Clip Vision, this is not&nbsp;&nbsp;

- [00:12:38](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=758) used for Wan 2.2 training. And you see there is&nbsp; high noise. This is used for dual model training&nbsp;&nbsp;

- [00:12:44](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=764) which I will explain after this. So you don't need&nbsp; to change anything else here. Don't change them.

- [00:12:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=770) In the training settings, you need to set your&nbsp; training epochs as I have explained. Since I&nbsp;&nbsp;

- [00:12:55](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=775) have 28 images, I am going to use 200 epochs. If&nbsp; you have 100 images, then you can reduce this to&nbsp;&nbsp;

- [00:13:02](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=782) 100. That is the logic. But up to 50 images, I&nbsp; recommend to train at least 200 epochs. Moreover,&nbsp;&nbsp;

- [00:13:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=788) there is save every N epochs. So we are saving&nbsp; every 20 epochs. It will save 10 checkpoints that&nbsp;&nbsp;

- [00:13:15](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=795) I can compare later. Then don't forget to save&nbsp; your configuration. So let's save this as like&nbsp;&nbsp;

- [00:13:22](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=802) this. LoRA tutorial text to image TOML. Okay, I&nbsp; will save it. Then I will click start training.

- [00:13:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=810) Then follow the CMD window. If you get&nbsp; any errors, you need to report me that.&nbsp;&nbsp;

- [00:13:36](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=816) So currently I have an error. Let's see where we&nbsp; have the error. Maybe we forgot something. Yes,&nbsp;&nbsp;

- [00:13:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=821) I have forgotten the VAE path. This can happen.&nbsp; So let's open all the panels like this and&nbsp;&nbsp;

- [00:13:49](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=829) search for VAE. Okay, the VAE is set here.&nbsp; Okay, I can see the error that I have made,&nbsp;&nbsp;

- [00:13:55](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=835) so I need to fix it. It will be like this. And&nbsp; this is also error. Okay, it will be like this.&nbsp;&nbsp;

- [00:14:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=840) So pay attention to the model paths save. You can&nbsp; use the folder icon start training. And you see&nbsp;&nbsp;

- [00:14:07](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=847) we are also supporting torch compile. I am using&nbsp; your Visual Studio installation automatically.&nbsp;&nbsp;

- [00:14:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=854) Therefore, it is super important for you to&nbsp; follow the requirements tutorial which you&nbsp;&nbsp;

- [00:14:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=860) must. So the requirements tutorial are listed&nbsp; here in the post. Follow this requirements&nbsp;&nbsp;

- [00:14:26](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=866) tutorial first. When you follow the Qwen image&nbsp; models training, you will know this already.

- [00:14:31](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=871) Okay, let's follow our training. So now&nbsp; it is loading model into the RAM memory,&nbsp;&nbsp;

- [00:14:36](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=876) then it will load into the GPU memory. If you&nbsp; don't have sufficient amount of RAM memory,&nbsp;&nbsp;

- [00:14:42](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=882) training both models, both low and high noise,&nbsp; is harder. And it is even not mandatory. You can&nbsp;&nbsp;

- [00:14:48](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=888) also train only low noise model and even generate&nbsp; videos. It will be a little bit lower quality,&nbsp;&nbsp;

- [00:14:55](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=895) but it will work. Don't worry. So the training&nbsp; is about to start. Yes, training starting. Okay,&nbsp;&nbsp;

- [00:15:01](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=901) training started. It is not using my entire GPU&nbsp; memory. This is good. You need to have some free&nbsp;&nbsp;

- [00:15:07](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=907) memory. You will also see that your VRAM usage&nbsp; is not static like this. This is because how&nbsp;&nbsp;

- [00:15:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=912) Kohya developed the Wan 2.2 model training. It&nbsp; clears the VRAM cache after every step because&nbsp;&nbsp;

- [00:15:19](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=919) it is designed to be work with both models at&nbsp; the same time during training. So the speed&nbsp;&nbsp;

- [00:15:25](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=925) is around 5 seconds per IT. It will get a little&nbsp; bit slower. This is how Kohya displays currently.&nbsp;&nbsp;

- [00:15:31](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=931) But it is pretty decent. It is taking like 8&nbsp; hours on my GPU. If I need faster training,&nbsp;&nbsp;

- [00:15:37](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=937) then I can reduce the resolution or I can increase&nbsp; the learning rate and I can do lesser epoch. So&nbsp;&nbsp;

- [00:15:44](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=944) let's say you want 100 epochs, then you need to&nbsp; multiply your learning rate with 2.5 and reduce&nbsp;&nbsp;

- [00:15:51](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=951) the maximum epochs number. However, you will&nbsp; lose some quality. So this set learning rate&nbsp;&nbsp;

- [00:15:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=958) and number of epochs are the best overall. It is&nbsp; a choice. It is a trade off. Either you need to&nbsp;&nbsp;

- [00:16:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=964) reduce the resolution to speed up or you need to&nbsp; lower the epoch count and increase your learning&nbsp;&nbsp;

- [00:16:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=972) rate. So this will train text to image LoRAs for&nbsp; us which you can use for text to video as well.

- [00:16:18](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=978) How we are going to use the trained models?&nbsp; I will stop the training. So for using the&nbsp;&nbsp;

- [00:16:23](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=983) trained models, we are going to use SwarmUI. The&nbsp; SwarmUI link is here. So let's also proceed with&nbsp;&nbsp;

- [00:16:29](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=989) it. If you don't know how to use SwarmUI&nbsp; already, you need to follow this tutorial.&nbsp;&nbsp;

- [00:16:34](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=994) You see this is the tutorial link in&nbsp; the top of the post. When you open it,&nbsp;&nbsp;

- [00:16:40](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1000) copy the zip file there. So let's download the&nbsp; latest zip file from here. Then I will move it&nbsp;&nbsp;

- [00:16:46](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1006) into my existing SwarmUI installation like this.&nbsp; Copy the file there. Right click and extract files&nbsp;&nbsp;

- [00:16:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1014) here and overwrite all. Then as a next step, we&nbsp; will update our ComfyUI installation. So go to&nbsp;&nbsp;

- [00:17:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1020) ComfyUI link from here. Download the latest&nbsp; zip file. You see there are 2 versions with&nbsp;&nbsp;

- [00:17:06](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1026) password protected one. If your antivirus, this&nbsp; happens with Windows Defender, causes issues,&nbsp;&nbsp;

- [00:17:13](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1033) download with the password protected, then move&nbsp; your ComfyUI zip file into your folder or you can&nbsp;&nbsp;

- [00:17:19](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1039) make a fresh installation. As usual, make an file&nbsp; extraction overwrite all files. Then to update or&nbsp;&nbsp;

- [00:17:27](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1047) install Windows install or update comfyui.bat&nbsp; file. We have moved the installation to uv&nbsp;&nbsp;

- [00:17:34](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1054) packages. So now it is much faster than before.&nbsp; Both update and installation. So you see it is&nbsp;&nbsp;

- [00:17:40](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1060) all updated. Then return back to your SwarmUI. In&nbsp; the SwarmUI, we have Windows update swarmui. So&nbsp;&nbsp;

- [00:17:48](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1068) double click it run. It will update SwarmUI and&nbsp; start your application. Everything was already&nbsp;&nbsp;

- [00:17:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1074) explained in other tutorials in more details,&nbsp; so you should watch them. Okay, SwarmUI update&nbsp;&nbsp;

- [00:18:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1080) succeed and it started. You will also get this&nbsp; error. This is not important, just ignore that.

- [00:18:06](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1086) First of all, you need to get the latest presets.&nbsp; So you can use import, choose file, go back to&nbsp;&nbsp;

- [00:18:13](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1093) your SwarmUI installation and select the latest&nbsp; preset from here. Then overwrite existing presets.&nbsp;&nbsp;

- [00:18:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1100) Alternatively, while the application is running,&nbsp; you can use the preset delete import. This will&nbsp;&nbsp;

- [00:18:26](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1106) delete your existing presets and just refresh it.&nbsp; Then click refresh and everything is here. The&nbsp;&nbsp;

- [00:18:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1113) presets are important because I have updated the&nbsp; presets for video generation for Wan 2.2 and also&nbsp;&nbsp;

- [00:18:40](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1120) image generation. So use the latest presets. Now&nbsp; how we are going to use our trained Wan 2.2 image&nbsp;&nbsp;

- [00:18:48](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1128) generation LoRAs. So first of all, quick tools&nbsp; reset params to default. Then from the presets,&nbsp;&nbsp;

- [00:18:53](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1133) the preset that you need to use is Wan 2.2&nbsp; generate realistic images. Click here and&nbsp;&nbsp;

- [00:18:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1138) direct apply. This will let you generate 960&nbsp; to 960 images. So let's type our prompt. Select&nbsp;&nbsp;

- [00:19:07](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1147) your aspect ratio whichever you want. For example,&nbsp; this one. And then you need to select your LoRA.

- [00:19:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1154) But there is one more thing. You need to have&nbsp; models downloaded for this preset to work. So&nbsp;&nbsp;

- [00:19:19](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1159) how you are going to download accurate models?&nbsp; You see Windows start download models app.bat&nbsp;&nbsp;

- [00:19:25](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1165) file. This will start the model downloader&nbsp; latest version. We have upgraded this. Also&nbsp;&nbsp;

- [00:19:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1170) it is using uv package. So now it is even faster&nbsp; to start. Okay, the downloader started. So which&nbsp;&nbsp;

- [00:19:37](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1177) models you need to download? You can download&nbsp; complete image generation and editing bundle.&nbsp;&nbsp;

- [00:19:42](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1182) Or you can download Wan 2.2 core 4 steps&nbsp; bundle. This is my recommended bundle. So&nbsp;&nbsp;

- [00:19:49](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1189) go to bottom and download all models inside&nbsp; this bundle. This way you will have all the&nbsp;&nbsp;

- [00:19:56](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1196) models that you need. Even the upscaler models&nbsp; that you need to use to get maximum quality.&nbsp;&nbsp;

- [00:20:03](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1203) You can follow the download process from the&nbsp; CMD window. So if I am missing any models,&nbsp;&nbsp;

- [00:20:09](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1209) they will get downloaded. If I already have some&nbsp; models, they will be verified and it will skip&nbsp;&nbsp;

- [00:20:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1214) the download. So currently it is downloading&nbsp; some of my missing models, but I have the&nbsp;&nbsp;

- [00:20:19](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1219) necessary models for image generation which is&nbsp; Wan 2.2 text to video low noise. So it is set.

- [00:20:26](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1226) Now I need to set my LoRA. So these are the LoRAs&nbsp; that it finds. It is compatible with Wan 2.2. My&nbsp;&nbsp;

- [00:20:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1233) trained LoRA is inside here. Wan 2.2 low noise.&nbsp; This is my trained LoRA. This is 200 epochs&nbsp;&nbsp;

- [00:20:42](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1242) training results. As in the Qwen image training&nbsp; tutorial, you should compare your checkpoints&nbsp;&nbsp;

- [00:20:47](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1247) with grid system and verify which model,&nbsp; which checkpoint is the best. Like 20 epochs,&nbsp;&nbsp;

- [00:20:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1254) 40 epochs, 60, 80. So this is 200 epochs. And&nbsp; let's generate 4 random images. Currently this&nbsp;&nbsp;

- [00:21:01](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1261) is using our ComfyUI installation. It&nbsp; was updated. It is using Sage Attention,&nbsp;&nbsp;

- [00:21:07](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1267) my first GPU. Everything was previously&nbsp; explained in other tutorials. Moreover,&nbsp;&nbsp;

- [00:21:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1272) you should follow your VRAM usage from nvitop&nbsp; to see what is happening. I have the sufficient&nbsp;&nbsp;

- [00:21:17](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1277) amount of RAM memory and VRAM. If your GPU&nbsp; is weak like 6 GB GPU, don't worry. Since&nbsp;&nbsp;

- [00:21:23](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1283) we are using the ComfyUI, it is automatically&nbsp; doing block swapping and also CPU offloading,&nbsp;&nbsp;

- [00:21:29](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1289) RAM offloading. Therefore, no matter what GPU&nbsp; you have, it will work. This is the beauty of&nbsp;&nbsp;

- [00:21:35](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1295) the SwarmUI with ComfyUI backend. So it is&nbsp; doing all the optimizations that you need.

- [00:21:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1301) The image generation in the base resolution will&nbsp; be pretty fast as you are seeing right now. Okay,&nbsp;&nbsp;

- [00:21:46](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1306) this was the first image. The face is not that&nbsp; great. We should do either face inpainting or&nbsp;&nbsp;

- [00:21:51](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1311) we are going to do upscaling which I will show&nbsp; in a moment. However, this took only 45 seconds.&nbsp;&nbsp;

- [00:21:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1318) You can also reduce the number of steps if you&nbsp; want. But this model is not a small model. This&nbsp;&nbsp;

- [00:22:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1324) is a big model. 14 billion parameters. This&nbsp; model is much more powerful than the Z Image&nbsp;&nbsp;

- [00:22:11](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1331) turbo model if you ask my opinion. Okay, this is&nbsp; another image. So generate few images like this.&nbsp;&nbsp;

- [00:22:16](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1336) Okay. Let's say I liked this image. So I will&nbsp; click reuse parameters. It will set its seed.&nbsp;&nbsp;

- [00:22:22](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1342) Then I will go to presets and I will apply the our&nbsp; upscale. You see there is 2x upscale direct apply.&nbsp;&nbsp;

- [00:22:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1350) And I will regenerate. Let's see the difference.&nbsp; Furthermore, you can change the base resolution&nbsp;&nbsp;

- [00:22:36](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1356) of this model. Up to 1280 to 1280. 1280 to 1280.&nbsp; This model is able to generate images. Sometimes&nbsp;&nbsp;

- [00:22:46](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1366) it may add some hallucinations to the right and&nbsp; left border. But you can increase the resolution,&nbsp;&nbsp;

- [00:22:52](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1372) the base resolution to 1280 to 1280. I will&nbsp; show after this how to do that. But let's&nbsp;&nbsp;

- [00:22:59](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1379) see the upscaled result. So as a reminding, let&nbsp; me remind you where you put your trained LoRAs.&nbsp;&nbsp;

- [00:23:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1384) They are put inside SwarmUI inside models inside&nbsp; LoRA folder. This is where you need to put your&nbsp;&nbsp;

- [00:23:10](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1390) trained LoRAs. Inside LoRA folder. Okay, now&nbsp; it is upscaling. The upscaling will take more&nbsp;&nbsp;

- [00:23:15](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1395) time obviously because we are doubling the image&nbsp; resolution. So as I said, don't worry if you don't&nbsp;&nbsp;

- [00:23:22](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1402) have a powerful GPU, it will still work with&nbsp; automatic block swapping and CPU offloading.&nbsp;&nbsp;

- [00:23:28](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1408) Everything will be handled by the ComfyUI.&nbsp; No matter what GPU you have, it will work.

- [00:23:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1413) And here it is. So let's make a comparison.&nbsp; This was the base image. You see this was the&nbsp;&nbsp;

- [00:23:38](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1418) base image. Then this is the upscaled image.&nbsp; As you can see, it fixed the face. It added&nbsp;&nbsp;

- [00:23:44](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1424) huge amount of details, quality. There is a&nbsp; significant difference between the base image&nbsp;&nbsp;

- [00:23:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1430) and the generated upscaled image. Again, as I&nbsp; said, you can increase the base resolution even&nbsp;&nbsp;

- [00:23:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1438) up to actually 1920 to 1080. However, it may add&nbsp; some hallucinations. Let's try this and to see&nbsp;&nbsp;

- [00:24:06](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1446) the difference. But in my experimentation what I&nbsp; did was I selected my model which is low noise.&nbsp;&nbsp;

- [00:24:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1454) So here low noise. So click this hamburger menu,&nbsp; edit metadata, and change this to 1280 1280. Okay,&nbsp;&nbsp;

- [00:24:23](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1463) like this. Save. Then select another model and&nbsp; select your model. It will update resolutions&nbsp;&nbsp;

- [00:24:29](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1469) accordingly. So when you select your aspect ratio&nbsp; now, it will set it accordingly to 1280 to 1280&nbsp;&nbsp;

- [00:24:37](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1477) base resolution. These are all optional. You can&nbsp; do whichever you want. And generate another image&nbsp;&nbsp;

- [00:24:43](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1483) with 1280 to 1280 total resolution. The aspect&nbsp; ratio is like this. But let's see also 1920 to&nbsp;&nbsp;

- [00:24:51](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1491) 1080 result as well. Okay. Yeah, not bad.&nbsp; Not as good as upscale but not bad at all.&nbsp;&nbsp;

- [00:24:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1498) Now with this 1280 to 1280 base resolution, if I&nbsp; also do upscale which I did in the example images,&nbsp;&nbsp;

- [00:25:07](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1507) it will become much better. For example, let&nbsp; me show you from the history row and Wan 2.2.&nbsp;&nbsp;

- [00:25:15](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1515) I have shown in the beginning of the tutorial but&nbsp; let's see again. So you see these images are huge,&nbsp;&nbsp;

- [00:25:21](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1521) massive. If I open, this is 3360 to 1920 pixels.&nbsp; Therefore, it has huge amount of details. I mean&nbsp;&nbsp;

- [00:25:34](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1534) look at these details. So you can generate&nbsp; massive images as well with this strategy.&nbsp;&nbsp;

- [00:25:40](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1540) Set the base resolution and upscale. Okay, this&nbsp; is 1280 to 1280. Now I will reuse parameters&nbsp;&nbsp;

- [00:25:46](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1546) and enable my upscale preset one more time. It&nbsp; is here direct apply and generate. Now it will&nbsp;&nbsp;

- [00:25:53](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1553) upscale this image into twice resolution.&nbsp; Of course, it will take time. However,&nbsp;&nbsp;

- [00:25:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1558) it will be the maximum quality. So this image&nbsp; full HD was generated in 73 seconds. This image&nbsp;&nbsp;

- [00:26:07](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1567) 1280 to 1280 was generated in 55 seconds. This&nbsp; big image was generated in 2 minute 25 seconds.&nbsp;&nbsp;

- [00:26:16](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1576) I mean these are the expected times because&nbsp; we are generating really high quality images,&nbsp;&nbsp;

- [00:26:21](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1581) really high resolution images with perfect&nbsp; accuracy. And this is literally the first&nbsp;&nbsp;

- [00:26:26](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1586) generation that we did. So I can generate&nbsp; more and pick the better ones and as you&nbsp;&nbsp;

- [00:26:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1590) do more generations you will understand&nbsp; how the model works, what are the basics.

- [00:26:35](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1595) Okay, so the upscale of the 1280 to 1280&nbsp; completed and let's look at the difference&nbsp;&nbsp;

- [00:26:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1601) one more time. So this is upscaled and&nbsp; this was the base. So from this base,&nbsp;&nbsp;

- [00:26:47](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1607) we upscaled into this masterpiece. The&nbsp; resolution is just amazing. The details,&nbsp;&nbsp;

- [00:26:53](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1613) the realism, everything is mind blowing compared&nbsp; to the other models. So can you use this model,&nbsp;&nbsp;

- [00:27:01](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1621) this only images trained low noise model&nbsp; for video generation? Yes. It will be&nbsp;&nbsp;

- [00:27:07](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1627) a little bit lower quality, but yes. Let me&nbsp; demonstrate that. So I will copy this prompt.&nbsp;&nbsp;

- [00:27:13](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1633) You can also use this for image to video as well,&nbsp; don't worry. Everything is same. So let's refresh.&nbsp;&nbsp;

- [00:27:19](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1639) Reset params to default. And there are several&nbsp; options that you can use. For the highest quality,&nbsp;&nbsp;

- [00:27:26](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1646) you need to use Wan 2.2 high quality text to video&nbsp; 20 steps. This will take some time. On my GPU it&nbsp;&nbsp;

- [00:27:34](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1654) takes like 6 to 7 minutes. Or you can use the&nbsp; lightning LoRA based one. You see Wan 2.2 text to&nbsp;&nbsp;

- [00:27:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1661) video. So let's see the lightning LoRA first, then&nbsp; let's see the other one. Then let's proceed to&nbsp;&nbsp;

- [00:27:47](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1667) dual model training. So direct apply. You see it&nbsp; is going to set the low and high noise LoRAs like&nbsp;&nbsp;

- [00:27:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1674) this automatically for you. Copy paste your prompt&nbsp; and select your trained LoRA. So this was the low&nbsp;&nbsp;

- [00:28:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1680) noise LoRA. Then generate. This will be pretty&nbsp; fast because it is only total 4 steps. And again,&nbsp;&nbsp;

- [00:28:09](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1689) for this preset to work flawlessly, you need to&nbsp; download this bundle. Or you will be having issues&nbsp;&nbsp;

- [00:28:16](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1696) with the models finding them, setting them.&nbsp; I am doing everything automatically for you.

- [00:28:21](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1701) Okay, so the generation started. First it is&nbsp; starting to use the high noise Wan 2.2 model.&nbsp;&nbsp;

- [00:28:28](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1708) Then it will use the low noise model. However,&nbsp; we should change the prompt to into a video&nbsp;&nbsp;

- [00:28:35](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1715) of. So it will be a little bit better. When you&nbsp; switch presets like this, since your base model&nbsp;&nbsp;

- [00:28:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1721) will change, it will take some time for loading&nbsp; reloading models, but everything is automatically&nbsp;&nbsp;

- [00:28:46](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1726) and properly made by the ComfyUI. So you shouldn't&nbsp; have any problems. And as you can see, it is able&nbsp;&nbsp;

- [00:28:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1734) to 100 percentage utilize my GPU. It is using&nbsp; my GPU 575 watts at once. So it is maximum power&nbsp;&nbsp;

- [00:29:05](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1745) usage. Okay, it is starting to generate. Since&nbsp; we trained our LoRA only on low noise model, this&nbsp;&nbsp;

- [00:29:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1752) will work a little bit lower quality than training&nbsp; on dual models which I will show. However,&nbsp;&nbsp;

- [00:29:18](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1758) you can use this LoRA on image to video presets&nbsp; as well. I will show that too after this. Okay,&nbsp;&nbsp;

- [00:29:24](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1764) so the video has been generated. Let's see&nbsp; the video. Yes, I can see the resemblance&nbsp;&nbsp;

- [00:29:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1770) and accuracy. It is pretty good as you can&nbsp; see. This is base video. If I upscale it,&nbsp;&nbsp;

- [00:29:35](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1775) it would be better, but I can certainly see the&nbsp; resemblance especially as it gets closer. Let me&nbsp;&nbsp;

- [00:29:42](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1782) show you the final frame perhaps like this. And&nbsp; it only took how many minutes. It only took 111&nbsp;&nbsp;

- [00:29:49](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1789) seconds. So it takes lesser than 2 minutes. If I&nbsp; want better quality, then I need to switch to this&nbsp;&nbsp;

- [00:29:56](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1796) preset. This will do real 20 steps with real&nbsp; CFG scale and it will be much higher quality.

- [00:30:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1804) What about using this on image to video. Can I do&nbsp; that? Yes. So I will do reset params to default.&nbsp;&nbsp;

- [00:30:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1812) Go to presets. With image to video again we have 2&nbsp; quality presets. You can use this high quality or&nbsp;&nbsp;

- [00:30:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1820) you can use Wan 2.2 image to video 4 steps. This&nbsp; is also really good quality. So direct apply. Then&nbsp;&nbsp;

- [00:30:27](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1827) let's select our prompt like this. I don't need&nbsp; to copy this of course this time. Just copy paste&nbsp;&nbsp;

- [00:30:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1833) the prompt. Then as an image for example let's use&nbsp; this image. So I will go to init image choose file&nbsp;&nbsp;

- [00:30:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1841) then choose this then resolution close use closest&nbsp; aspect ratio. And the preset was not selected.&nbsp;&nbsp;

- [00:30:48](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1848) Yes, I need to select the preset so refresh. Reset&nbsp; params to default. Then refresh models to be sure&nbsp;&nbsp;

- [00:30:55](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1855) you have everything. Then presets and let's go&nbsp; to presets. Okay this one direct apply. Yes,&nbsp;&nbsp;

- [00:31:02](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1862) you see it has selected the accurate model. This&nbsp; is important apply. Then let's go to init image&nbsp;&nbsp;

- [00:31:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1868) choose file select our photo closest aspect ratio.&nbsp; Okay. Now I also need to select my LoRA. So my&nbsp;&nbsp;

- [00:31:15](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1875) LoRA was let's see this one. This is only trained&nbsp; with the images on low noise and generate. This is&nbsp;&nbsp;

- [00:31:24](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1884) also 4 steps. So therefore it will be pretty good.&nbsp; However, since we are switching a lot of models,&nbsp;&nbsp;

- [00:31:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1890) it may leave some left over. Therefore what&nbsp; I am going to do is I will cancel this first.&nbsp;&nbsp;

- [00:31:35](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1895) Then server backend and restart all backends.&nbsp; This will refresh my memory usages like this.&nbsp;&nbsp;

- [00:31:43](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1903) You see it cleared everything. If you are going to&nbsp; do a lot of model switching, you can restart your&nbsp;&nbsp;

- [00:31:48](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1908) backend to clear your VRAM and RAM memory and then&nbsp; generate. Now it should work better. Otherwise you&nbsp;&nbsp;

- [00:31:56](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1916) need to trust the ComfyUI RAM management. So I&nbsp; recommend to restart backends when you are going&nbsp;&nbsp;

- [00:32:01](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1921) to do big models switching like this. Because&nbsp; we switched from LoRA text to video to image&nbsp;&nbsp;

- [00:32:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1928) to video. Okay, it is nice. It is loading into RAM&nbsp; then starting generation. This should be also take&nbsp;&nbsp;

- [00:32:15](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1935) lesser than 2 minutes especially the second and&nbsp; third generations. By the way this was an image&nbsp;&nbsp;

- [00:32:21](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1941) prompt. Therefore for video prompting you need&nbsp; to do better. But I am just showing an example.&nbsp;&nbsp;

- [00:32:28](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1948) So this prompt is not made for video generation.&nbsp; This was made for image generation. Therefore it&nbsp;&nbsp;

- [00:32:35](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1955) may not be perfect for video generation. However&nbsp; still we should see the you know the quality and&nbsp;&nbsp;

- [00:32:42](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1962) video. I have tested that training text to&nbsp; video model LoRA improves image to video as&nbsp;&nbsp;

- [00:32:49](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1969) well. Training image to video LoRA doesn't improve&nbsp; that well. It is weird I know. Maybe it is because&nbsp;&nbsp;

- [00:32:55](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1975) I have trained with only images static images&nbsp; not with videos. But this is really improving&nbsp;&nbsp;

- [00:33:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1980) image to video as well. So you can train an image&nbsp; LoRA from text to video model with just images and&nbsp;&nbsp;

- [00:33:07](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1987) it will improve your image to video generations&nbsp; as well. Okay, it failed for some reason maybe.&nbsp;&nbsp;

- [00:33:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=1994) Let's see the logs. No I don't see any error.&nbsp; Why it is showing an error. Okay, I think I&nbsp;&nbsp;

- [00:33:21](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2001) was out of RAM memory so the ComfyUI crashed&nbsp; probably because I am already using 2 GPUs.

- [00:33:29](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2009) RAM memory on my computer with all other stuff&nbsp; that is open. Okay, it crashed again due to out of&nbsp;&nbsp;

- [00:33:37](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2017) RAM memory. Therefore, I have added this argument:&nbsp; --cache-none. When you add this into your backend,&nbsp;&nbsp;

- [00:33:45](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2025) it will cache none of the models. So this is&nbsp; really useful when you are working with Wan 2.2&nbsp;&nbsp;

- [00:33:52](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2032) low and high noise models together at the same&nbsp; time if you don't have sufficient amount of RAM&nbsp;&nbsp;

- [00:33:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2038) memory, especially system RAM memory. So this&nbsp; way, it will use a model, generate, it will&nbsp;&nbsp;

- [00:34:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2044) then offload it, completely deleted, and load the&nbsp; next model. This will minimize your RAM usage,&nbsp;&nbsp;

- [00:34:11](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2051) also VRAM usage, with delay of loading models&nbsp; from the disk. But since my disks are fast,&nbsp;&nbsp;

- [00:34:18](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2058) it is not an issue. So it fixed the&nbsp; issue. There is no more RAM leakage&nbsp;&nbsp;

- [00:34:23](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2063) or RAM accumulation and we are about to&nbsp; get the video ready. It is pretty fast.

- [00:34:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2070) Okay, we got the video. So this was from image&nbsp; to video. As we get, you know, distant from the&nbsp;&nbsp;

- [00:34:37](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2077) camera, the resemblance gets decreased, but&nbsp; this was only static images trained model,&nbsp;&nbsp;

- [00:34:44](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2084) not the auto model. However, it is decent&nbsp; and it took only 2.4 minutes generation. So,&nbsp;&nbsp;

- [00:34:52](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2092) how we are going to train on dual models?&nbsp; Not only with the single model. If you want&nbsp;&nbsp;

- [00:34:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2098) to generate videos, this is what you should do.&nbsp; So I will just disable my backend to minimize my&nbsp;&nbsp;

- [00:35:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2104) VRAM usage. Then go back to your SECourses&nbsp; Musubi Tuner. So for dual model training,&nbsp;&nbsp;

- [00:35:13](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2113) we are just going to change our preset. So I will&nbsp; open a new tab here. Go to 1 Models Training,&nbsp;&nbsp;

- [00:35:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2120) open folder. Go back to Wan 2.2 training&nbsp; configurations. So we are going to use&nbsp;&nbsp;

- [00:35:24](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2124) text to video configs. These configurations train&nbsp; both high noise and low noise models at the same&nbsp;&nbsp;

- [00:35:32](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2132) time. However, this will use more VRAM and this&nbsp; will use more RAM memory since you need to have&nbsp;&nbsp;

- [00:35:38](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2138) both of the models loaded into your RAM memory.&nbsp; Therefore, be careful. I recommend to begin with&nbsp;&nbsp;

- [00:35:45](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2145) fp8 scaled versions. Then once you verified it&nbsp; is working, you can move to non-fp8. So let's&nbsp;&nbsp;

- [00:35:52](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2152) begin with fp8 scaled. Again, always save your&nbsp; configurations like this. Then we need to set&nbsp;&nbsp;

- [00:36:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2160) the output directory. Don't forget that. So&nbsp; I can just copy paste the directories here.

- [00:36:07](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2167) Training dataset. Again, I am only using static&nbsp; images. If you use videos, the videos will use&nbsp;&nbsp;

- [00:36:13](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2173) more VRAM memory. Therefore, be very careful.&nbsp; If you use videos, then you need to have more&nbsp;&nbsp;

- [00:36:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2180) VRAM memory, RAM memory. You need to do more&nbsp; block swapping. You may need to reduce your&nbsp;&nbsp;

- [00:36:25](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2185) training resolution. Everything changes. So first,&nbsp; train with only static images, verify results,&nbsp;&nbsp;

- [00:36:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2193) get some results, get some test done, then you can&nbsp; include videos into your training dataset. You can&nbsp;&nbsp;

- [00:36:39](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2199) have multiple subfolders for different training&nbsp; datasets and for each dataset have different&nbsp;&nbsp;

- [00:36:47](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2207) resolutions. So let me demonstrate an example.&nbsp; So what I did is I generated another folder like&nbsp;&nbsp;

- [00:36:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2214) this, 1_ohwx_video. However, I will be needed&nbsp; to set different captions, not automatic ones.&nbsp;&nbsp;

- [00:37:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2220) So I will copy paste it here. As I said, first I&nbsp; recommend to train with only static images then&nbsp;&nbsp;

- [00:37:06](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2226) include videos. But I will show how to include&nbsp; videos. And generate dataset configuration. Now,&nbsp;&nbsp;

- [00:37:13](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2233) it will generate datasets like this. You need to&nbsp; edit this TOML file. Why? Because you will see&nbsp;&nbsp;

- [00:37:22](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2242) that it is using the same resolution for both&nbsp; images and videos. What does this mean? This&nbsp;&nbsp;

- [00:37:28](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2248) means that if your images are high resolution,&nbsp; the videos will be too high resolution. Either&nbsp;&nbsp;

- [00:37:32](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2252) you need to auto downscale your videos to lower&nbsp; resolution or you need to edit this TOML file.&nbsp;&nbsp;

- [00:37:40](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2260) So I will open this TOML file. It is saved&nbsp; inside this folder. You need to do this manually.

- [00:37:47](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2267) So this is the generated TOML file. You can see&nbsp; that there are 2 different datasets. The first&nbsp;&nbsp;

- [00:37:53](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2273) one is the image dataset and the second one is&nbsp; the video dataset. And in the video dataset,&nbsp;&nbsp;

- [00:37:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2278) it is going to use some frame extraction method&nbsp; and it is going to use some methods like frame&nbsp;&nbsp;

- [00:38:03](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2283) extraction head, target frames, number of repeats,&nbsp; max frames. So I am going to set resolution here&nbsp;&nbsp;

- [00:38:09](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2289) as well and change the resolution like 480 to&nbsp; 480. So that it will use this resolution for&nbsp;&nbsp;

- [00:38:17](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2297) this dataset. You can also change the resolution&nbsp; of the image dataset from here. So you can have&nbsp;&nbsp;

- [00:38:23](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2303) different dataset resolutions. And if you are&nbsp; wondering what are these other options for&nbsp;&nbsp;

- [00:38:28](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2308) video datasets, it is all written here with&nbsp; details. When you scroll down, you will see&nbsp;&nbsp;

- [00:38:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2313) dataset preparation details. Open this and read&nbsp; here. However, as I said multiple times now,&nbsp;&nbsp;

- [00:38:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2321) train with only images then verify your results&nbsp; then do training with video dataset. Moreover,&nbsp;&nbsp;

- [00:38:49](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2329) now since we have 2 different folders, you will&nbsp; see that it generated the captions like this&nbsp;&nbsp;

- [00:38:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2334) for the video dataset. So you need to change the&nbsp; caption of the video as well. I didn't test this.&nbsp;&nbsp;

- [00:39:01](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2341) Probably this will work best with just trigger&nbsp; word or you can describe the video action as&nbsp;&nbsp;

- [00:39:07](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2347) well in these captions. However, for using the&nbsp; static images, I recommend only trigger word&nbsp;&nbsp;

- [00:39:13](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2353) as a caption. So I will delete this and I will&nbsp; regenerate dataset configuration. Yes. Now it will&nbsp;&nbsp;

- [00:39:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2360) use only my images dataset. It shows 56 double&nbsp; times but it is okay. There is actually 28 images.

- [00:39:27](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2367) Okay, we did set the dataset. This is&nbsp; how you prepare your video training&nbsp;&nbsp;

- [00:39:32](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2372) datasets. Then we set the other things same&nbsp; as the previous training. So in the models,&nbsp;&nbsp;

- [00:39:39](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2379) I will just copy paste the paths here, here,&nbsp; here. And additionally, for this model,&nbsp;&nbsp;

- [00:39:46](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2386) you need to set the high noise model as well. So&nbsp; it is here. So I will set it like this. You see,&nbsp;&nbsp;

- [00:39:53](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2393) high noise. And that's it. Now I will save my&nbsp; configuration and click training. It should&nbsp;&nbsp;

- [00:40:01](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2401) start and work right away. However, you need&nbsp; to have more RAM memory for this to work and it&nbsp;&nbsp;

- [00:40:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2408) will use a little bit more VRAM memory as well.&nbsp; So let's see what happens. The training window&nbsp;&nbsp;

- [00:40:15](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2415) is here. You will see that it will load both low&nbsp; noise and high noise models into the your system.&nbsp;&nbsp;

- [00:40:24](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2424) You should see them in the CMD window. So first&nbsp; it is loading the low noise then it is loading&nbsp;&nbsp;

- [00:40:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2430) the high noise. Low noise is for details,&nbsp; high noise is for the initial generation,&nbsp;&nbsp;

- [00:40:35](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2435) the base structure. We train only low noise for&nbsp; image generation. However, if you want to generate&nbsp;&nbsp;

- [00:40:42](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2442) videos, you should train on both of them. You can&nbsp; also train them individually one by one. However,&nbsp;&nbsp;

- [00:40:48](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2448) its quality is lower than training this dual&nbsp; model configuration. So you can load the image&nbsp;&nbsp;

- [00:40:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2454) configuration, image training configuration,&nbsp; change low noise model to high noise model. It&nbsp;&nbsp;

- [00:40:59](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2459) will work exactly same and train high noise&nbsp; model. If your system is not able to train&nbsp;&nbsp;

- [00:41:05](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2465) both models at the same time, you can follow&nbsp; that strategy. But what I recommend is rent&nbsp;&nbsp;

- [00:41:11](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2471) a machine from the RunPod or Massed Compute&nbsp; which I will show and train there and use.

- [00:41:16](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2476) Okay, it started. As you can see, both of the&nbsp; models is running at the same time now. You will&nbsp;&nbsp;

- [00:41:23](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2483) see that it will fluctuate the VRAM usage and how&nbsp; it changes model is like 87 percent. Okay, you see&nbsp;&nbsp;

- [00:41:31](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2491) the RAM usage increased because now it is loading&nbsp; the other model and it is switching between them.&nbsp;&nbsp;

- [00:41:37](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2497) Okay, you see. It is working nice. It is using&nbsp; almost my entire RAM memory right now so I should&nbsp;&nbsp;

- [00:41:43](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2503) decrease my RAM memory usage. I can also reduce my&nbsp; block swap count because I still have some VRAM.&nbsp;&nbsp;

- [00:41:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2510) So where you increase or decrease the block swap?&nbsp; So let's open all panels, search for swap and&nbsp;&nbsp;

- [00:41:57](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2517) you will see that there is block swap. So you can&nbsp; change block swaps. Currently since it is fitting&nbsp;&nbsp;

- [00:42:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2524) into my VRAM entirely with fp8 scaled, I am not&nbsp; doing any block swap. But if it doesn't fit in&nbsp;&nbsp;

- [00:42:10](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2530) your case, you need to increase this like 10, 20,&nbsp; 15. Moreover, I can use offload inactive DiT to&nbsp;&nbsp;

- [00:42:18](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2538) CPU. This requires more RAM memory and it is not&nbsp; compatible with blocks to swap. So when you do&nbsp;&nbsp;

- [00:42:24](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2544) block swap, you cannot use this unfortunately.&nbsp; Therefore, it is the fastest training that I&nbsp;&nbsp;

- [00:42:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2550) can get right now. Its speed is very good as&nbsp; you can see. It is also faster than training&nbsp;&nbsp;

- [00:42:34](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2554) only image model right now because I am doing fp8&nbsp; scaled training. If I were doing not fp8 scaling,&nbsp;&nbsp;

- [00:42:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2561) I would be needed to do block swapping and block&nbsp; swapping reduces your speed. So this dual model&nbsp;&nbsp;

- [00:42:47](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2567) training takes like 6 hours on my GPU, maybe&nbsp; even faster. This is how you train dual model.

- [00:42:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2574) And how you use dual model? Exactly same as the&nbsp; single image training. I only change my LoRA to&nbsp;&nbsp;

- [00:43:01](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2581) use it. So let's enable back our backend and let's&nbsp; open a new tab. Copy our prompt. Quick tools,&nbsp;&nbsp;

- [00:43:10](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2590) reset params to default presets. And let's use&nbsp; this high quality preset this time. Direct apply.&nbsp;&nbsp;

- [00:43:16](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2596) Type our prompt like this. Okay. Make sure to&nbsp; not include the LoRAs in your prompt when you&nbsp;&nbsp;

- [00:43:22](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2602) copy paste. Okay, it is like this. And I need to&nbsp; select my LoRA. So my LoRA is this one. High noise&nbsp;&nbsp;

- [00:43:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2610) low noise auto. So this was trained exactly as I&nbsp; have just shown you. This is the highest quality&nbsp;&nbsp;

- [00:43:36](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2616) and let's generate. So this generation will&nbsp; take significantly more time compared to the&nbsp;&nbsp;

- [00:43:43](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2623) fast generation which is Wan 2.2 text to video 8&nbsp; steps. Actually this is 4 steps. I forgotten to&nbsp;&nbsp;

- [00:43:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2630) change its name. Yes, this is 4 steps. So it will&nbsp; take more time but this is the highest quality&nbsp;&nbsp;

- [00:43:57](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2637) that you are going to get with video generation,&nbsp; with text to video generation. For image to video,&nbsp;&nbsp;

- [00:44:02](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2642) it doesn't make that much big difference. You can&nbsp; perfectly use this preset. But for text to video&nbsp;&nbsp;

- [00:44:09](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2649) generation, it makes really big difference&nbsp; still because none of the recent LoRAs are,&nbsp;&nbsp;

- [00:44:15](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2655) the lightning LoRAs are still as good as the image&nbsp; to video LoRAs. I am using the latest LoRA which&nbsp;&nbsp;

- [00:44:22](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2662) was published only 3 days ago. You see they&nbsp; are 17 December 2025 LoRAs but it is as I say.

- [00:44:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2670) So the generation started. As you can see, it&nbsp; should be done in like 6 minutes if I remember&nbsp;&nbsp;

- [00:44:37](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2677) correctly. We will see. Okay, so we got our video&nbsp; generated. It is not the best video. The quality&nbsp;&nbsp;

- [00:44:43](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2683) is good but the prompt is bad. So therefore I&nbsp; need to edit my prompt and generate new one.&nbsp;&nbsp;

- [00:44:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2690) The prompting do matters a lot. So play with your&nbsp; prompts to get the best results. Remember all the&nbsp;&nbsp;

- [00:44:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2698) generations will be inside your SwarmUI, inside&nbsp; output, inside local, inside raw folder. So you&nbsp;&nbsp;

- [00:45:06](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2706) will see all of your generations here. They will&nbsp; be saved by the date, latest date folder. You will&nbsp;&nbsp;

- [00:45:13](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2713) see and find your images and videos in this&nbsp; folder if you need them later. Furthermore,&nbsp;&nbsp;

- [00:45:19](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2719) you will see them in the history tab as well&nbsp; from the raw and with the folders like this.

- [00:45:25](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2725) So as a next step, I will show how to train on&nbsp; Massed Compute then I will show on RunPod. So&nbsp;&nbsp;

- [00:45:32](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2732) for Massed Compute, enter inside the extracted zip&nbsp; file folder and open Massed Compute instructions&nbsp;&nbsp;

- [00:45:38](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2738) read txt. Reading this file is mandatory&nbsp; and watching the Windows tutorial part are&nbsp;&nbsp;

- [00:45:43](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2743) mandatory. Please use this link to register. I&nbsp; appreciate that very much. Then go to billing&nbsp;&nbsp;

- [00:45:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2750) and set up some credits. They accept crypto&nbsp; payments as well. Then go to deploy. In here,&nbsp;&nbsp;

- [00:45:57](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2757) I recommend you to use RTX Pro 6000 Blackwell GPU.&nbsp; This is working really great. From the category,&nbsp;&nbsp;

- [00:46:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2764) select creator, select SECourses. Then enter&nbsp; your coupon. This is important. We are going&nbsp;&nbsp;

- [00:46:10](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2770) to use Category Creator, Image SECourses&nbsp; and discount coupon SECourses. You see the,&nbsp;&nbsp;

- [00:46:16](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2776) this very powerful GPU is 1.8 dollars&nbsp; per hour. When I apply my coupon,&nbsp;&nbsp;

- [00:46:23](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2783) it becomes 1.35 dollars. If you want&nbsp; even more powerful GPU, faster training,&nbsp;&nbsp;

- [00:46:29](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2789) select H200 NVL. This is even more powerful than&nbsp; RTX Pro 6000. It is 2.6 dollars. With our coupon,&nbsp;&nbsp;

- [00:46:38](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2798) it becomes 1.95 dollars. This is the fastest GPU.&nbsp; So you can use either of them. Let's use H200 for&nbsp;&nbsp;

- [00:46:46](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2806) this demonstration, for this tutorial. Deploy.&nbsp; Now we need to wait for machine to initialize.

- [00:46:53](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2813) To connect our machine, we are going to&nbsp; use ThinLinc Client. Download it. If you&nbsp;&nbsp;

- [00:46:57](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2817) followed other tutorials, you already know. After&nbsp; downloading, just next, next, next. Then you need&nbsp;&nbsp;

- [00:47:03](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2823) to set your shared folder from local devices.&nbsp; You see clipboard synchronization and drives.&nbsp;&nbsp;

- [00:47:09](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2829) And in here I have a shared folder with read&nbsp; and write permission. So I can copy my small&nbsp;&nbsp;

- [00:47:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2834) files and I can connect. For big files, you&nbsp; need to watch this tutorial. This will show&nbsp;&nbsp;

- [00:47:21](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2841) you how to upload and download big files to&nbsp; Hugging Face. You can also use Google Drive&nbsp;&nbsp;

- [00:47:26](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2846) or OneDrive. So other cloud services as well. You&nbsp; know how to use them. But you cannot use ThinLinc&nbsp;&nbsp;

- [00:47:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2853) Client to transfer big files. So let's just&nbsp; wait for initialization to be complete. Okay,&nbsp;&nbsp;

- [00:47:39](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2859) now it is running. You may refresh page time to&nbsp; time to update but it was auto updated for me.&nbsp;&nbsp;

- [00:47:45](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2865) Then click here and copy login URL. Open ThinLinc&nbsp; Client. Copy username for first time users and&nbsp;&nbsp;

- [00:47:52](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2872) copy password and connect. Then continue. Then&nbsp; click start. It will start the machine and&nbsp;&nbsp;

- [00:48:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2880) connect from the ThinLinc Client. Remember this&nbsp; is running on a remote machine, not on my machine.

- [00:48:05](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2885) Now I need to transfer my file, my installer file.&nbsp; You can use Patreon login from this browser or you&nbsp;&nbsp;

- [00:48:13](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2893) can use the shared folder which I prefer. So I&nbsp; will copy my file into my shared folder. You can&nbsp;&nbsp;

- [00:48:21](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2901) also put your training images here. Go back to&nbsp; home from here. Go to Thin Drives. This is your&nbsp;&nbsp;

- [00:48:26](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2906) shared folder. Do not run anything inside here.&nbsp; Run everything from downloads folder. Whatever it&nbsp;&nbsp;

- [00:48:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2913) is. It doesn't matter. The transfer speeds are not&nbsp; great with ThinLinc Client. Therefore you should&nbsp;&nbsp;

- [00:48:38](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2918) use the other method that I have just shown you,&nbsp; the link. You see this link for big files. Okay,&nbsp;&nbsp;

- [00:48:44](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2924) I will copy paste my images. Drag and drop or copy&nbsp; paste both works. And our latest SECourses Musubi&nbsp;&nbsp;

- [00:48:53](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2933) Tuner Premium Installer. So it is here. Drag and&nbsp; drop into downloads folder. Wait for files to be&nbsp;&nbsp;

- [00:48:59](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2939) transferred. You see it is being transferred right&nbsp; now. Then go to downloads folder. Extract the&nbsp;&nbsp;

- [00:49:05](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2945) Musubi Tuner. Extract here. And I will open the&nbsp; Massed Compute instructions. Copy this command.&nbsp;&nbsp;

- [00:49:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2952) While I am inside this folder, open in terminal.&nbsp; This is important. And paste. You need to be&nbsp;&nbsp;

- [00:49:18](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2958) in the accurate folder. Then it will start the&nbsp; installation. You will see that installation is&nbsp;&nbsp;

- [00:49:23](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2963) lightning fast. Actually let's see in real time.&nbsp; So it is going to use uv package installation.

- [00:49:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2970) I mean look at the speed. Look at the&nbsp; installation speed. It will take like&nbsp;&nbsp;

- [00:49:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2973) 30 seconds. Maybe 1 minute maximum. So it is&nbsp; almost done. Also we need to download training&nbsp;&nbsp;

- [00:49:40](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2980) models. So in here you see there is this command.&nbsp; python3 download_train_models. Again while I am&nbsp;&nbsp;

- [00:49:46](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2986) inside this folder, open in terminal. Paste. And&nbsp; it will ask you which models to download. So let's&nbsp;&nbsp;

- [00:49:53](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=2993) download text to video option 4. The download will&nbsp; begin. The downloader is also optimized. It will&nbsp;&nbsp;

- [00:50:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3000) be pretty fast. You see 500 megabytes per second.&nbsp; So it is 4 gigabits per second. 4 gigabits. So it&nbsp;&nbsp;

- [00:50:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3008) will be done almost in no time. The installation&nbsp; also completed and the trainer started. You see&nbsp;&nbsp;

- [00:50:15](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3015) installation take like 60 seconds. The rest&nbsp; is exactly same as in the Windows tutorial.&nbsp;&nbsp;

- [00:50:22](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3022) You will open your configuration. And there&nbsp; is one advantage because configuration is&nbsp;&nbsp;

- [00:50:27](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3027) set for Massed Compute folders directly. So&nbsp; I will download the Wan 2.2 training configs,&nbsp;&nbsp;

- [00:50:34](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3034) text to video configs. Whether you want to&nbsp; use dual GPU or whichever GPU. So this one&nbsp;&nbsp;

- [00:50:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3041) is using 140 gigabytes of GPU. Let's see our&nbsp; VRAM. nvidia-smi. Okay we have 140 gigabytes.&nbsp;&nbsp;

- [00:50:49](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3049) Therefore I am going to use this config. I&nbsp; hope it fits. You see it is maximum. Then&nbsp;&nbsp;

- [00:50:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3054) I need to set my output folder and other thing.&nbsp; I think output folder is already set. So I need&nbsp;&nbsp;

- [00:51:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3060) to set my training images or videos. So&nbsp; extract here. Okay. So copy this path and&nbsp;&nbsp;

- [00:51:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3068) copy paste here. Then don't forget to click&nbsp; generate dataset configuration. And let's see&nbsp;&nbsp;

- [00:51:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3074) if the model download is completed. Yes almost&nbsp; completed. You see 3 over 4. It is verifying.

- [00:51:22](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3082) So literally with like 5 minutes setup, you&nbsp; can begin training in Massed Compute. Like 5&nbsp;&nbsp;

- [00:51:29](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3089) minutes. Maybe 4 minutes. Once you do this like 1&nbsp; or 2 times, you will get used to it. You will be&nbsp;&nbsp;

- [00:51:35](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3095) immediately setup a machine and start training&nbsp; right away. Okay high noise model is getting&nbsp;&nbsp;

- [00:51:42](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3102) verified. I am also verifying files. Therefore&nbsp; you will never have any issues with downloaded&nbsp;&nbsp;

- [00:51:47](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3107) models. This is super important because sometimes&nbsp; models get corrupted and it is causing a lot of&nbsp;&nbsp;

- [00:51:52](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3112) issues. Okay it is downloading the last model.&nbsp; It is 50 seconds remaining time. Okay so all&nbsp;&nbsp;

- [00:51:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3118) the downloads have been completed. Now I did&nbsp; set the training dataset and start training.&nbsp;&nbsp;

- [00:52:05](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3125) It should work if the folder paths are accurate&nbsp; if I remember but probably we need to change one&nbsp;&nbsp;

- [00:52:11](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3131) more thing. Not ready yet. So in the 1 Models&nbsp; Settings, yes. You see this is 23 version so I&nbsp;&nbsp;

- [00:52:18](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3138) need to make this 24. You also need to change&nbsp; that. The output folder should be okay. Yes&nbsp;&nbsp;

- [00:52:26](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3146) it is okay. And start training. Now let's see.&nbsp; Oh by the way if this was a 2 model training,&nbsp;&nbsp;

- [00:52:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3153) yes it was. I need to also change this. So&nbsp; dual model. Okay. Now it will give another&nbsp;&nbsp;

- [00:52:38](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3158) error and I will click training again. So first&nbsp; it is caching the training dataset. Okay it&nbsp;&nbsp;

- [00:52:45](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3165) is loading. Okay I will stop and click start&nbsp; again. Yes now it should work perfectly fine.&nbsp;&nbsp;

- [00:52:51](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3171) Let's follow both of that. So let's zoom in.&nbsp; Another zoom. Okay. Let's see in real time. So&nbsp;&nbsp;

- [00:52:59](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3179) starting training. Loading models. You see loading&nbsp; models are amazingly fast. Blazing fast on Massed&nbsp;&nbsp;

- [00:53:06](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3186) Compute. This is why I like Massed Compute over&nbsp; RunPod. The disk speed on Massed Compute is like&nbsp;&nbsp;

- [00:53:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3192) 10x, maybe 20x faster than the RunPod. Okay 77&nbsp; gigabytes. Okay now it is training the low noise&nbsp;&nbsp;

- [00:53:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3200) then it will begin the high noise. Okay training&nbsp; is starting. 4 steps done on the first model then&nbsp;&nbsp;

- [00:53:27](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3207) it will load the second model. The logic of dual&nbsp; training is that it switches models between. Okay&nbsp;&nbsp;

- [00:53:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3213) why it is taking time? I think the first step is&nbsp; taking time. Okay. Now it did begin. Yes. So the&nbsp;&nbsp;

- [00:53:43](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3223) training started. We will see the training speed&nbsp; gets better. Okay it is using 80 gigabytes right&nbsp;&nbsp;

- [00:53:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3230) now. But the training speed is getting better.&nbsp; You see 2.2 second. It will get faster than 1 IT&nbsp;&nbsp;

- [00:53:57](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3237) per second. But it is still switching models.&nbsp; So what we can do, we can disable this and&nbsp;&nbsp;

- [00:54:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3244) see if it works. So let's disable this to get&nbsp; even faster. Did we load the inaccurate model?&nbsp;&nbsp;

- [00:54:10](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3250) Yeah this should be disabled. I will update&nbsp; this configuration for you so you won't have&nbsp;&nbsp;

- [00:54:15](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3255) this issue. Now it should be even faster than&nbsp; before because it will not offload each model.&nbsp;&nbsp;

- [00:54:21](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3261) It will load both of the models into the GPU.&nbsp; Let's see the speed. Of course you can enable&nbsp;&nbsp;

- [00:54:27](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3267) fp8 scaled. It will take even lesser VRAM so you&nbsp; can load it into like 80 gigabytes GPUs as well.

- [00:54:35](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3275) Okay training started. You see it is 1.5&nbsp; IT per second using only 106 gigabytes of&nbsp;&nbsp;

- [00:54:45](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3285) GPU. I will update the configuration so&nbsp; you won't have this issue. But this is&nbsp;&nbsp;

- [00:54:49](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3289) the training speed. It is going to&nbsp; take only 67 minutes. So with like&nbsp;&nbsp;

- [00:54:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3294) 2 dollars, you will be able to train very best&nbsp; Wan 2.2 model. And it's a dual model training,&nbsp;&nbsp;

- [00:55:02](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3302) not single model. You are seeing the speed right&nbsp; now. 1.35 IT per second. It is amazing. And it&nbsp;&nbsp;

- [00:55:09](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3309) will take only 68 minutes and we are doing 5600&nbsp; steps. Not a low number of steps. So it is working&nbsp;&nbsp;

- [00:55:17](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3317) just amazing. Yes I have updated the config as&nbsp; 140 gigabytes and it is using 107 gigabytes GPU.

- [00:55:25](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3325) So now I will show how to train on RunPod. The&nbsp; rest of the Massed Compute is same. You just&nbsp;&nbsp;

- [00:55:32](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3332) need to download your LoRA files and use&nbsp; on your computer or you can use in Massed&nbsp;&nbsp;

- [00:55:37](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3337) Compute as well. If you wonder how to use&nbsp; on Massed Compute, on our channel SECourses,&nbsp;&nbsp;

- [00:55:44](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3344) we have a video for this. The video is this one.&nbsp; Generate AI art 10x faster. The ultimate SwarmUI&nbsp;&nbsp;

- [00:55:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3350) and ComfyUI cloud tutorial. You can watch this&nbsp; tutorial to learn how to use SwarmUI on Massed&nbsp;&nbsp;

- [00:55:57](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3357) Compute or on RunPod. This tutorial covers both&nbsp; of them. So for training on RunPod, we have RunPod&nbsp;&nbsp;

- [00:56:05](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3365) instructions. Open that file. Always you need&nbsp; to follow this file for RunPod. Please register&nbsp;&nbsp;

- [00:56:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3372) RunPod from this link. After registering and&nbsp; logging in, go to billing and set up some credits.&nbsp;&nbsp;

- [00:56:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3380) Then once you have the credits, go to pods.&nbsp; If you want to use permanent storage, I have&nbsp;&nbsp;

- [00:56:26](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3386) a tutorial for that. It is here. If you want&nbsp; to learn how to upload and download big files,&nbsp;&nbsp;

- [00:56:31](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3391) the tutorial is here. And if you want to learn&nbsp; RunPod usage, there is another tutorial here.

- [00:56:36](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3396) So I will show on RunPod with RTX 5090. One of&nbsp; the most commonly used one. But you can use bigger&nbsp;&nbsp;

- [00:56:45](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3405) GPUs like B200 for even faster training. So this&nbsp; is the disk type and this is the RAM GPU filters&nbsp;&nbsp;

- [00:56:51](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3411) that I use. Let's select RTX 5090. Now a lot of&nbsp; people are getting confused here. You are not&nbsp;&nbsp;

- [00:56:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3418) going to use this template. You are always going&nbsp; to use the template that I write in this file.&nbsp;&nbsp;

- [00:57:05](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3425) Always. Because my installers are optimized for&nbsp; whatever it is written here. Otherwise you will&nbsp;&nbsp;

- [00:57:10](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3430) get errors. So change template. Select PyTorch&nbsp; 2.2.0. This is official template. Click edit&nbsp;&nbsp;

- [00:57:18](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3438) and increase the disk size like 200. It is up to&nbsp; you. If you want to save more checkpoints, you&nbsp;&nbsp;

- [00:57:24](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3444) need to increase this. Set overrides and deploy on&nbsp; demand. Now we need to wait for machine to become&nbsp;&nbsp;

- [00:57:32](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3452) ready but it should be very fast because this&nbsp; is official template. This is very lightweight&nbsp;&nbsp;

- [00:57:37](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3457) template. So let's see. Time to time click here.&nbsp; Okay it is ready. Jupyter Lab. If this doesn't&nbsp;&nbsp;

- [00:57:43](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3463) open, you need to refresh, click some several&nbsp; times. If still doesn't work, get a new machine.

- [00:57:49](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3469) Then I will upload my downloaded zip file. Wait&nbsp; for upload to complete. You see in the bottom it&nbsp;&nbsp;

- [00:57:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3474) shows. Then right click and extract archive.&nbsp; Click refresh. Then open RunPod instructions&nbsp;&nbsp;

- [00:58:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3480) read txt file. Copy this command. Open a new&nbsp; terminal from here. And terminal. Make sure&nbsp;&nbsp;

- [00:58:07](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3487) that you are in the workspace and copy paste&nbsp; like this. Then it will start the installation.&nbsp;&nbsp;

- [00:58:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3492) So run the commands inside workspace folder.&nbsp; Open new terminal. Make sure that you are in the&nbsp;&nbsp;

- [00:58:18](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3498) workspace wherever you have extracted the files.&nbsp; The installation will be extremely fast compared&nbsp;&nbsp;

- [00:58:25](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3505) to before. Why? Because we are using uv package.&nbsp; While installation is continuing, let's download&nbsp;&nbsp;

- [00:58:31](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3511) the model because model download may be slow.&nbsp; Open another terminal. Copy paste. Select the&nbsp;&nbsp;

- [00:58:38](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3518) model text to video. It will start downloading.&nbsp; This is also super optimized. Let's see. You&nbsp;&nbsp;

- [00:58:44](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3524) see it is downloading with like 300 megabytes per&nbsp; second. Pretty good. The uv installation is also&nbsp;&nbsp;

- [00:58:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3530) ultra fast. Before it was taking sometimes 10&nbsp; minutes, 20 minutes, 30 minutes to install. Now&nbsp;&nbsp;

- [00:58:56](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3536) it will take only few minutes to install. This&nbsp; is our latest optimization. I will move my all&nbsp;&nbsp;

- [00:59:03](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3543) installers to uv installation hopefully slowly.&nbsp; So you will always install my applications much&nbsp;&nbsp;

- [00:59:10](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3550) faster now compared to before. Also models&nbsp; are getting downloading meanwhile. So after&nbsp;&nbsp;

- [00:59:15](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3555) installation it will auto start the application.&nbsp; This warning is not important. You will get&nbsp;&nbsp;

- [00:59:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3560) this in all installers. The model merging and&nbsp; verification on RunPod is sadly slow compared&nbsp;&nbsp;

- [00:59:27](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3567) to Massed Compute because its disk speed&nbsp; is very slow compared to Massed Compute.

- [00:59:32](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3572) Okay application installation completed and&nbsp; started. I will open the Gradio live link.&nbsp;&nbsp;

- [00:59:38](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3578) However model downloading is still continuing.&nbsp; Meanwhile let's also upload our training images.&nbsp;&nbsp;

- [00:59:44](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3584) So for upload click this icon. Select your&nbsp; training images as a zip file. Upload. I&nbsp;&nbsp;

- [00:59:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3590) prefer this way of uploading. So we can set up the&nbsp; configuration while models are getting downloaded.&nbsp;&nbsp;

- [00:59:56](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3596) Wait for upload to be completed. You see it is&nbsp; slow. You can also use runpodctl for uploading.&nbsp;&nbsp;

- [01:00:02](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3602) I explain that in another tutorial video. For&nbsp; example you can watch this one. But it is not&nbsp;&nbsp;

- [01:00:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3608) mandatory to learn or you can use this tutorial&nbsp; to upload and download big files very fast. This&nbsp;&nbsp;

- [01:00:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3614) is the recommended tutorial. And interface&nbsp; is here. This is running on RunPod. 1 Models&nbsp;&nbsp;

- [01:00:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3620) Training. So from the configurations, let's&nbsp; pick the configuration we want to use. Let's&nbsp;&nbsp;

- [01:00:26](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3626) use the text to image configuration. So you see&nbsp; we have 32 gigabytes. Copy path. Right click and&nbsp;&nbsp;

- [01:00:32](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3632) copy path. Put a backslash. Always RunPod&nbsp; puts a backslash to beginning of path and&nbsp;&nbsp;

- [01:00:39](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3639) paste it. Then click this. It will auto load the&nbsp; config. You see it is loaded successfully. Then&nbsp;&nbsp;

- [01:00:46](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3646) set your output folder. Let's save them inside&nbsp; workspace a new folder: Trained LoRAs. Right&nbsp;&nbsp;

- [01:00:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3654) click and copy path. Then paste it here. Put a&nbsp; backslash to the beginning. This is important.

- [01:01:00](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3660) If you want to do Qwen training, it is same. We&nbsp; only load Qwen training into here. So this is how&nbsp;&nbsp;

- [01:01:06](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3666) you train on RunPod. You can change your save&nbsp; name or save every n epochs. And the training&nbsp;&nbsp;

- [01:01:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3672) dataset. So I have uploaded my images. Right click&nbsp; and extract archive. And they are named like this.&nbsp;&nbsp;

- [01:01:19](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3679) Remember this is mandatory. 1_ohwx. ohwx will be&nbsp; our captions. 1 is the repeating. So I am going to&nbsp;&nbsp;

- [01:01:28](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3688) give the path of the parent folder. Copy as, right&nbsp; click and copy as path. Backslash, put the path&nbsp;&nbsp;

- [01:01:35](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3695) then click generate dataset configuration.&nbsp; It is done. Then the 1 Models Settings,&nbsp;&nbsp;

- [01:01:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3701) we need to set the folder paths. So the models are&nbsp; downloaded inside Training Models 1. So copy path&nbsp;&nbsp;

- [01:01:49](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3709) and change them like this. And backslash. Okay.&nbsp; No not like this. It needs to be like this. Yes.&nbsp;&nbsp;

- [01:01:57](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3717) So this is the accurate pathing on RunPod. So like&nbsp; this. Yes. Verify the paths. And like this. Okay&nbsp;&nbsp;

- [01:02:06](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3726) all paths are set. This is single model so there&nbsp; is no high noise. And what else is left? Nothing&nbsp;&nbsp;

- [01:02:12](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3732) else is left. Save your configuration always.&nbsp; So you can load later if any error happens,&nbsp;&nbsp;

- [01:02:18](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3738) if you restart. Okay the models are still being&nbsp; downloaded. We need to wait them to be completed.&nbsp;&nbsp;

- [01:02:24](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3744) If you use RunPod permanent network storage&nbsp; system, you won't be needed to wait. However&nbsp;&nbsp;

- [01:02:30](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3750) currently we are not using. But it is also&nbsp; very slow. I mean unbearably slow. And you&nbsp;&nbsp;

- [01:02:36](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3756) see only 260 megabyte per second disk speed. On&nbsp; Massed Compute this is 2 gigabytes per second.

- [01:02:43](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3763) Okay all models downloaded. It is verifying the&nbsp; last one. So we can start training. So make sure&nbsp;&nbsp;

- [01:02:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3770) to save and click start training. Then we will&nbsp; follow what is happening on CMD window. This&nbsp;&nbsp;

- [01:02:58](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3778) is where we see the actual logs. If there is any&nbsp; error, you need to report from here. Unfortunately&nbsp;&nbsp;

- [01:03:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3784) setting up RunPod, starting training is like&nbsp; 10x slower than the Massed Compute. But it has&nbsp;&nbsp;

- [01:03:11](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3791) some other advantages like permanent storage or&nbsp; more GPU options. However this is the reality.&nbsp;&nbsp;

- [01:03:18](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3798) You see I click start training and I am still&nbsp; waiting for it to read from the disk to start&nbsp;&nbsp;

- [01:03:24](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3804) caching, loading models. Okay let's also install&nbsp; nvitop. pip install nvitop and just type nvitop.&nbsp;&nbsp;

- [01:03:34](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3814) So we can monitor the GPU usage as well. If&nbsp; you don't see GPU here, that means that your&nbsp;&nbsp;

- [01:03:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3821) machine is broken. It is going to start caching&nbsp; the text encoder outputs. It is started. Nice.&nbsp;&nbsp;

- [01:03:47](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3827) Now it will load model. Oh one more thing. We can&nbsp; increase the model loading speed. So I will stop&nbsp;&nbsp;

- [01:03:54](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3834) training. How? Open all panels and search for&nbsp; loading and disable numpy memory mapping. This&nbsp;&nbsp;

- [01:04:02](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3842) will increase your loading speed significantly&nbsp; on RunPod. So start training. Don't worry it&nbsp;&nbsp;

- [01:04:08](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3848) will just resume from wherever it is left. So we&nbsp; didn't lose any time. This is really speeding up&nbsp;&nbsp;

- [01:04:14](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3854) the model loadings on RunPod. Okay now we need&nbsp; to wait again. Okay it is loading the model.&nbsp;&nbsp;

- [01:04:20](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3860) Pretty fast. This optimization is making huge&nbsp; difference on RunPod. This has been implemented&nbsp;&nbsp;

- [01:04:27](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3867) after I have taught this issue to Kohya. But&nbsp; still nothing fast as the Massed Compute.

- [01:04:34](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3874) Okay training starting. You can monitor the GPU&nbsp; from nvitop. Yes started. You see the first step&nbsp;&nbsp;

- [01:04:41](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3881) speed was very slow. Now 1.46, 2 second IT, 2.15&nbsp; second IT. This is slower than the other GPUs&nbsp;&nbsp;

- [01:04:50](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3890) because we are also doing some block swapping&nbsp; since this is not fp8 scaled. So when we go&nbsp;&nbsp;

- [01:04:57](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3897) to swap, let's see how many we are doing. We are&nbsp; doing 10 block swapping. If you want even faster&nbsp;&nbsp;

- [01:05:03](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3903) training, you can do fp8 scaled training and&nbsp; you can make the block swap zero and it will be&nbsp;&nbsp;

- [01:05:09](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3909) faster. But this is okay speed. It is going to&nbsp; take like 4 5 hours. So it is bearable maybe a&nbsp;&nbsp;

- [01:05:16](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3916) little bit more. But this is how you do training&nbsp; on RunPod. The models will be saved inside my&nbsp;&nbsp;

- [01:05:23](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3923) Trained LoRAs folder. You see it is already&nbsp; generated and the dataset TOML file is saved&nbsp;&nbsp;

- [01:05:29](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3929) there. The training TOML file also saved there&nbsp; when you click training. So you will download&nbsp;&nbsp;

- [01:05:33](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3933) LoRAs from here and use them. Or if you want to&nbsp; use them on RunPod, we have a tutorial for how&nbsp;&nbsp;

- [01:05:39](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3939) to use SwarmUI and ComfyUI on RunPod. It is here.&nbsp; This is a very recent tutorial. Fully up to date.&nbsp;&nbsp;

- [01:05:45](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3945) So you can follow this tutorial to learn how&nbsp; to use SwarmUI and ComfyUI on RunPod and Massed&nbsp;&nbsp;

- [01:05:52](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3952) Compute. So this is it. I hope you have enjoyed.&nbsp; Don't forget to stop your machine, terminate your&nbsp;&nbsp;

- [01:05:59](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3959) machine. On Massed Compute, you need to delete&nbsp; your machine. Stopping your machine will not&nbsp;&nbsp;

- [01:06:04](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3964) stop your credit spending. Don't forget that. But&nbsp; before deleting, make sure to backup all of your&nbsp;&nbsp;

- [01:06:11](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3971) data. On RunPod, you can stop your machine and&nbsp; it will keep your data then you need to terminate&nbsp;&nbsp;

- [01:06:17](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3977) your machine to completely avoid credit usage. But&nbsp; stopping machine will make it very minimal amount&nbsp;&nbsp;

- [01:06:24](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3984) of credit usage. If you have any questions, always&nbsp; ask me. We have Z Image Turbo LoRA training, very&nbsp;&nbsp;

- [01:06:31](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=3991) up to date, working amazing, very lightweight.&nbsp; We have Flux SRPO training updated. We have Qwen&nbsp;&nbsp;

- [01:06:40](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=4000) training that you need to watch. Hopefully more&nbsp; videos are coming, more applications are coming.&nbsp;&nbsp;

- [01:06:45](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=4005) I am working 7 days every week literally like 10&nbsp; hours a day. So stay subscribed, leave a comment,&nbsp;&nbsp;

- [01:06:53](https://www.youtube.com/watch?v=ocEkhAsPOs4&t=4013) ask questions, join our Discord, ask questions,&nbsp; message me from Patreon. Hopefully see you later.
