# Epic Web UI DreamBooth Update - New Best Settings - 10 Stable Diffusion Training Compared on RunPods

## Full tutorial link > https://www.youtube.com/watch?v=sRdtVanSRl4

[![Epic Web UI DreamBooth Update - New Best Settings - 10 Stable Diffusion Training Compared on RunPods](https://img.youtube.com/vi/sRdtVanSRl4/sddefault.jpg)](https://www.youtube.com/watch?v=sRdtVanSRl4 "Epic Web UI DreamBooth Update - New Best Settings - 10 Stable Diffusion Training Compared on RunPods")

[![image](https://img.shields.io/discord/772774097734074388?label=Discord&logo=discord)](https://discord.com/servers/software-engineering-courses-secourses-772774097734074388) [![Hits](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/Epic-Web-UI-DreamBooth-Update-New-Best-Settings-10-Stable-Diffusion-Training-Compared-on-RunPods.md.svg?style=plastic&label=Hits%20Since%2025.08.27&labelColor=007ec6&logo=SECourses)](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/Epic-Web-UI-DreamBooth-Update-New-Best-Settings-10-Stable-Diffusion-Training-Compared-on-RunPods.md)
[![Patreon](https://img.shields.io/badge/Patreon-Support%20Me-F2EB0E?style=for-the-badge&logo=patreon)](https://www.patreon.com/c/SECourses) [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&logoColor=black)](https://www.buymeacoffee.com/DrFurkan) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/Medium-Follow%20Me-800080?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@furkangozukara) [![Codio](https://img.shields.io/static/v1?style=for-the-badge&message=Articles&color=4574E0&logo=Codio&logoColor=FFFFFF&label=CivitAI)](https://civitai.com/user/SECourses/articles) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/DeviantArt-Follow%20Me-990000?style=for-the-badge&logo=deviantart&logoColor=white)](https://www.deviantart.com/monstermmorpg)

[![YouTube Channel](https://img.shields.io/badge/YouTube-SECourses-C50C0C?style=for-the-badge&logo=youtube)](https://www.youtube.com/SECourses)  [![Furkan G√∂z√ºkara LinkedIn](https://img.shields.io/badge/LinkedIn-Follow%20Me-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/furkangozukara/)   [![Udemy](https://img.shields.io/static/v1?style=for-the-badge&message=Stable%20Diffusion%20Course&color=A435F0&logo=Udemy&logoColor=FFFFFF&label=Udemy)](https://www.udemy.com/course/stable-diffusion-dreambooth-lora-zero-to-hero/?referralCode=E327407C9BDF0CEA8156) [![Twitter Follow Furkan G√∂z√ºkara](https://img.shields.io/badge/Twitter-Follow%20Me-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/GozukaraFurkan)


RunPod: [https://bit.ly/RunPodIO.](https://bit.ly/RunPodIO.) Discord : [https://bit.ly/SECoursesDiscord.](https://bit.ly/SECoursesDiscord.) Web UI DreamBooth got epic update and we tested all new features to find the best settings (10). If I have been of assistance to you and you would like to show your support for my work, please consider becoming a patron on ü•∞ [https://www.patreon.com/SECourses](https://www.patreon.com/SECourses)

Playlist of #StableDiffusion Tutorials, #Automatic1111 and Google Colab Guides, #DreamBooth, Textual Inversion / Embedding, LoRA, AI Upscaling, Pix2Pix, Img2Img:

[https://www.youtube.com/playlist?list=PL_pbwdIyffsmclLl0O144nQRnezKlNdx3](https://www.youtube.com/playlist?list=PL_pbwdIyffsmclLl0O144nQRnezKlNdx3)

The experiments are done on RunPods but if you have 20 GB vram having Windows PC, you can exactly do the same things. All you need to do is installing latest Automatic1111 and DreamBooth extension.

Where you can find executed commands and prompts and more info:

[https://gist.github.com/FurkanGozukara/ad1aa55e64576d49f829450278a15885](https://gist.github.com/FurkanGozukara/ad1aa55e64576d49f829450278a15885)

2400 Photo Of Man classification images:

[https://drive.google.com/file/d/1qBf8VyUbmPNalKqm076yOsQjE8BrcG7R/view](https://drive.google.com/file/d/1qBf8VyUbmPNalKqm076yOsQjE8BrcG7R/view)

DreamBooth extension repo link:

[https://github.com/d8ahazard/sd_dreambooth_extension](https://github.com/d8ahazard/sd_dreambooth_extension)

Easiest Way to Install & Run Stable Diffusion Web UI on PC by Using Open Source Automatic Installer:

[https://youtu.be/AZg6vzWHOTA](https://youtu.be/AZg6vzWHOTA)

How to use Stable Diffusion V2.1 and Different Models in the Web UI - SD 1.5 vs 2.1 vs Anything V3:

[https://youtu.be/aAyvsX-EpG4](https://youtu.be/aAyvsX-EpG4)

Zero To Hero Stable Diffusion DreamBooth Tutorial By Using Automatic1111 Web UI - Ultra Detailed:

[https://youtu.be/Bdl-jWR3Ukc](https://youtu.be/Bdl-jWR3Ukc)

How To Do Stable Diffusion Textual Inversion (TI) / Text Embeddings By Automatic1111 Web UI Tutorial:

[https://youtu.be/dNOpWt-epdQ](https://youtu.be/dNOpWt-epdQ)

Sketches into Epic Art with 1 Click: A Guide to Stable Diffusion ControlNet in Automatic1111 Web UI:

[https://youtu.be/vhqqmkTBMlU](https://youtu.be/vhqqmkTBMlU)

Ultimate RunPod Tutorial For Stable Diffusion - Automatic1111 - Data Transfers, Extensions, CivitAI:

[https://youtu.be/QN1vdGhjcRc](https://youtu.be/QN1vdGhjcRc)

Transform Your Selfie into a Stunning AI Avatar with Stable Diffusion - Better than Lensa for Free:

[https://youtu.be/mnCY8uM7E50](https://youtu.be/mnCY8uM7E50)

Stable Diffusion Google Colab, Continue, Directory, Transfer, Clone, Custom Models, CKPT SafeTensors:

[https://youtu.be/kIyqAdd_i10](https://youtu.be/kIyqAdd_i10)

Fantastic New ControlNet OpenPose Editor Extension & Image Mixing - Stable Diffusion Web UI Tutorial:

[https://youtu.be/iFRdrRyAQdQ](https://youtu.be/iFRdrRyAQdQ)

[00:00:00](https://youtu.be/sRdtVanSRl4?t=0) Introduction to DreamBooth new update best settings experiments

[00:00:49](https://youtu.be/sRdtVanSRl4?t=49) How to setup a new Pod and install DreamBooth newest update properly

[00:02:25](https://youtu.be/sRdtVanSRl4?t=145) New RunPod started time to setup and install DreamBooth

[00:03:20](https://youtu.be/sRdtVanSRl4?t=200) Install DreamBooth extension manually and fix errors

[00:07:15](https://youtu.be/sRdtVanSRl4?t=435) Starting first experiment test0 - setup of settings

[00:07:35](https://youtu.be/sRdtVanSRl4?t=455) Best DreamBooth settings for 12GB VRAM having GPUs

[00:11:05](https://youtu.be/sRdtVanSRl4?t=665) Setting and starting second experiment test1 DEIS Noise Scheduler

[00:11:55](https://youtu.be/sRdtVanSRl4?t=715) Test2 Unfreeze Model

[00:12:12](https://youtu.be/sRdtVanSRl4?t=732) Test3 Lion Optimizer

[00:12:40](https://youtu.be/sRdtVanSRl4?t=760) Test4 Stable Diffusion Offset Noise

[00:13:17](https://youtu.be/sRdtVanSRl4?t=797) Test5 Freeze Clip Normalization Layers DreamBooth

[00:13:33](https://youtu.be/sRdtVanSRl4?t=813) Test6 Use EMA + Use EMA for Prediction

[00:14:37](https://youtu.be/sRdtVanSRl4?t=877) Test7 Use EMA + Use EMA Weights for Inference

[00:14:55](https://youtu.be/sRdtVanSRl4?t=895) Test8 Use EMA only

[00:15:05](https://youtu.be/sRdtVanSRl4?t=905) Solution of configuration index out of range Web UI error

[00:15:18](https://youtu.be/sRdtVanSRl4?t=918) Test9 Don't use xformers - default memory attention and fp16

[00:15:49](https://youtu.be/sRdtVanSRl4?t=949) How to add more disk space to your existing RunPod

[00:17:08](https://youtu.be/sRdtVanSRl4?t=1028) xformers related bug error

[00:18:20](https://youtu.be/sRdtVanSRl4?t=1100) How to continue DreamBooth training if an error occurs or for any reason halted

[00:18:49](https://youtu.be/sRdtVanSRl4?t=1129) All tests have been completed time to check their training samples

[00:18:58](https://youtu.be/sRdtVanSRl4?t=1138) Test0 training samples - previously known best settings for 12GB VRAM

[00:19:25](https://youtu.be/sRdtVanSRl4?t=1165) Test1 training samples - DEIS Noise Scheduler

[00:19:44](https://youtu.be/sRdtVanSRl4?t=1184) Test2 training samples

[00:20:20](https://youtu.be/sRdtVanSRl4?t=1220) Test3 training samples

[00:21:13](https://youtu.be/sRdtVanSRl4?t=1273) Test4 training samples

[00:21:38](https://youtu.be/sRdtVanSRl4?t=1298) Test5 training samples

[00:22:15](https://youtu.be/sRdtVanSRl4?t=1335) Test6 training samples

[00:23:27](https://youtu.be/sRdtVanSRl4?t=1407) Test7 training samples

[00:24:19](https://youtu.be/sRdtVanSRl4?t=1459) Test8 training samples

[00:24:54](https://youtu.be/sRdtVanSRl4?t=1494) Test9 training samples

[00:25:36](https://youtu.be/sRdtVanSRl4?t=1536) Finding a good seed to compare all checkpoints within each trained model

[00:26:46](https://youtu.be/sRdtVanSRl4?t=1606) What seed and prompt I used to compare checkpoints

[00:27:50](https://youtu.be/sRdtVanSRl4?t=1670) What is the logic of starting seed when using batch image generation

[00:28:14](https://youtu.be/sRdtVanSRl4?t=1694) How to use x/y/z plot to compare checkpoints to find best trained model

[00:29:10](https://youtu.be/sRdtVanSRl4?t=1750) Where to find x/y/z plot generated grid image file

[00:29:40](https://youtu.be/sRdtVanSRl4?t=1780) Comparing generated grid files of all experiments

[00:29:44](https://youtu.be/sRdtVanSRl4?t=1784) Test0 Checkpoints Grid

[00:31:26](https://youtu.be/sRdtVanSRl4?t=1886) Test1 Checkpoints Grid DEIS Noise Scheduler

[00:32:50](https://youtu.be/sRdtVanSRl4?t=1970) Test2 Checkpoints Grid

[00:34:48](https://youtu.be/sRdtVanSRl4?t=2088) Test3 Checkpoints Grid

[00:38:19](https://youtu.be/sRdtVanSRl4?t=2299) Test4 Checkpoints Grid

[00:40:30](https://youtu.be/sRdtVanSRl4?t=2430) Test5 Checkpoints Grid

[00:41:25](https://youtu.be/sRdtVanSRl4?t=2485) Test6 Checkpoints Grid

[00:43:14](https://youtu.be/sRdtVanSRl4?t=2594) Test7 Checkpoints Grid

[00:44:00](https://youtu.be/sRdtVanSRl4?t=2640) Explanation of overtraining by a comparison

[00:45:27](https://youtu.be/sRdtVanSRl4?t=2727) Test8 Checkpoints Grid

[00:46:59](https://youtu.be/sRdtVanSRl4?t=2819) Test9 Checkpoints Grid

[00:48:30](https://youtu.be/sRdtVanSRl4?t=2910) How to download all decided best checkpoints via runpodctl

[00:49:09](https://youtu.be/sRdtVanSRl4?t=2949) What is RunPod connect to web terminal and how to use it when jupyter connection is not available

[00:49:56](https://youtu.be/sRdtVanSRl4?t=2996) Where to put downloaded safetensors model files

[00:50:10](https://youtu.be/sRdtVanSRl4?t=3010) Using x/y/z plot do conduct final comparison experiment on my local web UI

[00:50:40](https://youtu.be/sRdtVanSRl4?t=3040) Test7 model file size is smaller than others

[00:51:00](https://youtu.be/sRdtVanSRl4?t=3060) Each model file sizes

[00:51:05](https://youtu.be/sRdtVanSRl4?t=3065) Comparing all of the experiments test0 vs test1 vs test2 ...

[01:00:20](https://youtu.be/sRdtVanSRl4?t=3620) Ending Speech



### Video Transcription


- [00:00:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=0) Greetings everyone. DreamBooth extension&nbsp; of Automatic1111 Web UI got a major update&nbsp;&nbsp;

- [00:00:04](https://www.youtube.com/watch?v=sRdtVanSRl4&t=4) very recently. Interface has been changed and&nbsp; many new features are added. In this video,&nbsp;&nbsp;

- [00:00:09](https://www.youtube.com/watch?v=sRdtVanSRl4&t=9) I am going to test all these new features&nbsp; on RunPod io pods simultaneously. Moreover,&nbsp;&nbsp;

- [00:00:14](https://www.youtube.com/watch?v=sRdtVanSRl4&t=14) I will show you how you can properly install and&nbsp; run newest DreamBooth version on the pods. All&nbsp;&nbsp;

- [00:00:20](https://www.youtube.com/watch?v=sRdtVanSRl4&t=20) of the features of DreamBooth I am going&nbsp; to explain in this video applies to the&nbsp;&nbsp;

- [00:00:24](https://www.youtube.com/watch?v=sRdtVanSRl4&t=24) Automatic1111 Web UI PC version as well. We are&nbsp; going to conduct 10 different experiments on 10&nbsp;&nbsp;

- [00:00:30](https://www.youtube.com/watch?v=sRdtVanSRl4&t=30) different pods as displayed here to find out&nbsp; which features improves our training quality.&nbsp;&nbsp;

- [00:00:36](https://www.youtube.com/watch?v=sRdtVanSRl4&t=36) My pods are ready for testing. As you can&nbsp; see here, I have literally spent like 5&nbsp;&nbsp;

- [00:00:41](https://www.youtube.com/watch?v=sRdtVanSRl4&t=41) hours yesterday to prepare these pods, solve the&nbsp; new problems that I have encountered. But first,&nbsp;&nbsp;

- [00:00:48](https://www.youtube.com/watch?v=sRdtVanSRl4&t=48) let's begin with setting up a new pod, showing you&nbsp; how you can properly install DreamBooth extension.&nbsp;&nbsp;

- [00:00:54](https://www.youtube.com/watch?v=sRdtVanSRl4&t=54) I am picking my new pod with having over 60GB RAM.&nbsp; This is important when you are doing DreamBooth&nbsp;&nbsp;

- [00:01:02](https://www.youtube.com/watch?v=sRdtVanSRl4&t=62) training. I am selecting Stale Diffusion 1.5&nbsp; version template. I am selecting 5GB temporary&nbsp;&nbsp;

- [00:01:09](https://www.youtube.com/watch?v=sRdtVanSRl4&t=69) disk, 150GB persistent volume. This is up to&nbsp; you. You can increase this persistent volume&nbsp;&nbsp;

- [00:01:14](https://www.youtube.com/watch?v=sRdtVanSRl4&t=74) or temporary disk later time. However, you can't&nbsp; decrease persistent volume later. So be careful&nbsp;&nbsp;

- [00:01:19](https://www.youtube.com/watch?v=sRdtVanSRl4&t=79) with that. So the things we are going to do on&nbsp; RunPod Stale Diffusion 1.5 applies to the 2.1&nbsp;&nbsp;

- [00:01:25](https://www.youtube.com/watch?v=sRdtVanSRl4&t=85) version template as well. If you don't know what&nbsp; is Stale Diffusion, how to use Stale Diffusion,&nbsp;&nbsp;

- [00:01:30](https://www.youtube.com/watch?v=sRdtVanSRl4&t=90) what is Automatic1111 web UI, how to install&nbsp; it on your computer, I have excellent video&nbsp;&nbsp;

- [00:01:35](https://www.youtube.com/watch?v=sRdtVanSRl4&t=95) tutorial series as you can see right now. So&nbsp; in the first two video, I am showing how you&nbsp;&nbsp;

- [00:01:40](https://www.youtube.com/watch?v=sRdtVanSRl4&t=100) can use DreamBooth on Google Colab for free.&nbsp; In these two videos, I am showing how you can&nbsp;&nbsp;

- [00:01:46](https://www.youtube.com/watch?v=sRdtVanSRl4&t=106) install and run Automatic1111 web UI on your PC.&nbsp; In this video, I am explaining what is DreamBooth,&nbsp;&nbsp;

- [00:01:52](https://www.youtube.com/watch?v=sRdtVanSRl4&t=112) how does it work. So this is a major video that&nbsp; you should watch for learning DreamBooth training.&nbsp;&nbsp;

- [00:01:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=117) This is my another major video that you should&nbsp; watch to learn textual inversion. You can look all&nbsp;&nbsp;

- [00:02:02](https://www.youtube.com/watch?v=sRdtVanSRl4&t=122) other videos in here. I have videos for ControlNet&nbsp; and I have ultimate tutorial for RunPod IO. This&nbsp;&nbsp;

- [00:02:09](https://www.youtube.com/watch?v=sRdtVanSRl4&t=129) is really important if you don't know how to use&nbsp; RunPod. And this is my another very recent video&nbsp;&nbsp;

- [00:02:13](https://www.youtube.com/watch?v=sRdtVanSRl4&t=133) about DreamBooth training on RunPod. But now today&nbsp; we are going to make another one because there is&nbsp;&nbsp;

- [00:02:20](https://www.youtube.com/watch?v=sRdtVanSRl4&t=140) a major update of DreamBooth. So our new pod is&nbsp; started. Let's connect with connect to Jupyter&nbsp;&nbsp;

- [00:02:25](https://www.youtube.com/watch?v=sRdtVanSRl4&t=145) Lab. When you install a fresh RunPod, it already&nbsp; starts an instance of Automatic1111 web UI. You&nbsp;&nbsp;

- [00:02:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=152) can see the instance started here. However, this&nbsp; instance command line interface, the terminal you&nbsp;&nbsp;

- [00:02:38](https://www.youtube.com/watch?v=sRdtVanSRl4&t=158) can't see. So therefore, we are going to make two&nbsp; things. The first thing we are going to make is&nbsp;&nbsp;

- [00:02:43](https://www.youtube.com/watch?v=sRdtVanSRl4&t=163) preventing relauncher to continuous launching so&nbsp; that when we kill that initially started instance,&nbsp;&nbsp;

- [00:02:49](https://www.youtube.com/watch?v=sRdtVanSRl4&t=169) it won't relaunch again. All we need to do is&nbsp; changing this forever running while loop as&nbsp;&nbsp;

- [00:02:55](https://www.youtube.com/watch?v=sRdtVanSRl4&t=175) (n &lt; 1). So after one time trying relaunching,&nbsp; it won't relaunch again. The second thing we&nbsp;&nbsp;

- [00:03:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=180) need to do is open web UI dash user.sh file and&nbsp; add dash dash share command. So with this way,&nbsp;&nbsp;

- [00:03:07](https://www.youtube.com/watch?v=sRdtVanSRl4&t=187) we will be able to access our Automatic1111&nbsp; web UI instance from a Gradio link on our&nbsp;&nbsp;

- [00:03:13](https://www.youtube.com/watch?v=sRdtVanSRl4&t=193) browser like we are using it on our computer. No&nbsp; difference. Now I will restart pod for these to&nbsp;&nbsp;

- [00:03:18](https://www.youtube.com/watch?v=sRdtVanSRl4&t=198) be effective. To restart the pod, click here and&nbsp; click restart pod. Pod has been restarted. We are&nbsp;&nbsp;

- [00:03:24](https://www.youtube.com/watch?v=sRdtVanSRl4&t=204) going to install DreamBooth extension manually,&nbsp; not through the web UI interface. To do that,&nbsp;&nbsp;

- [00:03:29](https://www.youtube.com/watch?v=sRdtVanSRl4&t=209) go to the extensions folder and in here&nbsp; open a new launcher, new terminal from here,&nbsp;&nbsp;

- [00:03:34](https://www.youtube.com/watch?v=sRdtVanSRl4&t=214) copy the URL of the DreamBooth extension, type&nbsp; here git clone and paste the URL. It will clone&nbsp;&nbsp;

- [00:03:40](https://www.youtube.com/watch?v=sRdtVanSRl4&t=220) the DreamBooth extension. If you already have&nbsp; the extension, enter inside the extension folder,&nbsp;&nbsp;

- [00:03:45](https://www.youtube.com/watch?v=sRdtVanSRl4&t=225) open a new launcher here, type git pull command&nbsp; like this. It will pull the latest version for you&nbsp;&nbsp;

- [00:03:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=231) git pull. Once you have done this, we are going to&nbsp; install the requirements manually. For installing&nbsp;&nbsp;

- [00:03:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=237) requirements. We are going to run this command&nbsp; copy, paste it. So the command is pip install&nbsp;&nbsp;

- [00:04:03](https://www.youtube.com/watch?v=sRdtVanSRl4&t=243) dash dash upgrade dash dash no deps dash dash&nbsp; force reinstall dash R requirements txt. It will&nbsp;&nbsp;

- [00:04:09](https://www.youtube.com/watch?v=sRdtVanSRl4&t=249) install all of the requirements for DreamBooth&nbsp; extension. This is very convenient because it&nbsp;&nbsp;

- [00:04:14](https://www.youtube.com/watch?v=sRdtVanSRl4&t=254) is going to also install the newest xformers&nbsp; version so it will work out of box. You see,&nbsp;&nbsp;

- [00:04:19](https://www.youtube.com/watch?v=sRdtVanSRl4&t=259) it is attempting to uninstall Xformers then it is&nbsp; going to install the latest version of xformers&nbsp;&nbsp;

- [00:04:24](https://www.youtube.com/watch?v=sRdtVanSRl4&t=264) and in the end you will see successfully installed&nbsp; Accelerate requirements version and other things&nbsp;&nbsp;

- [00:04:30](https://www.youtube.com/watch?v=sRdtVanSRl4&t=270) as you can see. Are we done? No, unfortunately&nbsp; not. So I am opening requirements txt here.&nbsp;&nbsp;

- [00:04:35](https://www.youtube.com/watch?v=sRdtVanSRl4&t=275) Then I will open the requirements txt of the&nbsp; Automatic1111 web UI as well requirements versions&nbsp;&nbsp;

- [00:04:42](https://www.youtube.com/watch?v=sRdtVanSRl4&t=282) actually. So there are some conflicting versions&nbsp; of Automatic1111 web UI and the DreamBooth.&nbsp;&nbsp;

- [00:04:48](https://www.youtube.com/watch?v=sRdtVanSRl4&t=288) The very important one is Accelerate. You see&nbsp; DreamBooth is using 0.16 however, Automatic1111&nbsp;&nbsp;

- [00:04:54](https://www.youtube.com/watch?v=sRdtVanSRl4&t=294) currently using 0.12 therefore, you need to change&nbsp; the conflicting versions of Automatic1111 web UI&nbsp;&nbsp;

- [00:05:02](https://www.youtube.com/watch?v=sRdtVanSRl4&t=302) requirements to the same level as the DreamBooth.&nbsp; To do this, you can manually check each one of&nbsp;&nbsp;

- [00:05:07](https://www.youtube.com/watch?v=sRdtVanSRl4&t=307) the requirements in each file and compare them&nbsp; whether they are existing and conflicting or not.&nbsp;&nbsp;

- [00:05:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=312) Okay I see git python 3.1.27 and yes, this is a&nbsp; newer version in DreamBooth. So I will update the&nbsp;&nbsp;

- [00:05:20](https://www.youtube.com/watch?v=sRdtVanSRl4&t=320) version in Automatic1111 web UI requirements.&nbsp; Why I am doing that because when you launch&nbsp;&nbsp;

- [00:05:25](https://www.youtube.com/watch?v=sRdtVanSRl4&t=325) web UI it is overriding back to the lower version&nbsp; of requirement and that causes your DreamBooth to&nbsp;&nbsp;

- [00:05:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=332) fail. Okay I see also transformers is conflicting.&nbsp; So I am updating it and it is all done. So we&nbsp;&nbsp;

- [00:05:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=337) have changed it Accelerate, git python and the&nbsp; transformers version. That's it. Now we are ready&nbsp;&nbsp;

- [00:05:43](https://www.youtube.com/watch?v=sRdtVanSRl4&t=343) to relaunch our web UI instance. To do that first&nbsp; we are going to kill the instance that is already&nbsp;&nbsp;

- [00:05:49](https://www.youtube.com/watch?v=sRdtVanSRl4&t=349) running which is starting when you first time run&nbsp; the pod with this command fuse r-k the port of the&nbsp;&nbsp;

- [00:05:56](https://www.youtube.com/watch?v=sRdtVanSRl4&t=356) instance which is 3000 by default and slash tcp.&nbsp; You see it is killed I am seeing the message here.&nbsp;&nbsp;

- [00:06:03](https://www.youtube.com/watch?v=sRdtVanSRl4&t=363) Then you need to run relauncher.py with python&nbsp; relauncher.py it will run and we will have our&nbsp;&nbsp;

- [00:06:10](https://www.youtube.com/watch?v=sRdtVanSRl4&t=370) fully working DreamBooth extension. Okay after&nbsp; web UI if you get this error, this is caused by&nbsp;&nbsp;

- [00:06:16](https://www.youtube.com/watch?v=sRdtVanSRl4&t=376) the Tensorflow installation. All you need to do is&nbsp; running this command. After you run this command,&nbsp;&nbsp;

- [00:06:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=382) it will work. Okay there is one final thing that&nbsp; I had to do. You see if you get this error. After&nbsp;&nbsp;

- [00:06:29](https://www.youtube.com/watch?v=sRdtVanSRl4&t=389) all this, let me show you the error that cannot&nbsp; add middleware after an application has started,&nbsp;&nbsp;

- [00:06:35](https://www.youtube.com/watch?v=sRdtVanSRl4&t=395) you need to run one additional command which is&nbsp; pip install dash dash force fast api equal equal&nbsp;&nbsp;

- [00:06:42](https://www.youtube.com/watch?v=sRdtVanSRl4&t=402) 0.90.1. So unfortunately it is installing the&nbsp; latest version on a new RunPod as 0.92.0 however,&nbsp;&nbsp;

- [00:06:52](https://www.youtube.com/watch?v=sRdtVanSRl4&t=412) this is not working so we need to downgrade it&nbsp; and we are using this command to downgrade it.&nbsp;&nbsp;

- [00:06:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=417) Additionally, if you encounter any problem while&nbsp; launching it, join our discord channel. The link&nbsp;&nbsp;

- [00:07:02](https://www.youtube.com/watch?v=sRdtVanSRl4&t=422) will be in description and also in the comments&nbsp; and ask me your encountered problem. Let me show&nbsp;&nbsp;

- [00:07:08](https://www.youtube.com/watch?v=sRdtVanSRl4&t=428) you the freshly installed DreamBooth so you&nbsp; see, we have the DreamBooth tab with all of&nbsp;&nbsp;

- [00:07:13](https://www.youtube.com/watch?v=sRdtVanSRl4&t=433) the necessary versions and it is ready to work. So&nbsp; i'm going to start composing my first test. When&nbsp;&nbsp;

- [00:07:19](https://www.youtube.com/watch?v=sRdtVanSRl4&t=439) composing a new model, make sure that you have&nbsp; selected your source checkpoint, give it a name,&nbsp;&nbsp;

- [00:07:25](https://www.youtube.com/watch?v=sRdtVanSRl4&t=445) and then create model. Base test test0 is the same&nbsp; settings as previously known as the best settings.&nbsp;&nbsp;

- [00:07:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=452) Okay, now I will show you the best settings I am&nbsp; using. I mean as a base settings that were best&nbsp;&nbsp;

- [00:07:38](https://www.youtube.com/watch?v=sRdtVanSRl4&t=458) previously. So none of these are checked training&nbsp; steps per image epochs: this is up to you. I am&nbsp;&nbsp;

- [00:07:44](https://www.youtube.com/watch?v=sRdtVanSRl4&t=464) training up 200 epochs in this experiment. I will&nbsp; save every 10 epochs a model checkpoint. I will&nbsp;&nbsp;

- [00:07:50](https://www.youtube.com/watch?v=sRdtVanSRl4&t=470) save preview images every five epochs. Batch size&nbsp; and gradient accumulation steps are one. These are&nbsp;&nbsp;

- [00:07:56](https://www.youtube.com/watch?v=sRdtVanSRl4&t=476) working best for faces. Class batch size is one.&nbsp; Because i'm not going to generate classification&nbsp;&nbsp;

- [00:08:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=480) images. Why? Because I have pre-generated&nbsp; classification images, these are uploaded to a&nbsp;&nbsp;

- [00:08:06](https://www.youtube.com/watch?v=sRdtVanSRl4&t=486) google drive, so you will be also able to download&nbsp; and use them. The link will be in the description.&nbsp;&nbsp;

- [00:08:10](https://www.youtube.com/watch?v=sRdtVanSRl4&t=490) I'm not checking gradient checkpointing. The&nbsp; learning rate I am using is this one 0.000002. The&nbsp;&nbsp;

- [00:08:18](https://www.youtube.com/watch?v=sRdtVanSRl4&t=498) resolution is 512 because we are using 1.5 pruned&nbsp; ckpt. The sanity prompt is photo of ohwx man by&nbsp;&nbsp;

- [00:08:25](https://www.youtube.com/watch?v=sRdtVanSRl4&t=505) tomer hanuka. I'm not changing the sanity sample.&nbsp; It will be same in all of the RunPod instances in&nbsp;&nbsp;

- [00:08:31](https://www.youtube.com/watch?v=sRdtVanSRl4&t=511) all of the trainings. These are not set because&nbsp; we don't need. I am using optimizer 8Bit Adam.&nbsp;&nbsp;

- [00:08:36](https://www.youtube.com/watch?v=sRdtVanSRl4&t=516) I am not using EMA because this is increasing&nbsp; vram usage a lot. If you have 12 gigabyte vram&nbsp;&nbsp;

- [00:08:42](https://www.youtube.com/watch?v=sRdtVanSRl4&t=522) having gpu, you won't be able to run with the same&nbsp; settings. Mixed precision bf16 memory attention&nbsp;&nbsp;

- [00:08:47](https://www.youtube.com/watch?v=sRdtVanSRl4&t=527) xformers if your card is not supporting bf16, if&nbsp; it is an old card, then you need to select fp16 so&nbsp;&nbsp;

- [00:08:54](https://www.youtube.com/watch?v=sRdtVanSRl4&t=534) it depends on your gpu model. Cache latents, train&nbsp; UNET, step ratio of text encoding is 75 percent,&nbsp;&nbsp;

- [00:09:01](https://www.youtube.com/watch?v=sRdtVanSRl4&t=541) offset noise is zero, freeze clip normalization&nbsp; layers is unchecked. Clip skip is one AdamW&nbsp;&nbsp;

- [00:09:07](https://www.youtube.com/watch?v=sRdtVanSRl4&t=547) Weight decay is one percent and the other&nbsp; settings are same default. In the concepts&nbsp;&nbsp;

- [00:09:13](https://www.youtube.com/watch?v=sRdtVanSRl4&t=553) we are setting our data set directory as&nbsp; training images and classification images.&nbsp;&nbsp;

- [00:09:18](https://www.youtube.com/watch?v=sRdtVanSRl4&t=558) This is my training images data set as usual. No&nbsp; same backgrounds, no same clothings, different&nbsp;&nbsp;

- [00:09:24](https://www.youtube.com/watch?v=sRdtVanSRl4&t=564) shots of the face from different distances. I'm&nbsp; not using filewords because when teaching faces,&nbsp;&nbsp;

- [00:09:31](https://www.youtube.com/watch?v=sRdtVanSRl4&t=571) I find that no need to use filewords file&nbsp; captions so my instance prompt is ohwx man.&nbsp;&nbsp;

- [00:09:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=577) Ohwx is my rare token. Man is my class token.&nbsp; This will be used when training training images&nbsp;&nbsp;

- [00:09:43](https://www.youtube.com/watch?v=sRdtVanSRl4&t=583) class from this photo of man. This will be used&nbsp; when it is training for regularization to keep&nbsp;&nbsp;

- [00:09:48](https://www.youtube.com/watch?v=sRdtVanSRl4&t=588) the model sanity and prevent over training. So&nbsp; when you generate your classification images,&nbsp;&nbsp;

- [00:09:53](https://www.youtube.com/watch?v=sRdtVanSRl4&t=593) you should use same prompt that you have typed&nbsp; here. Sample image prompt is photo of ohwx&nbsp;&nbsp;

- [00:09:59](https://www.youtube.com/watch?v=sRdtVanSRl4&t=599) man. Okay, now this part we are going to use 50&nbsp; classification images per training instance: the&nbsp;&nbsp;

- [00:10:05](https://www.youtube.com/watch?v=sRdtVanSRl4&t=605) classification scale seven these matters if you&nbsp; generate these new classification images through&nbsp;&nbsp;

- [00:10:10](https://www.youtube.com/watch?v=sRdtVanSRl4&t=610) the DreamBooth extension, so you should also set&nbsp; these values. I'm going to generate two samples&nbsp;&nbsp;

- [00:10:16](https://www.youtube.com/watch?v=sRdtVanSRl4&t=616) during training. The sample seed will be minus one&nbsp; okay, and these are the other settings. Okay. In&nbsp;&nbsp;

- [00:10:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=622) saving tab, we are going to generate a ckpt file&nbsp; when saving during training. So we did set save&nbsp;&nbsp;

- [00:10:30](https://www.youtube.com/watch?v=sRdtVanSRl4&t=630) every 10 epoch. So every 10 epoch it will generate&nbsp; a ckpt file. Make sure that you have sufficient&nbsp;&nbsp;

- [00:10:35](https://www.youtube.com/watch?v=sRdtVanSRl4&t=635) amount of hard drive so this will generate 20&nbsp; checkpoints and it will take roughly 100 gigabytes&nbsp;&nbsp;

- [00:10:43](https://www.youtube.com/watch?v=sRdtVanSRl4&t=643) disk space. And in the testing tab, I am going to&nbsp; pick deterministic so it should be same in all of&nbsp;&nbsp;

- [00:10:50](https://www.youtube.com/watch?v=sRdtVanSRl4&t=650) the training experimentation. This is supposed to&nbsp; make your training reproducible. Save settings,&nbsp;&nbsp;

- [00:10:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=657) hit train, and the training for first experiment,&nbsp; the base experiment, the test zero is started. Now&nbsp;&nbsp;

- [00:11:04](https://www.youtube.com/watch?v=sRdtVanSRl4&t=664) time to start other tests. Okay, when generating&nbsp; test one, I am going to show you a neat trick I&nbsp;&nbsp;

- [00:11:10](https://www.youtube.com/watch?v=sRdtVanSRl4&t=670) am using. I am first selecting test zero. This&nbsp; test zero settings exist in this machine as well&nbsp;&nbsp;

- [00:11:15](https://www.youtube.com/watch?v=sRdtVanSRl4&t=675) because I have cloned it. Load settings then go to&nbsp; the create tab. Type test1. Select the pruned ckpt&nbsp;&nbsp;

- [00:11:21](https://www.youtube.com/watch?v=sRdtVanSRl4&t=681) file, hit create model. Now test one is selected&nbsp; here and the settings are loaded so they didn't&nbsp;&nbsp;

- [00:11:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=688) changed when a new model is generated so that I&nbsp; won't be need to set all of the parameters again&nbsp;&nbsp;

- [00:11:34](https://www.youtube.com/watch?v=sRdtVanSRl4&t=694) and again. The only different parameter for test&nbsp; one is use DEIS for noise scheduler so it is an&nbsp;&nbsp;

- [00:11:40](https://www.youtube.com/watch?v=sRdtVanSRl4&t=700) option a new option here. It changes the noise&nbsp; scheduler and I am setting deterministic save&nbsp;&nbsp;

- [00:11:46](https://www.youtube.com/watch?v=sRdtVanSRl4&t=706) settings. You can also click load settings to be&nbsp; sure, then click train and it will start training.&nbsp;&nbsp;

- [00:11:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=711) So test one has been started now time to go for&nbsp; test two. The only different thing in test two is&nbsp;&nbsp;

- [00:11:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=717) unfreeze model and so we are going to test it.&nbsp; Create model with unfreeze model option okay,&nbsp;&nbsp;

- [00:12:03](https://www.youtube.com/watch?v=sRdtVanSRl4&t=723) save settings load settings. Test two is selected,&nbsp; make sure that you see hit train and test two is&nbsp;&nbsp;

- [00:12:10](https://www.youtube.com/watch?v=sRdtVanSRl4&t=730) started okay. Test three is Lion optimizer which&nbsp; is a new optimizer we select in the advanced tab&nbsp;&nbsp;

- [00:12:17](https://www.youtube.com/watch?v=sRdtVanSRl4&t=737) which is here. Instead of optimizer 8Bit Adam&nbsp; we are going to select Lion. When we use Lion&nbsp;&nbsp;

- [00:12:23](https://www.youtube.com/watch?v=sRdtVanSRl4&t=743) optimizer, it is suggested to use 10% of the&nbsp; original learning rate, so I will set it as&nbsp;&nbsp;

- [00:12:30](https://www.youtube.com/watch?v=sRdtVanSRl4&t=750) that. So I am changing my learning rate to this&nbsp; one. You see 0.0000002 hit save settings and hit&nbsp;&nbsp;

- [00:12:39](https://www.youtube.com/watch?v=sRdtVanSRl4&t=759) train. So the test three has been started. Okay&nbsp; for the test four, we are going to use the new&nbsp;&nbsp;

- [00:12:45](https://www.youtube.com/watch?v=sRdtVanSRl4&t=765) feature Offset Noise. If you wonder what is Offset&nbsp; Noise, it is supposed to improve generating dark&nbsp;&nbsp;

- [00:12:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=771) or light images easily. So which value of Offset&nbsp; Noise we should use? Recommended value is between&nbsp;&nbsp;

- [00:12:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=777) five percent and ten percent. So I will pick this&nbsp; seven percent and this setting is available in the&nbsp;&nbsp;

- [00:13:04](https://www.youtube.com/watch?v=sRdtVanSRl4&t=784) advanced tab. In the bottom, you will see Offset&nbsp; Noise. So let's set it as seven percent like this&nbsp;&nbsp;

- [00:13:10](https://www.youtube.com/watch?v=sRdtVanSRl4&t=790) okay and hit save settings, load settings,&nbsp; make sure that the correct model is selected,&nbsp;&nbsp;

- [00:13:16](https://www.youtube.com/watch?v=sRdtVanSRl4&t=796) hit train and it is started. Okay in the&nbsp; test five, we are going to use Freeze Clip&nbsp;&nbsp;

- [00:13:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=802) Normalization Layers which is an another option&nbsp; in the advanced tab. So i'm setting it, hit save,&nbsp;&nbsp;

- [00:13:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=808) load settings and hit train. So the test five&nbsp; has already started. Okay, now time to do the&nbsp;&nbsp;

- [00:13:34](https://www.youtube.com/watch?v=sRdtVanSRl4&t=814) tests that require more than 12 gigabyte vram&nbsp; having gpus. So the first test we are going to&nbsp;&nbsp;

- [00:13:41](https://www.youtube.com/watch?v=sRdtVanSRl4&t=821) do is EMA plus EMA for prediction. If you have&nbsp; only 12 gigabytes having GPU, unfortunately,&nbsp;&nbsp;

- [00:13:48](https://www.youtube.com/watch?v=sRdtVanSRl4&t=828) you won't be able to run this test. For this&nbsp; test to work first we need to enable EMA during&nbsp;&nbsp;

- [00:13:53](https://www.youtube.com/watch?v=sRdtVanSRl4&t=833) training from advanced tab. Use EMA. Then we need&nbsp; to pick use EMA for prediction, save settings&nbsp;&nbsp;

- [00:14:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=840) and the training has been started. Unfortunately,&nbsp; the graduate interface is frozen. This may happen&nbsp;&nbsp;

- [00:14:05](https://www.youtube.com/watch?v=sRdtVanSRl4&t=845) sometimes, but still you can see the process in&nbsp; your terminal window as you can see right now.&nbsp;&nbsp;

- [00:14:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=852) My test seven RunPod is currently do not have any&nbsp; GPU available. Therefore, I will run test seven&nbsp;&nbsp;

- [00:14:20](https://www.youtube.com/watch?v=sRdtVanSRl4&t=860) on my test zero RunPod. So you may encounter&nbsp; this problem. When you encounter this problem,&nbsp;&nbsp;

- [00:14:26](https://www.youtube.com/watch?v=sRdtVanSRl4&t=866) you can still start your RunPod, get your files,&nbsp; download them. However, you have to wait until a&nbsp;&nbsp;

- [00:14:33](https://www.youtube.com/watch?v=sRdtVanSRl4&t=873) GPU gets free on this RunPod, to use GPU on that&nbsp; RunPod. Okay for test seven, we are going to use&nbsp;&nbsp;

- [00:14:40](https://www.youtube.com/watch?v=sRdtVanSRl4&t=880) EMA from here and then we are going to enable use&nbsp; EMA weights for inference in the saving tab, save&nbsp;&nbsp;

- [00:14:48](https://www.youtube.com/watch?v=sRdtVanSRl4&t=888) settings, load settings, and hit train. So the&nbsp; test seven is also started. Now time to do test&nbsp;&nbsp;

- [00:14:55](https://www.youtube.com/watch?v=sRdtVanSRl4&t=895) eight. In the test eight only difference is use&nbsp; EMA so let's set it and start the training. Use&nbsp;&nbsp;

- [00:15:01](https://www.youtube.com/watch?v=sRdtVanSRl4&t=901) EMA here. Save settings, load settings, and hit&nbsp; train. Okay, I have encountered the configuration&nbsp;&nbsp;

- [00:15:08](https://www.youtube.com/watch?v=sRdtVanSRl4&t=908) error. This may happen sometimes. Just reload the&nbsp; web UI, refresh model list, select your model,&nbsp;&nbsp;

- [00:15:15](https://www.youtube.com/watch?v=sRdtVanSRl4&t=915) load settings. You may want to quickly verify&nbsp; your settings. Hit train and now it is started&nbsp;&nbsp;

- [00:15:21](https://www.youtube.com/watch?v=sRdtVanSRl4&t=921) you see. Okay for the test nine we are not going&nbsp; to use xformers. To be sure that it is not used&nbsp;&nbsp;

- [00:15:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=928) I will disable xformers command from the command&nbsp; line arguments from here webui-user.sh file: once&nbsp;&nbsp;

- [00:15:38](https://www.youtube.com/watch?v=sRdtVanSRl4&t=938) you do this, you need to restart webui obviously.&nbsp; Okay looks like test nine Pod also, don't have any&nbsp;&nbsp;

- [00:15:45](https://www.youtube.com/watch?v=sRdtVanSRl4&t=945) GPU at the moment, so I will run this test nine&nbsp; on test one Pod. Because this has been completed,&nbsp;&nbsp;

- [00:15:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=951) I will add more disk space to it and use that.&nbsp; Okay, now time to do test nine. In the test nine,&nbsp;&nbsp;

- [00:15:58](https://www.youtube.com/watch?v=sRdtVanSRl4&t=958) we are not going to use xformers. I have started&nbsp; the webui instance without xformers, so now in&nbsp;&nbsp;

- [00:16:04](https://www.youtube.com/watch?v=sRdtVanSRl4&t=964) the advanced tab, I will select memory attention&nbsp; default mixed precision fp16 deterministic save&nbsp;&nbsp;

- [00:16:11](https://www.youtube.com/watch?v=sRdtVanSRl4&t=971) settings, load settings, make sure that settings&nbsp; are correct and hit train. You can also see that&nbsp;&nbsp;

- [00:16:18](https://www.youtube.com/watch?v=sRdtVanSRl4&t=978) no module xformers proceeding without it in this&nbsp; instance because the xformers is removed from the&nbsp;&nbsp;

- [00:16:25](https://www.youtube.com/watch?v=sRdtVanSRl4&t=985) command line arguments. Okay, now I noticed&nbsp; something else. When you use EMA + EMA for&nbsp;&nbsp;

- [00:16:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=992) prediction, it uses nine gigabyte VRAM. However,&nbsp; when you use EMA plus EMA weights for inference,&nbsp;&nbsp;

- [00:16:39](https://www.youtube.com/watch?v=sRdtVanSRl4&t=999) it uses 12.5 gigabyte VRAM. And when you use only&nbsp; EMA, it uses 12.5 gigabyte VRAM. And when we don't&nbsp;&nbsp;

- [00:16:48](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1008) use xformers and when we use fp16, it is using&nbsp; 17.4 gigabyte VRAM and we are not even using EMA.&nbsp;&nbsp;

- [00:16:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1017) You see, EMA is false in these ones EMA is true.&nbsp; EMA is true. EMA is true because this is test six,&nbsp;&nbsp;

- [00:17:04](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1024) test seven, test eight and now we are running&nbsp; test nine. Okay, looks like for test nine to run,&nbsp;&nbsp;

- [00:17:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1032) we have to enable xformers, but we will not pick&nbsp; xformers in the settings because it is not working&nbsp;&nbsp;

- [00:17:19](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1039) when we run it without xformers. Yeah, this is&nbsp; definitely an error probably in the programming,&nbsp;&nbsp;

- [00:17:25](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1045) so it is trying to use somewhere xformers&nbsp; even if we don't pick it. Yeah. Okay,&nbsp;&nbsp;

- [00:17:31](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1051) so I am restarting the test nine this time.&nbsp; Xformers flag is given, but I am just selecting&nbsp;&nbsp;

- [00:17:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1057) default fp16 so let's see if it will work this&nbsp; time or not. Okay, load settings hit train. Okay,&nbsp;&nbsp;

- [00:17:43](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1063) it is using 17.4 gigabytes VRAM again. So since&nbsp; we didn't enable xformers, but I hope we don't&nbsp;&nbsp;

- [00:17:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1071) get error this time. Okay, looks like error&nbsp; were caused when it was generating samples.&nbsp;&nbsp;

- [00:17:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1077) Probably the error cause was it was expecting to&nbsp; use xformers during generating image, but it was&nbsp;&nbsp;

- [00:18:04](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1084) not enabled and you see after generating samples&nbsp; the VRAM usage also dropped. This is really really&nbsp;&nbsp;

- [00:18:09](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1089) interesting. There is definitely some bug in&nbsp; the script, but it is working so far. Good. In&nbsp;&nbsp;

- [00:18:15](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1095) the experiment six, I have encountered errors two&nbsp; times. For example, the last encountered error is&nbsp;&nbsp;

- [00:18:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1102) happened at the step 4320. So basically I am going&nbsp; to continue training with the remaining epochs.&nbsp;&nbsp;

- [00:18:31](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1111) How many epochs I need more? I need 20 epochs&nbsp; because our target epoch is 200 and the model&nbsp;&nbsp;

- [00:18:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1117) epoch is currently 180. Therefore, set the epoch&nbsp; count as 20. Save settings, load settings, and&nbsp;&nbsp;

- [00:18:45](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1125) hit train and then you can continue training if&nbsp; you encounter such errors during training. Okay,&nbsp;&nbsp;

- [00:18:50](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1130) all tests have been completed. I have downloaded&nbsp; the samples of all tests. Let's take a look at&nbsp;&nbsp;

- [00:18:56](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1136) the samples. This is the base test. Test zero.&nbsp; My face becomes like me after like 480 steps. So&nbsp;&nbsp;

- [00:19:04](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1144) let's take notes. Test zero. Okay, 480 steps&nbsp; beginning we can say then when it loses its&nbsp;&nbsp;

- [00:19:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1152) styling ability is okay i'm checking. Yeah, it's&nbsp; still able to keep up. Okay, after 2040 steps and&nbsp;&nbsp;

- [00:19:21](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1161) then it becomes you see really really bad as you&nbsp; can see. Okay, let's take a look at the test one.&nbsp;&nbsp;

- [00:19:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1168) So in the test one the differences we are using&nbsp; DEIS noise scheduler. The face becomes like me&nbsp;&nbsp;

- [00:19:35](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1175) after again 480 steps. And when does it loses&nbsp; its styling capability is i'm checking. I think&nbsp;&nbsp;

- [00:19:44](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1184) it loses its capability same yeah 2040. Okay, this&nbsp; is test two in the test two yeah 480 steps. Oh I&nbsp;&nbsp;

- [00:19:52](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1192) see a nice one here. Okay and when does it loses&nbsp; its styling capability is okay i am checking that.&nbsp;&nbsp;

- [00:20:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1200) So like 2040 steps again. So nothing different for&nbsp; test two as well. Which is unfreeze model. By the&nbsp;&nbsp;

- [00:20:08](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1208) way we will compare how well they are performing&nbsp; when we use a certain prompt so do not worry about&nbsp;&nbsp;

- [00:20:15](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1215) it. I have a systematic for that I will explain.&nbsp; So let's check out the test three. Okay in the&nbsp;&nbsp;

- [00:20:21](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1221) test three we start to see some difference than&nbsp; the previous ones. We can say the face starts to&nbsp;&nbsp;

- [00:20:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1228) appear after yeah yeah let's say okay yeah none of&nbsp; them is really looking good. So maybe maybe I will&nbsp;&nbsp;

- [00:20:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1237) say this one 1080 steps and when does it loses&nbsp; its styling capability is i'm checking. Wow it&nbsp;&nbsp;

- [00:20:45](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1245) is really able to keep styling pretty decent if&nbsp; you ask me. So when does the test three lost its&nbsp;&nbsp;

- [00:20:53](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1253) styling capability is yeah right after the best&nbsp; one 3120 steps. Okay so in the test three we were&nbsp;&nbsp;

- [00:21:02](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1262) using Lion optimizer. Looks like Lion optimizer&nbsp; is yeah looks like better than the previous one&nbsp;&nbsp;

- [00:21:08](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1268) actually. So the Lion optimizer made the biggest&nbsp; difference among all other settings so far. Okay,&nbsp;&nbsp;

- [00:21:15](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1275) now let's look at the test four. In the test four,&nbsp; we see that styling capability of the model is&nbsp;&nbsp;

- [00:21:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1282) lost pretty quickly just after 600 steps and and&nbsp; the face also appeared pretty quickly. We can say&nbsp;&nbsp;

- [00:21:31](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1291) that after 600 steps, so let's take note of them.&nbsp; Okay now let's see the test five results. In the&nbsp;&nbsp;

- [00:21:39](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1299) test five I see that the face appeared in the 480&nbsp; steps and the styling capability is lost after&nbsp;&nbsp;

- [00:21:48](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1308) 2040 steps so not much looking like different from&nbsp; the first ones. In the test5 we used freeze clip&nbsp;&nbsp;

- [00:21:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1317) normalization layers, but there is a styled image&nbsp; here which looks like pretty decent. So we will&nbsp;&nbsp;

- [00:22:05](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1325) see the results of these when we are doing full&nbsp; comparison with x/y plot and a nice prompt. So I&nbsp;&nbsp;

- [00:22:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1332) have taken note of the test five as well. Let's&nbsp; move to the test six. Okay, in the test six,&nbsp;&nbsp;

- [00:22:18](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1338) we see that the face somewhat appeared after&nbsp; only 480 steps, but none of it looking good&nbsp;&nbsp;

- [00:22:26](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1346) actually. So the face is looking weird than the&nbsp; original training data set. Unlike the others,&nbsp;&nbsp;

- [00:22:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1352) the styling capability of the model is never&nbsp; lost. This is pretty significant. It is always&nbsp;&nbsp;

- [00:22:39](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1359) able to stylize the model, stylize the output&nbsp; you see never lost. So this is a very significant&nbsp;&nbsp;

- [00:22:48](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1368) difference than others. But the face is not&nbsp; looking good, so we will see how it will behave&nbsp;&nbsp;

- [00:22:53](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1373) when we are doing comparison. I am wondering that&nbsp; as well. In the test six, you see we are using EMA&nbsp;&nbsp;

- [00:23:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1380) plus EMA for prediction. So this is the first test&nbsp; I have used EMA as an experiment and the results&nbsp;&nbsp;

- [00:23:07](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1387) are significantly different than others. Actually&nbsp; or styling, I will save 4320 after that not much&nbsp;&nbsp;

- [00:23:15](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1395) styled. This is just looking as black and white.&nbsp; So the result is for test six is 400 steps first&nbsp;&nbsp;

- [00:23:21](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1401) face and 4320 steps loses styling ability. Now&nbsp; let's look at the test seven. In the test seven,&nbsp;&nbsp;

- [00:23:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1408) we see that the face appeared almost as soon&nbsp; as 240 steps or I will take 480. Yeah, this&nbsp;&nbsp;

- [00:23:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1417) looks bad. Yeah, this looks better. 480 steps and&nbsp; the styling capability is quickly lost. Actually,&nbsp;&nbsp;

- [00:23:45](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1425) after 840 steps after that, I don't see the&nbsp; styling and then it quickly becomes pretty&nbsp;&nbsp;

- [00:23:53](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1433) over trained. So this became over trained pretty&nbsp; quickly. So the test seven is 480 steps first face&nbsp;&nbsp;

- [00:24:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1440) and 840 steps loses styling ability. In the test&nbsp; seven, we used EMA + EMA weights for inference.&nbsp;&nbsp;

- [00:24:08](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1448) As you can see in both cases, the model trained&nbsp; pretty quickly, but this one has much better faces&nbsp;&nbsp;

- [00:24:15](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1455) than EMA for prediction and the test seven. Okay,&nbsp; in the test seven, we see that the first face 360.&nbsp;&nbsp;

- [00:24:23](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1463) Yeah. And then the styling ability is lost after&nbsp; 840. Yeah, this is like the previous one pretty&nbsp;&nbsp;

- [00:24:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1472) much over trained quickly. So for test 8, 360&nbsp; first face and 840 steps loses styling ability.&nbsp;&nbsp;

- [00:24:41](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1481) So in the test eight we only used EMA weights. The&nbsp; face is looking decent but it became over trained&nbsp;&nbsp;

- [00:24:49](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1489) pretty quickly so we will see how it will perform&nbsp; in the end. And for the test nine. In this test&nbsp;&nbsp;

- [00:24:55](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1495) we used the best settings of the default settings.&nbsp; But we didn't use xformers and instead of xformers&nbsp;&nbsp;

- [00:25:02](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1502) we used default memory attention. So let's see&nbsp; how it performed. This is supposed to working&nbsp;&nbsp;

- [00:25:08](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1508) better than the original because xformers is&nbsp; supposed to reducing your training quality. Okay,&nbsp;&nbsp;

- [00:25:13](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1513) I see the first face somewhat in 600 steps and the&nbsp; styling capability is kept until 1560 steps. The&nbsp;&nbsp;

- [00:25:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1522) results are looking decent. We will see in the end&nbsp; how it performs. I can say that without xformers&nbsp;&nbsp;

- [00:25:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1528) it is trained faster than with the xformers&nbsp; version. Okay, we have done all the comparison&nbsp;&nbsp;

- [00:25:34](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1534) of the samples. Now time to do final comparison.&nbsp; Okay now time to do our final tests. For these&nbsp;&nbsp;

- [00:25:43](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1543) first we are going to find a good seed. Then with&nbsp; this seed and the prompts we have prepared, we are&nbsp;&nbsp;

- [00:25:50](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1550) going to compare every checkpoint of every trained&nbsp; model. Test one test two, test three, test nine,&nbsp;&nbsp;

- [00:25:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1557) test zero. Then we will analyze these checkpoints&nbsp; of every test and get their best checkpoint and&nbsp;&nbsp;

- [00:26:06](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1566) we will compare all of the best checkpoints of&nbsp; all trained models and we will decide which one&nbsp;&nbsp;

- [00:26:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1572) is performing best. So I am going to find my good&nbsp; seed on the test zero, which is our base test and&nbsp;&nbsp;

- [00:26:21](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1581) I am going to use 50 epoch of the base test.&nbsp; Actually, how did I decide this 50 epoch to&nbsp;&nbsp;

- [00:26:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1588) test it to find the good seed? This is same as our&nbsp; previously known experiment 50 epoch and I decided&nbsp;&nbsp;

- [00:26:36](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1596) it by looking at the sample images. 50 epoch is&nbsp; looking like best 1200 steps. Okay, let's find the&nbsp;&nbsp;

- [00:26:44](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1604) good seed first. Okay, I think I got a decent seed&nbsp; to test that. I have shortened the prompt for both&nbsp;&nbsp;

- [00:26:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1611) negative and positive prompt because very long&nbsp; prompt drives the image away from yourself from&nbsp;&nbsp;

- [00:26:58](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1618) your subject. So making too long prompt is not&nbsp; very good in some cases. This applies for negative&nbsp;&nbsp;

- [00:27:05](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1625) prompt as well. I am going to share all of the&nbsp; prompts and other settings used in this experiment&nbsp;&nbsp;

- [00:27:11](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1631) on a gist post on my GitHub. This will be public&nbsp; and this will be in the description. So let me&nbsp;&nbsp;

- [00:27:19](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1639) show you the images. This is the first image.&nbsp; These are the original training images and this&nbsp;&nbsp;

- [00:27:24](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1644) is the first image. This is the second image. This&nbsp; is the third image. This is the fourth image. This&nbsp;&nbsp;

- [00:27:30](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1650) is the fifth image. This is the sixth image. This&nbsp; is the seventh image and this is the eighth image.&nbsp;&nbsp;

- [00:27:35](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1655) So let's test this setup on all of the training&nbsp; sets on all of the experiments and find out the&nbsp;&nbsp;

- [00:27:44](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1664) best checkpoint for each of the experiment. So for&nbsp; testing this, we are going to use: x/y plot. Open&nbsp;&nbsp;

- [00:27:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1671) the x/y plot in the below. But before doing that,&nbsp; verify that your seed is working as expected. You&nbsp;&nbsp;

- [00:27:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1677) need to use the seed of the first image displayed&nbsp; in here because when you generate images as batch,&nbsp;&nbsp;

- [00:28:03](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1683) it starts from your seed and increase it by one&nbsp; for other images. So if you use the second image&nbsp;&nbsp;

- [00:28:09](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1689) as a starting point, then the ending image will&nbsp; be different than this data set. Okay, in the x/y&nbsp;&nbsp;

- [00:28:15](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1695) plot, we are going to use checkpoint name. A box&nbsp; should appear here to fill all of the checkpoints.&nbsp;&nbsp;

- [00:28:23](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1703) If it is not appearing like in my case right now,&nbsp; you should refresh your web UI instance, which&nbsp;&nbsp;

- [00:28:29](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1709) I am going to do. Okay after I did refresh, it&nbsp; appeared. Click this. Select the checkpoints that&nbsp;&nbsp;

- [00:28:35](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1715) you want to test. You see now it is automatically&nbsp; generating safe tensors files instead of ckpt&nbsp;&nbsp;

- [00:28:41](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1721) files. This is the safe version of ckpt. This file&nbsp; cannot contain any harmful applications hidden&nbsp;&nbsp;

- [00:28:48](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1728) inside your model. Okay, all settings are set for&nbsp; test zero. I am also going to add a grid margin 20&nbsp;&nbsp;

- [00:28:56](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1736) pixel like this. Also, do not check keep minus one&nbsp; for seeds. Otherwise you won't be able to compare&nbsp;&nbsp;

- [00:29:03](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1743) same seed between different checkpoints and then&nbsp; hit generate. Okay, all tests have been completed.&nbsp;&nbsp;

- [00:29:09](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1749) Now I will show you where you can download the&nbsp; generated image. It is inside, outputs inside,&nbsp;&nbsp;

- [00:29:15](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1755) text to image grids. In, go to the latest folder&nbsp; and in here you will see a png file which will&nbsp;&nbsp;

- [00:29:23](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1763) be pretty big. For example, 72.9 megabytes.&nbsp; Right click download. So this is how you can&nbsp;&nbsp;

- [00:29:30](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1770) download the grid files. All grid files have been&nbsp; downloaded. No we will compare them and see which&nbsp;&nbsp;

- [00:29:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1777) one of the checkpoint is performing best. So let's&nbsp; begin with the test: zero. Okay, in the test zero,&nbsp;&nbsp;

- [00:29:44](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1784) it is starting from epoch 10 and every 10 epoch&nbsp; is saved. So this is epoch 10. This is epoch 20.&nbsp;&nbsp;

- [00:29:52](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1792) Still not my face. Epoch 30 I see my face. I am&nbsp; starting to see my face. Epoch 40 50 and it goes&nbsp;&nbsp;

- [00:30:01](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1801) on 60 70, 80, 90, 100, 110 epochs okay, I see&nbsp; a very good clear image here. The stylizing and&nbsp;&nbsp;

- [00:30:13](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1813) the quality is still significantly good. 120&nbsp; 130, 140 the quality is still really good. It&nbsp;&nbsp;

- [00:30:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1822) looks like they have improved the DreamBooth&nbsp; extension. Okay, on 150 or even more. This&nbsp;&nbsp;

- [00:30:29](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1829) is okay. This is 180 you see stylizing is almost&nbsp; gone. Uh, it is just drawing out my image. Okay,&nbsp;&nbsp;

- [00:30:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1837) this is 190 epochs but the quality is still good.&nbsp; And this is the 200. Yes, we can see it is already&nbsp;&nbsp;

- [00:30:45](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1845) becoming over trained. So which epoch is best? It&nbsp; is hard to decide which epoch is looking best. It&nbsp;&nbsp;

- [00:30:53](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1853) is a subjective term of course, but from images.&nbsp; Yeah, I am having hard time to decide which epoch&nbsp;&nbsp;

- [00:31:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1860) is looking best. They are really all good quality,&nbsp; but this one is looking like the best one. So this&nbsp;&nbsp;

- [00:31:07](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1867) is epoch 140 epoch 130 is also pretty decent so it&nbsp; is hard to decide. As I said, but I will say 140&nbsp;&nbsp;

- [00:31:17](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1877) epoch test 0 140 epoch looking like best. 30 60&nbsp; steps. Okay, now time to move test1. Okay, so in&nbsp;&nbsp;

- [00:31:27](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1887) the test one, let's see even in the 10 epoch there&nbsp; is some similarity, but not much. In the 20 epoch,&nbsp;&nbsp;

- [00:31:34](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1894) there is certainly similarity of the face. In the&nbsp; 30 epoch it becomes more like my face and in 40&nbsp;&nbsp;

- [00:31:42](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1902) epoch it is even getting better. This is 50 epoch&nbsp; looking very good. 60 epoch looking very decent by&nbsp;&nbsp;

- [00:31:50](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1910) the way. If we remember test one was use DEIS for&nbsp; noise scheduler, we are seeing pretty good results&nbsp;&nbsp;

- [00:31:56](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1916) overall. So it is again hard to decide which&nbsp; one is looking best. But I can say that after&nbsp;&nbsp;

- [00:32:02](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1922) 100 epochs it becomes more like over trained.&nbsp; I see that stylizing is kind of reduced or not&nbsp;&nbsp;

- [00:32:10](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1930) Maybe? Yeah, really really hard to decide. All of&nbsp; them is looking pretty good. Good quality. There&nbsp;&nbsp;

- [00:32:16](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1936) is no degradation in the faces. Pretty decent.&nbsp; Okay, so how are we going to decide which one of&nbsp;&nbsp;

- [00:32:24](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1944) the epoch is best for test1? It is really really&nbsp; good. Actually, it only becomes over trained in&nbsp;&nbsp;

- [00:32:30](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1950) the very end as you can see, so this is the 200&nbsp; epoch, 190 epoch 180 170. Okay, so according to&nbsp;&nbsp;

- [00:32:41](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1961) me let's see which one is best. Really hard to&nbsp; decide. It is really hard to decide, but I think&nbsp;&nbsp;

- [00:32:47](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1967) I will go with 3120 steps which is 130 epoch&nbsp; looking best. Okay, now we are at the test two,&nbsp;&nbsp;

- [00:32:56](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1976) which is unfreeze model. Let's see if there is any&nbsp; significant difference between the base or not.&nbsp;&nbsp;

- [00:33:04](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1984) Okay, I see decent images, but let's see which&nbsp; one is best. It has added me a good mustache&nbsp;&nbsp;

- [00:33:10](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1990) actually. Even though none of my pictures have&nbsp; mustache, let me show you once again. So you see,&nbsp;&nbsp;

- [00:33:16](https://www.youtube.com/watch?v=sRdtVanSRl4&t=1996) I have no mustache in any of them. But there's a&nbsp; good mustache here. Okay, so which one is best?&nbsp;&nbsp;

- [00:33:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2002) This is the million dollar question. Which one is&nbsp; looking best? wow, this is really cool one. Okay,&nbsp;&nbsp;

- [00:33:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2008) let's see. This is looking very very good.&nbsp; The images are looking very very good. I think&nbsp;&nbsp;

- [00:33:33](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2013) there is an overall improvement in the DreamBooth&nbsp; training when compared to previous versions. Okay,&nbsp;&nbsp;

- [00:33:39](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2019) this is 130 epoch, 140 epoch. Actually, this is&nbsp; 150 epoch. This is 160 epoch for test two 170 180&nbsp;&nbsp;

- [00:33:52](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2032) epoch 190 epoch. Each epoch is still looking good,&nbsp; producing different images and this is 200 epoch.&nbsp;&nbsp;

- [00:34:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2040) The results are really really good. So which one&nbsp; should we take? Yeah, I am having hard time once&nbsp;&nbsp;

- [00:34:07](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2047) again to decide because they are all looking good&nbsp; so it is not like one of them is best. So what&nbsp;&nbsp;

- [00:34:13](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2053) could you do in such situation? You could do batch&nbsp; processing and generate images on the different&nbsp;&nbsp;

- [00:34:20](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2060) epochs and pick the best ones you like. So you&nbsp; would use all of the epochs and you would get&nbsp;&nbsp;

- [00:34:27](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2067) the best images you want. You don't have to stick&nbsp; one epoch. You can choose the best working epochs&nbsp;&nbsp;

- [00:34:34](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2074) and generate images on all of them with xy plot.&nbsp; This strategy would work very well. For this video&nbsp;&nbsp;

- [00:34:40](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2080) I think I am going to take 100 epoch. Yes, I will&nbsp; take 100 epoch for test two. Okay, now test three&nbsp;&nbsp;

- [00:34:49](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2089) in the test three. We did Lion optimizer this&nbsp; is a new optimizer that has been just added and&nbsp;&nbsp;

- [00:34:56](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2096) let's see in the 10 epoch, not my face. 20, not my&nbsp; face. 30 somewhat similar 40 still, yeah, somewhat&nbsp;&nbsp;

- [00:35:05](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2105) similar 50 somewhat okay 60 epoch stylizing is&nbsp; much better. You see it is completely stylized.&nbsp;&nbsp;

- [00:35:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2112) It is following the prompt. Significantly better&nbsp; than others as you can see. Okay, we are almost at&nbsp;&nbsp;

- [00:35:19](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2119) 100 epoch. But yeah, the pictures are really good.&nbsp; I think this optimizer is working better than the&nbsp;&nbsp;

- [00:35:26](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2126) other for stylizing, but it is up to you to pick&nbsp; the which one you want. Okay, this one is looking&nbsp;&nbsp;

- [00:35:31](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2131) pretty decent so it is hard to decide. This is 100&nbsp; epoch. This is 110 epoch. This is 120 epoch. This&nbsp;&nbsp;

- [00:35:42](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2142) is 130 epoch. This is 140 epoch. This is 150&nbsp; epoch. This optimizer is making a significant&nbsp;&nbsp;

- [00:35:50](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2150) difference in your training results. So this is&nbsp; 160 epoch. I see some over training perhaps. Yes,&nbsp;&nbsp;

- [00:35:58](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2158) okay, this is 180 epoch. And yeah, pretty decent.&nbsp; The results are pretty pretty decent. Okay,&nbsp;&nbsp;

- [00:36:06](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2166) hard to decide which one to take. This is 190&nbsp; epoch. Wow. The quality is amazing. Okay, so which&nbsp;&nbsp;

- [00:36:14](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2174) one should we take? This is 190 epoch. Yeah, very&nbsp; good image there is. And this is 200 epoch. Okay,&nbsp;&nbsp;

- [00:36:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2182) wow. This is like taken from a movie. It looks&nbsp; like taken from an real movie actually. Okay,&nbsp;&nbsp;

- [00:36:29](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2189) so which one should we take and still not over&nbsp; trained I think. So with new Lion optimizer the&nbsp;&nbsp;

- [00:36:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2197) over training problem looks like in the past. I&nbsp; mean it is harder to over train now because I can&nbsp;&nbsp;

- [00:36:44](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2204) clearly see that it is much more able to follow&nbsp; my prompt. Why? Because in all pictures I have&nbsp;&nbsp;

- [00:36:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2211) armors. As you can see, this is a significant&nbsp; difference from the others so it is much more&nbsp;&nbsp;

- [00:36:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2217) able to follow our prompts. In all images&nbsp; I see that I have armor like I have typed.&nbsp;&nbsp;

- [00:37:05](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2225) So this was the prompt that I have used. Face&nbsp; of ohwx man wearing royal armor. You see with&nbsp;&nbsp;

- [00:37:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2232) Lion optimizer it is much more able to follow and&nbsp; obey this prompt because I see a royal armor is&nbsp;&nbsp;

- [00:37:21](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2241) worn in every image. This is not the case in other&nbsp; images. Of course the face similarity is somewhat&nbsp;&nbsp;

- [00:37:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2248) problematic, but it is following the prompt very&nbsp; well. This means that it is not over trained&nbsp;&nbsp;

- [00:37:33](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2253) with my subject. This is extremely good, extremely&nbsp; useful, and this is a new discovery for DreamBooth&nbsp;&nbsp;

- [00:37:42](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2262) training. Yes, I like it. This is pretty pretty&nbsp; good. So which epoch should we take? This is the&nbsp;&nbsp;

- [00:37:49](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2269) million dollar question. Yes and I can't decide&nbsp; it. All of them is looking good. All of them is&nbsp;&nbsp;

- [00:37:56](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2276) somewhat different. You see like these pictures&nbsp; are from taken real movie. As you can see this&nbsp;&nbsp;

- [00:38:02](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2282) is really interesting results. Okay, which one&nbsp; should we take? I think I will go with 150 epoch.&nbsp;&nbsp;

- [00:38:09](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2289) It looks like I was playing in a movie. They are&nbsp; looking really good but all of them is really&nbsp;&nbsp;

- [00:38:16](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2296) decent. So hard to decide. But for this one I will&nbsp; decide this. Okay so now time to check out test4.&nbsp;&nbsp;

- [00:38:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2302) Okay in the test4 we got significantly different&nbsp; results even though we were using the same seed.&nbsp;&nbsp;

- [00:38:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2308) This is really interesting. You see even though&nbsp; everything was same during the training test4 is&nbsp;&nbsp;

- [00:38:35](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2315) significantly different. Why? Because in test4&nbsp; we have used offset noise and I can clearly&nbsp;&nbsp;

- [00:38:42](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2322) see that the images generated by it has much more&nbsp; significant black and whites. Yes, this looks like&nbsp;&nbsp;

- [00:38:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2331) the case. The overall images are like more black.&nbsp; So if you want to obtain more black or white&nbsp;&nbsp;

- [00:38:58](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2338) then you should use offset noise. You see there&nbsp; is a clear difference between the black background&nbsp;&nbsp;

- [00:39:04](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2344) and the white foreground. This is significant&nbsp; discovery. Yes, I can clearly see the lightning&nbsp;&nbsp;

- [00:39:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2352) it has added. You see the blackness it has added.&nbsp; So this is another significant discovery. Yes,&nbsp;&nbsp;

- [00:39:18](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2358) which image is looking best. However, the&nbsp; stylizing capability is not looking very well,&nbsp;&nbsp;

- [00:39:24](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2364) but the images are still pretty significant. So&nbsp; in the 200 steps, there is overtraining in the&nbsp;&nbsp;

- [00:39:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2372) 200 epochs. In the 190 epochs. Yeah, results&nbsp; are not very good, but you can clearly see&nbsp;&nbsp;

- [00:39:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2377) the black and white differences like the article&nbsp; mentioned. Okay, so which epoch is best to choose?&nbsp;&nbsp;

- [00:39:44](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2384) This is the 150 epoch. It is looking decent. So&nbsp; if you want to generate black and white images,&nbsp;&nbsp;

- [00:39:50](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2390) this is really, really good. If you want to obtain&nbsp; contrast, this is really, really good. Okay, this&nbsp;&nbsp;

- [00:39:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2397) is 130 epochs, 120 epoch, 110 epoch, 100 epochs.&nbsp; This looks pretty decent. 190 epoch: this is also&nbsp;&nbsp;

- [00:40:09](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2409) looking pretty decent. So which one should we take&nbsp; and 100 and this is actually this 90 epoch. This&nbsp;&nbsp;

- [00:40:16](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2416) is 80 epoch. In the 80 epoch the face is not very&nbsp; much like me, so yes, this probably requires more&nbsp;&nbsp;

- [00:40:24](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2424) training. I think I will go with 100 epoch. This&nbsp; looks like the best one. Yeah, okay, now we are&nbsp;&nbsp;

- [00:40:31](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2431) seeing the test5 results. In the test five we have&nbsp; tested freeze clip normalization layers and let's&nbsp;&nbsp;

- [00:40:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2437) see if there is any significant difference in the&nbsp; test five. Let's look at them like this. When we&nbsp;&nbsp;

- [00:40:44](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2444) compare all the best checkpoints we will get a&nbsp; more clear idea between these minor differences.&nbsp;&nbsp;

- [00:40:52](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2452) So this is 100 epoch. This is 90 epoch. This is&nbsp; 110 epoch. This is also looking pretty decent one:&nbsp;&nbsp;

- [00:41:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2460) 120 epoch 130 epoch 140 150, so 160. Okay, it&nbsp; looks like the quality is degrading after this.&nbsp;&nbsp;

- [00:41:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2472) Which one should we take for this one? Okay, I&nbsp; think I will take the same as the base one so we&nbsp;&nbsp;

- [00:41:19](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2479) can see if there is any difference. 140 epoch i'm&nbsp; going to take. Okay, now the results of EMA. There&nbsp;&nbsp;

- [00:41:26](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2486) is a significant discovery in this one as well.&nbsp; Let me explain to you, the face is not like me.&nbsp;&nbsp;

- [00:41:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2492) But the important thing is I think it is much more&nbsp; able to stylize. Let me show you what I mean. All&nbsp;&nbsp;

- [00:41:41](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2501) of the images are stylized, so this methodology&nbsp; perhaps could be used for teaching a style. This&nbsp;&nbsp;

- [00:41:49](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2509) is a wroth to try and you are learning this in&nbsp; this video in our channel. So please subscribe,&nbsp;&nbsp;

- [00:41:55](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2515) make a comment, share. And if you be a Patron&nbsp; supporter us I would appreciate very much.&nbsp;&nbsp;

- [00:42:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2520) Every image is stylized. It is perfectly able to&nbsp; keep styling so you see it is never becoming like&nbsp;&nbsp;

- [00:42:07](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2527) original subject. All images are stylized. Even&nbsp; the 200 epoch you see it is fully stylized. The&nbsp;&nbsp;

- [00:42:14](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2534) face is not like me, but it is able to follow&nbsp; style very well. The style is amazing. This is&nbsp;&nbsp;

- [00:42:21](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2541) able to keep the style perfectly fine. The face&nbsp; needs to be worked on definitely. Probably I&nbsp;&nbsp;

- [00:42:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2548) need to modify the prompt and maybe I can get the&nbsp; better face. But style is amazing. It is perfectly&nbsp;&nbsp;

- [00:42:35](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2555) able to keep style in every image. So this is a&nbsp; new discovery. I think this is worth to experiment&nbsp;&nbsp;

- [00:42:41](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2561) with and test out. Okay, for test six, I am going&nbsp; to use 100 epoch actually 100 110 120 130. I can't&nbsp;&nbsp;

- [00:42:54](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2574) decide but 120 epoch? yes. But for this one, we&nbsp; really need to test different seeds, different&nbsp;&nbsp;

- [00:43:03](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2583) prompting because the way prompts work on this&nbsp; one is significantly different than others. So&nbsp;&nbsp;

- [00:43:10](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2590) let's pick 120 model for this one. Okay, in the&nbsp; test seven, let's see the results: 10 epoch 20&nbsp;&nbsp;

- [00:43:18](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2598) epoch 30 epoch 40 epoch 50 epoch 60 still not much&nbsp; my face. 70 epoch yes, I see some similarity. 80&nbsp;&nbsp;

- [00:43:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2608) epoch 90 epoch 100 epoch I think this is also able&nbsp; to follow our prompts much better. 110 epoch 120&nbsp;&nbsp;

- [00:43:38](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2618) epoch yes, significantly different than others&nbsp; 130 140 so if you use these new things EMA for&nbsp;&nbsp;

- [00:43:47](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2627) prediction or EMA weights for inference then you&nbsp; will be need to change your prompting style than&nbsp;&nbsp;

- [00:43:53](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2633) before. It is still fully stylized. You see there&nbsp; is no overtraining. The images are not becoming&nbsp;&nbsp;

- [00:43:58](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2638) raw our subject even in the 200 epochs. Yes, even&nbsp; in the 200 epochs, it is not becoming our subject.&nbsp;&nbsp;

- [00:44:08](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2648) Let me show you what I mean by a comparison. So&nbsp; in the base model this is the 200 epoch you see&nbsp;&nbsp;

- [00:44:15](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2655) simply my face. Not much stylizing, but in this&nbsp; model it is completely stylized based on our&nbsp;&nbsp;

- [00:44:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2662) prompt. But the face is not much our face. So&nbsp; there is a significant difference between how&nbsp;&nbsp;

- [00:44:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2668) you prompt when you use EMA weights for inference&nbsp; or when you don't use EMA weights for inference.&nbsp;&nbsp;

- [00:44:34](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2674) Or if the same rule applies for when you use EMA&nbsp; for prediction. It completely changes the output&nbsp;&nbsp;

- [00:44:42](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2682) as you can see. So for this one which one should&nbsp; we take, there is not much difference between each&nbsp;&nbsp;

- [00:44:49](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2689) of the epochs actually. It is almost. Yeah, it&nbsp; is producing same. This is almost same, not the&nbsp;&nbsp;

- [00:44:56](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2696) same. You see there is a difference in here, so&nbsp; it changes, but it changes pretty insignificantly&nbsp;&nbsp;

- [00:45:02](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2702) between different epochs. I wonder that if&nbsp; these method needs more training, more training&nbsp;&nbsp;

- [00:45:09](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2709) than 200 epochs, perhaps it is requiring. I see&nbsp; somewhat similarity in this one. This epoch. Yes,&nbsp;&nbsp;

- [00:45:16](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2716) there are some similarities in this one as well.&nbsp; I think I am going to pick the 120 epoch. Yes,&nbsp;&nbsp;

- [00:45:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2722) so let's pick the 120 epoch for test 7. Okay,&nbsp; now we are seeing the test 8. In the test 8&nbsp;&nbsp;

- [00:45:29](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2729) we use EMA only and no other options. So for&nbsp; comparison I think I will pick the same epoch&nbsp;&nbsp;

- [00:45:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2737) of the base model which is by. But before let me&nbsp; show you each one of the epochs. So this is 200&nbsp;&nbsp;

- [00:45:43](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2743) epoch. This is 190 epoch. There is a significant&nbsp; difference between the base when you use EMA. So&nbsp;&nbsp;

- [00:45:49](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2749) if you don't have a high VRAM having graphic card&nbsp; like me, there is a significant difference that&nbsp;&nbsp;

- [00:45:56](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2756) we are missing perhaps. So 200 epoch 190 epoch 180&nbsp; epoch 170 epoch not much changes when you use EMA.&nbsp;&nbsp;

- [00:46:06](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2766) This is something that I noticed. So which epoch&nbsp; is looking best? By the way, this image can be&nbsp;&nbsp;

- [00:46:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2772) fixed very easily with inpainting. Hopefully I am&nbsp; planning a video for that as well. So for distance&nbsp;&nbsp;

- [00:46:17](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2777) shots, don't worry that you can fix them. Okay,&nbsp; this one is looking pretty decent. 110 epochs this&nbsp;&nbsp;

- [00:46:24](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2784) is looking pretty decent. 120 also looking pretty&nbsp; decent. Maybe I should pick this one. 130 is also&nbsp;&nbsp;

- [00:46:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2792) pretty decent, so it is again hard to choose&nbsp; the best one. Okay, so which one should we take?&nbsp;&nbsp;

- [00:46:39](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2799) Perhaps I have saved too many checkpoints with&nbsp; 10 epochs. That is why we are having hard time.&nbsp;&nbsp;

- [00:46:44](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2804) Perhaps I should have made them 25 epochs. It&nbsp; would make the difference more significant between&nbsp;&nbsp;

- [00:46:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2811) different checkpoints. Okay, I think I am going&nbsp; to take 130 epoch. This looks like the best. Okay,&nbsp;&nbsp;

- [00:46:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2817) now test nine, let's see what difference do the&nbsp; xformers is making in the test nine. We didn't&nbsp;&nbsp;

- [00:47:05](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2825) use xformers default memory attention and fp16&nbsp; is used, so let's see what kind of difference we&nbsp;&nbsp;

- [00:47:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2832) have. By the way, this has significantly increased&nbsp; the VRAM usage. So you need 24 gigabyte to use&nbsp;&nbsp;

- [00:47:20](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2840) this. I think there is a significant difference&nbsp; between the base model and this one. Yeah, the&nbsp;&nbsp;

- [00:47:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2848) results are stunning. When we don't use xformers,&nbsp; I like them. So let's see which one is looking&nbsp;&nbsp;

- [00:47:34](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2854) best. So let's start from 200 epochs. So this is&nbsp; 200 epoch. Pretty much over trained you see, there&nbsp;&nbsp;

- [00:47:41](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2861) is no styling at all almost. 190 180, 170, 160,&nbsp; 150, 140, 130, 120 110. Yes, 120 is also still&nbsp;&nbsp;

- [00:47:58](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2878) not stylized outputting my face. But in 110 all&nbsp; images are stylized and this is 100 epoch. So 110&nbsp;&nbsp;

- [00:48:08](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2888) is looking like the best one. This is 90, this is&nbsp; 90. This is 80. This is 70. Yeah, I think I will&nbsp;&nbsp;

- [00:48:17](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2897) go with 100, 110 epoch or 100. I cant decide. Yes,&nbsp; I think I will go with 110 epoch for test nine.&nbsp;&nbsp;

- [00:48:26](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2906) So now I will download all of these checkpoints&nbsp; and I will make the final grid file. Currently I&nbsp;&nbsp;

- [00:48:35](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2915) am downloading four of the model files by using&nbsp; runpodctl. Let me show you, I am able to get 100&nbsp;&nbsp;

- [00:48:42](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2922) megabits download speed by downloading multiple&nbsp; files at the same time. My internet speed is&nbsp;&nbsp;

- [00:48:49](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2929) 100 megabit. I am and I am almost at full. I have&nbsp; started the ports with only cpu option so they are&nbsp;&nbsp;

- [00:48:56](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2936) using lesser credits of me. If you don't know how&nbsp; to use RunPod I have explained everything about&nbsp;&nbsp;

- [00:49:03](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2943) RunPod in this ultimate tutorial video including&nbsp; how to use RunPodctl. Currently I am having&nbsp;&nbsp;

- [00:49:10](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2950) problem to connect jupyter lab of this RunPod. So&nbsp; I have connected to web terminal and in the web&nbsp;&nbsp;

- [00:49:16](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2956) terminal. This is an interface that you get. When&nbsp; you type dir, you will get the directory listing.&nbsp;&nbsp;

- [00:49:23](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2963) So in this directory listing I am going to go&nbsp; models and in here I will go Stable diffusion:&nbsp;&nbsp;

- [00:49:29](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2969) okay, inside this model, I am going inside cd test&nbsp; 2 and when I dir it will show me all of the files.&nbsp;&nbsp;

- [00:49:38](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2978) I am going to download this file. RunPodctl send&nbsp; test 2 2400 safetensors. So now file is getting&nbsp;&nbsp;

- [00:49:47](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2987) downloaded. If you encounter such jupyter error,&nbsp; you can also use this interface and when you click&nbsp;&nbsp;

- [00:49:54](https://www.youtube.com/watch?v=sRdtVanSRl4&t=2994) again, connect to the web terminal it will open&nbsp; another terminal for you. Okay, so all files have&nbsp;&nbsp;

- [00:50:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3000) been downloaded and I have moved all of them into&nbsp; my local folder and my local instance of web UI is&nbsp;&nbsp;

- [00:50:07](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3007) running. Now I am going to do the final test with&nbsp; the same prompt on all of the models. All test&nbsp;&nbsp;

- [00:50:14](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3014) files are checked from the checkpoint name test01.&nbsp; By the way, this is missing some. So what we need&nbsp;&nbsp;

- [00:50:23](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3023) to do is click refresh here. Click refresh here&nbsp; again. Okay, zero, one, two, three, four, five,&nbsp;&nbsp;

- [00:50:30](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3030) six, seven, eight nine. All is there, the seed&nbsp; is set, batch size is set, CFG value is set,&nbsp;&nbsp;

- [00:50:37](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3037) and now time to test. There is one thing that I&nbsp; want to mention. Test seven has lesser size than&nbsp;&nbsp;

- [00:50:43](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3043) the other ones because in test seven we used Use&nbsp; EMA plus, use EMA weights for inference. Inference&nbsp;&nbsp;

- [00:50:50](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3050) means that when you are going to generate an image&nbsp; a new image, therefore only EMA weights are saved&nbsp;&nbsp;

- [00:50:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3057) and it is lesser than the original weights. You&nbsp; see this is 2.21 gigabytes and other ones are&nbsp;&nbsp;

- [00:51:04](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3064) 3.81 gigabytes. So the final experiment has been&nbsp; completed before delving into it. Please consider&nbsp;&nbsp;

- [00:51:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3072) subscribing our channel. Support us by joining&nbsp; if you are able to. Join our discord channel. You&nbsp;&nbsp;

- [00:51:19](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3079) will find the link in here or in the description&nbsp; or the comment of the video. And if you support&nbsp;&nbsp;

- [00:51:26](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3086) us on Patreon, I would appreciate that very very&nbsp; much. Currently we have 27 patrons and they are&nbsp;&nbsp;

- [00:51:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3092) helping me tremendously. I am hoping that you will&nbsp; also become our Patreon. Thank you so much! Okay,&nbsp;&nbsp;

- [00:51:38](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3098) now time for final comparison. This is test zero.&nbsp; Test one and let's see the results. Okay, I will&nbsp;&nbsp;

- [00:51:47](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3107) slowly show all of the results. This is test&nbsp; zero as you can see, pretty good, pretty decent&nbsp;&nbsp;

- [00:51:53](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3113) quality. This is our base test by the way. This&nbsp; is test one. In the test one. What difference do&nbsp;&nbsp;

- [00:52:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3120) we have? Both of them are looking very very good.&nbsp; Can we say one of them is better? So the test one&nbsp;&nbsp;

- [00:52:07](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3127) seems like a little bit better than the original&nbsp; and in the test one we used DEIS for noise&nbsp;&nbsp;

- [00:52:13](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3133) scheduler. Therefore, I think this is improving&nbsp; the overall quality. Not very significantly,&nbsp;&nbsp;

- [00:52:19](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3139) but it is increasing. Okay in the test two, the&nbsp; results are little bit more different than the&nbsp;&nbsp;

- [00:52:26](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3146) test one. You see different pictures generated.&nbsp; This is extremely well stylized but not very&nbsp;&nbsp;

- [00:52:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3152) similar to my face. So which one is better? Okay,&nbsp; apparently unfreezing model makes difference,&nbsp;&nbsp;

- [00:52:38](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3158) but is that difference is better or not? This&nbsp; is the ultimate question so I will compare it&nbsp;&nbsp;

- [00:52:44](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3164) with the base test. I am copying this, opening a&nbsp; new page. Double size. Okay, so this is test zero&nbsp;&nbsp;

- [00:52:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3171) and this is test two. Which one is better we can&nbsp; say. I think base model is looking a little bit&nbsp;&nbsp;

- [00:52:59](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3179) better. So I can't say unfreeze model making huge&nbsp; difference. It is certainly making difference, but&nbsp;&nbsp;

- [00:53:06](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3186) we can't say huge difference so it is up to you&nbsp; to use it or not. I think both of them are decent,&nbsp;&nbsp;

- [00:53:12](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3192) so it may be a little bit improving or maybe not&nbsp; improving, but I can't say this harmful to enable&nbsp;&nbsp;

- [00:53:19](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3199) this so you can use it from now on. Both results&nbsp; are very decent and when we do unfreeze model,&nbsp;&nbsp;

- [00:53:27](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3207) it took lesser steps to get this quality so that&nbsp; is another important thing to consider. Okay,&nbsp;&nbsp;

- [00:53:34](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3214) this is test three. Let's also compare&nbsp; test three to test base test. Okay,&nbsp;&nbsp;

- [00:53:39](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3219) now we are seeing test zero versus test three.&nbsp; In this case, which one is better. Test three&nbsp;&nbsp;

- [00:53:46](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3226) is more looking like from a real movie if you&nbsp; ask me. So I can say that the quality of test&nbsp;&nbsp;

- [00:53:52](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3232) three is better. The face is somewhat not like me&nbsp; in this test, but the quality is better so it is&nbsp;&nbsp;

- [00:54:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3240) up to your taste. If you prefer, you can go with&nbsp; test three configuration which is Lion optimizer.&nbsp;&nbsp;

- [00:54:06](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3246) You may obtain better quality images in terms of&nbsp; reality with Lion optimizer, but it looks like&nbsp;&nbsp;

- [00:54:15](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3255) not exactly my face so therefore it may require&nbsp; more fine tuning of the prompts. But if you can&nbsp;&nbsp;

- [00:54:22](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3262) do that then you may get good results. Better&nbsp; results than the base test for sure. Okay now&nbsp;&nbsp;

- [00:54:29](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3269) test4. In test4 the difference is very clear. You&nbsp; see the darkness and whiteness of the images are&nbsp;&nbsp;

- [00:54:35](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3275) much better because as mentioned in this article,&nbsp; it allows Stale Diffusion to generate very dark or&nbsp;&nbsp;

- [00:54:43](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3283) light images easily. Therefore, you see the images&nbsp; have much better dark and light difference so it&nbsp;&nbsp;

- [00:54:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3291) is up to your taste which one you prefer, but this&nbsp; is significantly changing the output results. Let&nbsp;&nbsp;

- [00:54:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3297) me show you the comparison. So this is base&nbsp; test versus offset noise. It is up to you to&nbsp;&nbsp;

- [00:55:03](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3303) prefer which one of them you want. If you want&nbsp; to have much better light and dark images then&nbsp;&nbsp;

- [00:55:10](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3310) you should go with offset noise. Or you can set&nbsp; the offset noise lower than I did so it may not&nbsp;&nbsp;

- [00:55:16](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3316) have this much significant impact, but it clearly&nbsp; improved the light and dark difference you see.&nbsp;&nbsp;

- [00:55:24](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3324) Okay now test five. In the test five, this is&nbsp; the results comparison. I can't say test five&nbsp;&nbsp;

- [00:55:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3332) is looking better than the base model actually. So&nbsp; what did we use in test five? We used freeze clip&nbsp;&nbsp;

- [00:55:38](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3338) normalization layers. I can say that it didn't&nbsp; improve well. The images are almost exactly same,&nbsp;&nbsp;

- [00:55:45](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3345) but which one is better. For example, let's zoom&nbsp; in this one. Freeze normalization layers results.&nbsp;&nbsp;

- [00:55:51](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3351) And this is not freezing, so let's make a better&nbsp; comparison. Okay, so which one is looking better?&nbsp;&nbsp;

- [00:55:58](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3358) Actually, when we look at the fine details,&nbsp; there is a slight difference in here and in&nbsp;&nbsp;

- [00:56:05](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3365) here they are almost looking like same. So I&nbsp; don't know what kind of effect did this freeze&nbsp;&nbsp;

- [00:56:13](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3373) normalization layers made. They are almost looking&nbsp; like same. There are also some subtle differences&nbsp;&nbsp;

- [00:56:19](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3379) in differences in here you see, but I don't know.&nbsp; I think I would probably wouldn't use freeze clip&nbsp;&nbsp;

- [00:56:28](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3388) normalization layers option because I don't see&nbsp; any benefit and it might be slightly worse than&nbsp;&nbsp;

- [00:56:35](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3395) the original. Not sure it is up to you. Okay,&nbsp; now test six. The difference is really, really&nbsp;&nbsp;

- [00:56:41](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3401) huge in test six because it's completely changed&nbsp; the output being while we are using same seed and&nbsp;&nbsp;

- [00:56:48](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3408) same prompt. Okay, in the test six, the face is&nbsp; definitely not my face. It is pretty different,&nbsp;&nbsp;

- [00:56:54](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3414) but it is much more stylized than the original&nbsp; results as you can see, so therefore it is up to&nbsp;&nbsp;

- [00:57:01](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3421) you to use which one you want. In the test six&nbsp; we have used Use EMA + use EMA for prediction.&nbsp;&nbsp;

- [00:57:08](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3428) Therefore, the results are significantly&nbsp; different, but more stylized. This may&nbsp;&nbsp;

- [00:57:14](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3434) work better for when we teach a style, so this is&nbsp; worth to test it. Experiment it for styling. Okay,&nbsp;&nbsp;

- [00:57:21](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3441) now we are seeing test seven results. The results&nbsp; are significantly different in this one as well.&nbsp;&nbsp;

- [00:57:26](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3446) They are not also very good. The faces are not&nbsp; very good, not like me. It is highly stylized,&nbsp;&nbsp;

- [00:57:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3452) so therefore these may work better for teaching&nbsp; styles. You see. This is test seven. Use&nbsp;&nbsp;

- [00:57:38](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3458) EMA + use EMA weights for inference. So it is up&nbsp; to you to test this and if you like it more than&nbsp;&nbsp;

- [00:57:44](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3464) you can use it. But this changes how you need&nbsp; to prompt than before. This is for sure. Okay,&nbsp;&nbsp;

- [00:57:50](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3470) this is test eight. The results are different once&nbsp; again, however, they are good in their way. This&nbsp;&nbsp;

- [00:57:57](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3477) is more stylized. The face is mine definitely.&nbsp; They are looking pretty decent as well. So when we&nbsp;&nbsp;

- [00:58:04](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3484) use EMA I think we are able to keep styling better&nbsp; than when we not use it. Therefore, if I had more&nbsp;&nbsp;

- [00:58:11](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3491) VRAM I think I would use EMA for sure. Because it&nbsp; is certainly improving our success rate. When we&nbsp;&nbsp;

- [00:58:17](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3497) are able to generate more stylized images always&nbsp; we can modify our prompt to get what we want,&nbsp;&nbsp;

- [00:58:24](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3504) so use EMA weights definitely increasing the&nbsp; learning rate success because you see more&nbsp;&nbsp;

- [00:58:31](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3511) stylizing means that our model learned with lesser&nbsp; over training. So the weights are more generalized&nbsp;&nbsp;

- [00:58:39](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3519) in the contextual underlying context of the model.&nbsp; Therefore, use EMA weights definitely improves&nbsp;&nbsp;

- [00:58:46](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3526) the success rate of the training. However, this&nbsp; requires more than 12 gigabytes VRAM. Probably&nbsp;&nbsp;

- [00:58:52](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3532) you need minimum 16 gigabytes of VRAM. So this&nbsp; is the negative side of the EMA weights. However,&nbsp;&nbsp;

- [00:59:00](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3540) you can always hire a RunPod and do your&nbsp; training on that. If you hire a RunPod,&nbsp;&nbsp;

- [00:59:05](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3545) please register through my referral link which you&nbsp; will find in the description and in the comment&nbsp;&nbsp;

- [00:59:11](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3551) section. I would appreciate that very much. Okay,&nbsp; now this is our final comparison. Test nine versus&nbsp;&nbsp;

- [00:59:18](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3558) test zero. The only difference is that in test&nbsp; nine we didn't use xformers and I can say that&nbsp;&nbsp;

- [00:59:25](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3565) definitely test nine is looking better. It is a&nbsp; personal opinion of course you may find the other&nbsp;&nbsp;

- [00:59:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3572) one is better. However, I find that the test nine&nbsp; is able to stylize better also keeping my face.&nbsp;&nbsp;

- [00:59:39](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3579) In some pictures the face is not very similar,&nbsp; but it is, I think definitely better. However,&nbsp;&nbsp;

- [00:59:45](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3585) this is requiring much more VRAM. So you really&nbsp; need a higher VRAM having GPU for this to work.&nbsp;&nbsp;

- [00:59:53](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3593) So this is also out of options for many of the&nbsp; people I saw that it was using 17 gigabytes of&nbsp;&nbsp;

- [00:59:59](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3599) VRAM. Therefore, you probably need 24 gigabytes&nbsp; VRAM having card for this or you can use RunPod.&nbsp;&nbsp;

- [01:00:07](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3607) So you will see the sign up link for RunPod in&nbsp; the description and in the comment section of this&nbsp;&nbsp;

- [01:00:13](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3613) video. If you use these links I would appreciate&nbsp; that very much. That is my referral having link.&nbsp;&nbsp;

- [01:00:18](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3618) This is all for today. I literally spent one day&nbsp; just for recording video and then I will be have&nbsp;&nbsp;

- [01:00:25](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3625) to spend a lot of hours to post process this&nbsp; video. Prepare fully manually fixed subtitles.&nbsp;&nbsp;

- [01:00:32](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3632) Prepare the sections of the video. So please&nbsp; like, share, subscribe. Support us by joining,&nbsp;&nbsp;

- [01:00:39](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3639) support us through patreon. I would appreciate&nbsp; that very much. Hopefully see you in another&nbsp;&nbsp;

- [01:00:44](https://www.youtube.com/watch?v=sRdtVanSRl4&t=3644) awesome video and thank you RunPod for providing&nbsp; me credits to run these amazing experiments!
