# How to Install & Run TensorRT on RunPod, Unix, Linux for 2x Faster Stable Diffusion Inference Speed

## Full tutorial link > https://www.youtube.com/watch?v=eKnMVXVjVoU

[![How to Install & Run TensorRT on RunPod, Unix, Linux for 2x Faster Stable Diffusion Inference Speed](https://img.youtube.com/vi/eKnMVXVjVoU/sddefault.jpg)](https://www.youtube.com/watch?v=eKnMVXVjVoU "How to Install & Run TensorRT on RunPod, Unix, Linux for 2x Faster Stable Diffusion Inference Speed")

[![image](https://img.shields.io/discord/772774097734074388?label=Discord&logo=discord)](https://discord.com/servers/software-engineering-courses-secourses-772774097734074388) [![Hits](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/How-to-Install-and-Run-TensorRT-on-RunPod-Unix-Linux-for-2x-Faster-Stable-Diffusion-Inference-Speed.md.svg?style=plastic&label=Hits%20Since%2025.08.27&labelColor=007ec6&logo=SECourses)](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/How-to-Install-and-Run-TensorRT-on-RunPod-Unix-Linux-for-2x-Faster-Stable-Diffusion-Inference-Speed.md)
[![Patreon](https://img.shields.io/badge/Patreon-Support%20Me-F2EB0E?style=for-the-badge&logo=patreon)](https://www.patreon.com/c/SECourses) [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&logoColor=black)](https://www.buymeacoffee.com/DrFurkan) [![Furkan Gözükara Medium](https://img.shields.io/badge/Medium-Follow%20Me-800080?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@furkangozukara) [![Codio](https://img.shields.io/static/v1?style=for-the-badge&message=Articles&color=4574E0&logo=Codio&logoColor=FFFFFF&label=CivitAI)](https://civitai.com/user/SECourses/articles) [![Furkan Gözükara Medium](https://img.shields.io/badge/DeviantArt-Follow%20Me-990000?style=for-the-badge&logo=deviantart&logoColor=white)](https://www.deviantart.com/monstermmorpg)

[![YouTube Channel](https://img.shields.io/badge/YouTube-SECourses-C50C0C?style=for-the-badge&logo=youtube)](https://www.youtube.com/SECourses)  [![Furkan Gözükara LinkedIn](https://img.shields.io/badge/LinkedIn-Follow%20Me-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/furkangozukara/)   [![Udemy](https://img.shields.io/static/v1?style=for-the-badge&message=Stable%20Diffusion%20Course&color=A435F0&logo=Udemy&logoColor=FFFFFF&label=Udemy)](https://www.udemy.com/course/stable-diffusion-dreambooth-lora-zero-to-hero/?referralCode=E327407C9BDF0CEA8156) [![Twitter Follow Furkan Gözükara](https://img.shields.io/badge/Twitter-Follow%20Me-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/GozukaraFurkan)


Stable Diffusion Gets A Major Boost With RTX Acceleration. One of the most common ways to use Stable Diffusion, the popular Generative AI tool that allows users to produce images from simple text descriptions, is through the Stable Diffusion Web UI by Automatic1111. In today’s Game Ready Driver, NVIDIA added TensorRT acceleration for Stable Diffusion Web UI, which boosts GeForce RTX performance by up to 2X. In this tutorial video I will show you everything about this new Speed up via extension installation and TensorRT SD UNET generation on RunPod. The tutorial can be also used on other Unix systems and on local Linux Operating Systems.

#TensorRT #StableDiffusion #NVIDIA

Automatic Installer Of Tutorial ⤵️

[https://www.patreon.com/posts/86438018](https://www.patreon.com/posts/86438018)

Comprehensive TensorRT Main Tutorial ⤵️

[https://youtu.be/kvxX6NrPtEk](https://youtu.be/kvxX6NrPtEk)

TensorRT Official GitHub Repo ⤵️

[https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT](https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT)

SECourses Discord To Get Full Support ⤵️

[https://discord.com/servers/software-engineering-courses-secourses-772774097734074388](https://discord.com/servers/software-engineering-courses-secourses-772774097734074388)

My LinkedIn ⤵️

[https://www.linkedin.com/in/furkangozukara/](https://www.linkedin.com/in/furkangozukara/)

My Instagram ⤵️

[https://www.instagram.com/gozukarafurkan/](https://www.instagram.com/gozukarafurkan/)

My Medium ⤵️

@FurkanGozukara [https://medium.com/@furkangozukara](https://medium.com/@furkangozukara)

My CivitAI ⤵️

[https://civitai.com/user/SECourses](https://civitai.com/user/SECourses)

[00:00:00](https://youtu.be/eKnMVXVjVoU?t=0) Introduction to speed increase of TensorRT - RTX Acceleration on RunPod & Unix

[00:03:10](https://youtu.be/eKnMVXVjVoU?t=190) Image quality comparison of TensorRT on vs TensorRT off for Stable Diffusion XL (SDXL)

[00:04:14](https://youtu.be/eKnMVXVjVoU?t=254) How to install TensorRT on RunPod and on local Unix operating systems

[00:07:30](https://youtu.be/eKnMVXVjVoU?t=450) How to check your current Nvidia driver on RunPod and on Unix

[00:08:10](https://youtu.be/eKnMVXVjVoU?t=490) Extra tips for TensorRT

8.45 How to connect / open your Automatic1111 Web UI on RunPod

[00:09:27](https://youtu.be/eKnMVXVjVoU?t=567) How to enable quick selection drop down options for VAE and TensorRT UNET

[00:10:09](https://youtu.be/eKnMVXVjVoU?t=609) How to generate your first TensorRT model

[00:10:19](https://youtu.be/eKnMVXVjVoU?t=619) TensorRT engine generation speed and duration

[00:10:55](https://youtu.be/eKnMVXVjVoU?t=655) How to reload last image generation settings quickly

[00:11:44](https://youtu.be/eKnMVXVjVoU?t=704) The amount of speed increase on RTX 3090 on RunPod with TensorRT



### Video Transcription


- [00:00:00](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=0) Greetings everyone.

- [00:00:01](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=1) At the moment you are seeing two identical pods running on RunPod.

- [00:00:06](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=6) Let me show you and I will show you the their speed difference when generating Stable Diffusion

- [00:00:13](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=13) XL (SDXL) images.

- [00:00:15](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=15) They are totally same pods, same GPU.

- [00:00:18](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=18) One of them is running on the default setup.

- [00:00:22](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=22) The other one is using the TensorRT RTX acceleration which is newly arrived feature.

- [00:00:30](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=30) So I have got 50 amazing prompts generated with the ChatGPT GPT4, as you are seeing right

- [00:00:37](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=37) now.

- [00:00:38](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=38) I have copied it.

- [00:00:39](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=39) Let's go to the prompts from textbox, paste it there, and do the same thing in the other

- [00:00:45](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=45) instance, and paste that and then let's hit generate and let's see the speed.

- [00:00:51](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=51) So on the left one, which is the default pod, it is going to take about 3 minutes on the

- [00:00:57](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=57) right one, which is the TensorRT using pod.

- [00:01:01](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=61) It is going to about 1.5 minutes.

- [00:01:04](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=64) You see the speed difference.

- [00:01:06](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=66) It is huge.

- [00:01:07](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=67) So in this tutorial I am going to show you how to install TensorRT on RunPod and also

- [00:01:14](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=74) this installation applies to the Unix users because RunPod is using UbuntU operating system.

- [00:01:22](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=82) Therefore it's a Unix system.

- [00:01:23](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=83) So if you don't know how to install TensorRT on a Unix system or on RunPod, watch this

- [00:01:30](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=90) tutorial and get amazing speed.

- [00:01:32](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=92) Let me also show you the it per second.

- [00:01:35](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=95) So currently I am generating 1024 x 1024 default resolution of SDXL images with 20 steps.

- [00:01:42](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=102) This is the default pod, which is not using TensorRT on RTX 4090 GPU.

- [00:01:49](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=109) And this is the TensorRT using pod 4090 GPU.

- [00:01:53](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=113) The speed is slower than what it should be because this is using still older Nvidia driver.

- [00:02:00](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=120) Unfortunately there is no way to upgrade Nvidia driver on RunPod template at the moment.

- [00:02:07](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=127) The RunPod developers have to upgrade the Nvidia driver, but still the speed difference

- [00:02:12](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=132) is huge, as you are seeing right now.

- [00:02:14](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=134) The image quality is totally same, just the speed is much more improved.

- [00:02:20](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=140) The VRAM usages are also almost same as you are seeing right now.

- [00:02:24](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=144) So the TensorRT version already finished the processing.

- [00:02:27](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=147) The TensorRT pod took only 1 minute, 44 seconds to generate 50 images and the images are same

- [00:02:36](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=156) as the other pod.

- [00:02:38](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=158) Let's go to the first image.

- [00:02:39](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=159) Then we will compare that.

- [00:02:41](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=161) The regular pod also finished the processing.

- [00:02:44](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=164) It took 2 minutes, 53 seconds.

- [00:02:47](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=167) So what is the difference?

- [00:02:49](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=169) Let's calculate it.

- [00:02:50](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=170) The regular pod took 173 seconds.

- [00:02:52](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=172) The TensorRT pod took 104 seconds.

- [00:02:57](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=177) So what is the speed difference?

- [00:02:59](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=179) Let's calculate it.

- [00:03:00](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=180) Over 104.

- [00:03:01](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=181) The TensorRT pod is 66 percent faster than the regular pod.

- [00:03:08](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=188) So with just this installation, you gain 66 percent speed difference.

- [00:03:14](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=194) Is the image quality same?

- [00:03:17](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=197) Let's also compare them.

- [00:03:18](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=198) So on the left we are seeing the regular pod.

- [00:03:21](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=201) On the right, we are seeing the TensorRT pod.

- [00:03:24](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=204) I used the same seed.

- [00:03:26](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=206) Let's move image by image to see they are almost exactly same.

- [00:03:32](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=212) There is a very little difference from the xFormers maybe you know that.

- [00:03:37](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=217) So you see and when the Nvidia drivers of the pod template got upgraded, we will get

- [00:03:44](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=224) even better speeds.

- [00:03:45](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=225) On your own computer with installing TensorRT, you can get even much better improvements.

- [00:03:51](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=231) For example, on Windows on RTX 3090 I got over 75 percent speed increase.

- [00:03:59](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=239) So if you don't know TensorRT watch this amazing tutorial.

- [00:04:02](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=242) It is over 40 minutes.

- [00:04:04](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=244) I have explained it all of the details of TensorRT in this tutorial.

- [00:04:09](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=249) In this video I will show you how to install TensorRT on Unix.

- [00:04:13](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=253) So let me close my Tensor pod.

- [00:04:16](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=256) Actually I will close both of them.

- [00:04:17](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=257) I will make a fresh pod so you will see how to make it.

- [00:04:21](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=261) Let's delete them.

- [00:04:22](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=262) Let's go to the community Cloud select extreme speed from here.

- [00:04:26](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=266) For demonstration I will use RTX 4090.

- [00:04:30](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=270) Actually let's this time use RTX 3090 and see the speed difference in that one.

- [00:04:36](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=276) So we are getting our pod, wait until the connect button appears here.

- [00:04:41](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=281) So the pod has been started.

- [00:04:43](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=283) Let's connect, connect to JupyterLab.

- [00:04:45](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=285) Okay it is not ready yet.

- [00:04:47](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=287) Let's refresh until this becomes ready.

- [00:04:49](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=289) This may take a while sometimes.

- [00:04:51](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=291) Sometimes it will show you orange.

- [00:04:53](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=293) Okay, it turned to blue so it should be ready now.

- [00:04:56](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=296) Yeah.

- [00:04:57](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=297) So to follow this tutorial we are going to download the attachments from this amazingly

- [00:05:02](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=302) detailed Patreon post.

- [00:05:05](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=305) For downloading the attachments go to the very bottom, you will see all of the attachments,

- [00:05:09](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=309) let's click all of them one by one and download.

- [00:05:11](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=311) The first thing is that since we are using Stable Diffusion Web UI template, you are

- [00:05:17](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=317) seeing here Stable Diffusion Web UI template, we need to change the relauncher.py.

- [00:05:23](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=323) Because it will permanently relaunch your Web UI instance whenever you restart it.

- [00:05:29](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=329) I don't like this behavior.

- [00:05:30](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=330) You see it has a while loop.

- [00:05:32](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=332) So I am going to upload the relauncher.py from my Patreon post.

- [00:05:37](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=337) And after that you will see it becomes like this.

- [00:05:40](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=340) When you first time do this operation, you need to restart your pod so that it will become

- [00:05:45](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=345) effective.

- [00:05:46](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=346) So I just did restart and after restart, just wait a little bit and then connect to JupyterLab

- [00:05:52](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=352) one more time.

- [00:05:53](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=353) Wait until it becomes available.

- [00:05:55](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=355) It should be pretty quick this time.

- [00:05:57](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=357) So the pod is restarted and we got our pod.

- [00:06:00](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=360) So go back to the workspace, click this icon and upload install_tensorRT.sh file and 1_click_auto1111_SDXL.sh

- [00:06:07](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=367) file.

- [00:06:09](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=369) Then in the Patreon post, you will see that this command.

- [00:06:14](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=374) First execute this command, this will download the latest VAE file for us.

- [00:06:19](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=379) This doesn't do much at the moment because they have added the accurate SDXL base version

- [00:06:25](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=385) into the template.

- [00:06:26](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=386) So we are just downloading the best VAE files for both SDXL and both SD 1.5 based version.

- [00:06:35](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=395) Then just wait for Stable Diffusion Web UI instance to start.

- [00:06:39](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=399) So instance has been started.

- [00:06:41](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=401) So let's open another terminal.

- [00:06:43](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=403) And what we are going to do is copy this, like this.

- [00:06:47](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=407) Copy, copy-paste it here.

- [00:06:49](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=409) And this time it will install the TensorRT latest version with its necessary dependencies.

- [00:06:56](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=416) When you use the extension install feature of the Automatic1111 Web UI, it will not work.

- [00:07:03](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=423) I have tested it, even if you restart it.

- [00:07:06](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=426) Even if you remove the skip install, it doesn't work.

- [00:07:08](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=428) So, I have come up with a specific way to install it.

- [00:07:13](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=433) And if you are a Unix user on your computer, you can just edit the this install_tensorRT.sh

- [00:07:21](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=441) file, and change the folder paths and use it on your local installation.

- [00:07:27](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=447) The installation is pretty fast actually, as you are seeing right now.

- [00:07:30](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=450) Let me also show you the current Nvidia driver on the RunPod.

- [00:07:34](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=454) It is 525.

- [00:07:35](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=455) This is a super old driver.

- [00:07:39](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=459) The TensorRT developers are suggesting this driver for Linux, which is Unix, 450.

- [00:07:47](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=467) But we are using a very old one.

- [00:07:49](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=469) Therefore, this is why we are not getting the expected speed output from our TensorRT

- [00:07:55](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=475) installation.

- [00:07:56](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=476) So, when this driver got upgraded, hopefully we will get much better speeds.

- [00:08:02](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=482) On your local computer you can install the latest driver and enjoy it.

- [00:08:06](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=486) There are also some other tips here.

- [00:08:09](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=489) For example, when you generated your TensorRT with necessary resolutions, you can use high

- [00:08:15](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=495) resolution fix as well as you are seeing right now.

- [00:08:18](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=498) There are also some other tips here.

- [00:08:20](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=500) So also read these instructions.

- [00:08:22](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=502) I will put the link of this repository and this Patreon post into the description of

- [00:08:28](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=508) the video and also the comment section of the video.

- [00:08:31](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=511) For Udemy users you will get the attachments in the attachments section.

- [00:08:36](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=516) So let's look at the installation.

- [00:08:38](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=518) Okay, Web UI is starting right now.

- [00:08:40](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=520) The necessary packages have been installed and Web UI started as you are seeing right

- [00:08:45](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=525) now.

- [00:08:46](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=526) So let's connect it.

- [00:08:47](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=527) For connecting I am preferring connect.

- [00:08:48](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=528) Connect to HTTP service 3001 port.

- [00:08:52](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=532) So we got our default installation.

- [00:08:54](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=534) We didn't use the TensorRT yet.

- [00:08:56](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=536) Let's try one of the prompt here and see the speed.

- [00:08:59](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=539) This is RTX 3090.

- [00:09:01](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=541) Okay so let's make this, let's make the batch count 3.

- [00:09:05](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=545) This will generate images one by one.

- [00:09:07](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=547) So this is not batch size.

- [00:09:09](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=549) Let's generate 3 images and look at the it per second.

- [00:09:12](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=552) So we are getting about 3.64.

- [00:09:15](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=555) 3.65 it per second.

- [00:09:18](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=558) Okay 3.65 it per second.

- [00:09:23](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=563) We are getting the images and 3.65.

- [00:09:26](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=566) Yes this is our it per second right now.

- [00:09:29](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=569) First of all go to the settings and in here go to the user interface, from here select

- [00:09:36](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=576) SD_VAE and SD_UNET here as you are seeing right now.

- [00:09:40](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=580) Apply settings, reload UI.

- [00:09:42](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=582) This is important.

- [00:09:43](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=583) This will reload the UI as you are seeing right now.

- [00:09:46](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=586) Then we will have two options two quick selection box which we will use.

- [00:09:52](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=592) Okay it is getting reloaded.

- [00:09:54](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=594) Okay.

- [00:09:55](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=595) So now we can set our VAE and our SD UNET.

- [00:09:58](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=598) We don't have any SD UNET yet.

- [00:10:00](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=600) Go to the TensorRT.

- [00:10:02](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=602) As I said, watch this amazing full tutorial to learn much more about TensorRT.

- [00:10:07](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=607) In this tutorial I am just going to show how to install and use it.

- [00:10:11](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=611) I will make 1024 1024 batch size 1 static engine.

- [00:10:15](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=615) So these are all the settings.

- [00:10:17](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=617) Export engine.

- [00:10:18](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=618) This export engine duration totally depends on the GPU.

- [00:10:22](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=622) For example, on RTX 4090 with this older driver, it took about 3 minutes.

- [00:10:28](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=628) Let's see how much time it will take on RTX 3090.

- [00:10:32](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=632) During the ONNX file generation, you will not get any messages.

- [00:10:36](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=636) You will see this screen.

- [00:10:38](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=638) After that it will start generating the TensorRT file that we need.

- [00:10:43](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=643) And you will see the progress like this.

- [00:10:45](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=645) So the TensorRT generation has been completed.

- [00:10:48](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=648) It took 320 seconds and now we can begin using it.

- [00:10:53](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=653) So let's go to text to image tab again.

- [00:10:55](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=655) We have loaded the last used settings with clicking this icon.

- [00:10:59](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=659) Let's make the seed random.

- [00:11:01](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=661) Then let's refresh the VAE and UNET from here.

- [00:11:05](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=665) I will use the best SDXL VAE FP16.

- [00:11:09](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=669) This doesn't bring any speed increase but I am using it so that without using --no-half-vae

- [00:11:17](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=677) you can use both SD 1.5 based models and SDXL models.

- [00:11:19](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=679) Okay it is selected.

- [00:11:24](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=684) After that let's refresh the UNET and you can leave this automatic or select it.

- [00:11:28](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=688) I will select it.

- [00:11:30](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=690) Okay let's generate image so the first image generation may be slower than the consequent

- [00:11:35](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=695) ones.

- [00:11:36](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=696) Okay it per second is 6.24.

- [00:11:40](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=700) Let's generate 9 images and see the speed.

- [00:11:44](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=704) 6.16 it per second.

- [00:11:47](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=707) So from 3.64 it to 6.17 it per second.

- [00:11:52](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=712) How much speed increase does this make?

- [00:11:54](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=714) Let's calculate it.

- [00:11:55](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=715) So 617 minus 364 over 364.

- [00:12:01](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=721) We got about 69 speed increase about 70 increase with RTX 3090 on this very old Nvidia driver.

- [00:12:14](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=734) So you see this is huge.

- [00:12:16](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=736) TensorRT is huge.

- [00:12:18](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=738) You just wait several minutes one time then you can generate images so well, so fast and

- [00:12:24](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=744) you can generate thousands of hundreds of images faster and save your time hugely.

- [00:12:30](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=750) This is amazing.

- [00:12:32](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=752) Hopefully it will get even better over the time.

- [00:12:34](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=754) The developer is working very active.

- [00:12:37](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=757) Don't forget to watch this tutorial.

- [00:12:39](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=759) Then you can use the advanced setup by unchecking this use static shapes change these settings,

- [00:12:45](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=765) set up them as your needs and get the speed up.

- [00:12:48](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=768) Hopefully you have enjoyed, please like our channel subscribe our channel.

- [00:12:52](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=772) The links will be in the description of the video and also in the comment section of the

- [00:12:57](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=777) video.

- [00:12:58](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=778) We have links here.

- [00:12:59](https://www.youtube.com/watch?v=eKnMVXVjVoU&t=779) Hopefully see you in another amazing tutorial video.
