# New Stuff Of OpenAI - GPT-4 Turbo - Longer Context - More Control - Many More - OpenAI DevDay Part 2

## Full tutorial link > https://www.youtube.com/watch?v=4mQXxk8FeEE

[![New Stuff Of OpenAI - GPT-4 Turbo - Longer Context - More Control - Many More - OpenAI DevDay Part 2](https://img.youtube.com/vi/4mQXxk8FeEE/sddefault.jpg)](https://www.youtube.com/watch?v=4mQXxk8FeEE "New Stuff Of OpenAI - GPT-4 Turbo - Longer Context - More Control - Many More - OpenAI DevDay Part 2")

[![image](https://img.shields.io/discord/772774097734074388?label=Discord&logo=discord)](https://discord.com/servers/software-engineering-courses-secourses-772774097734074388) [![Hits](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/New-Stuff-Of-OpenAI-GPT-4-Turbo-Longer-Context-More-Control-Many-More-OpenAI-DevDay-Part-2.md.svg?style=plastic&label=Hits%20Since%2025.08.27&labelColor=007ec6&logo=SECourses)](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/New-Stuff-Of-OpenAI-GPT-4-Turbo-Longer-Context-More-Control-Many-More-OpenAI-DevDay-Part-2.md)
[![Patreon](https://img.shields.io/badge/Patreon-Support%20Me-F2EB0E?style=for-the-badge&logo=patreon)](https://www.patreon.com/c/SECourses) [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&logoColor=black)](https://www.buymeacoffee.com/DrFurkan) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/Medium-Follow%20Me-800080?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@furkangozukara) [![Codio](https://img.shields.io/static/v1?style=for-the-badge&message=Articles&color=4574E0&logo=Codio&logoColor=FFFFFF&label=CivitAI)](https://civitai.com/user/SECourses/articles) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/DeviantArt-Follow%20Me-990000?style=for-the-badge&logo=deviantart&logoColor=white)](https://www.deviantart.com/monstermmorpg)

[![YouTube Channel](https://img.shields.io/badge/YouTube-SECourses-C50C0C?style=for-the-badge&logo=youtube)](https://www.youtube.com/SECourses)  [![Furkan G√∂z√ºkara LinkedIn](https://img.shields.io/badge/LinkedIn-Follow%20Me-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/furkangozukara/)   [![Udemy](https://img.shields.io/static/v1?style=for-the-badge&message=Stable%20Diffusion%20Course&color=A435F0&logo=Udemy&logoColor=FFFFFF&label=Udemy)](https://www.udemy.com/course/stable-diffusion-dreambooth-lora-zero-to-hero/?referralCode=E327407C9BDF0CEA8156) [![Twitter Follow Furkan G√∂z√ºkara](https://img.shields.io/badge/Twitter-Follow%20Me-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/GozukaraFurkan)


This video is Part 2 of #OpenAI #DevDay #GPT

Full Event : [https://www.youtube.com/watch?v=U9mJuUkhUzk](https://www.youtube.com/watch?v=U9mJuUkhUzk)

Credits : [https://www.youtube.com/watch?v=U9mJuUkhUzk](https://www.youtube.com/watch?v=U9mJuUkhUzk)

[00:00:00](https://youtu.be/4mQXxk8FeEE?t=0) üöÄ OpenAI is launching GPT-4 Turbo, a new model that addresses user feedback and offers significant improvements.

[00:01:20](https://youtu.be/4mQXxk8FeEE?t=80) üéÆ GPT-4 Turbo provides more control to developers, including JSON mode, better function calling, and reproducible outputs.

[00:02:30](https://youtu.be/4mQXxk8FeEE?t=150) üåç OpenAI is introducing retrieval in the platform, enabling users to access external knowledge, and updating the knowledge cutoff to April 2023.

[00:03:41](https://youtu.be/4mQXxk8FeEE?t=221) üì∑ GPT-4 Turbo supports image inputs via the API, generating captions, classifications, and analysis. It also includes a natural-sounding text-to-speech model.

[00:04:51](https://youtu.be/4mQXxk8FeEE?t=291) üó£Ô∏è OpenAI is releasing Whisper V3, an improved speech recognition model, and expanding customization options for models.

[00:06:29](https://youtu.be/4mQXxk8FeEE?t=389) üíº OpenAI is doubling the tokens per minute for GPT-4 customers and introducing Copyright Shield to protect against legal claims related to copyright infringement.

[00:07:33](https://youtu.be/4mQXxk8FeEE?t=453) üí∞ GPT-4 Turbo offers a significantly lower price, with a 3x reduction in prompt token cost and a 2x reduction in completion token cost compared to GPT-4.



### Video Transcription


- [00:00:00](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=0) So now on to the new stuff and we have got a&nbsp; lot. First we're going to talk about a bunch of&nbsp;&nbsp;

- [00:00:08](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=8) improvements we've made and then we'll talk about&nbsp; where we're headed next. Over the last year we&nbsp;&nbsp;

- [00:00:14](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=14) spent a lot of time talking to developers around&nbsp; the world. We've heard a lot of your feedback.&nbsp;&nbsp;

- [00:00:20](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=20) It's really informed what we have to show you&nbsp; today. Today we are launching a new model GPT-4&nbsp;&nbsp;

- [00:00:28](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=28) Turbo. GPT-4 Turbo will address many of the&nbsp; things that you all have asked for. So let's&nbsp;&nbsp;

- [00:00:39](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=39) go through what's new. We've got six major things&nbsp; to talk about for this part. Number one, context&nbsp;&nbsp;

- [00:00:46](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=46) length. A lot of people have tasks that require a&nbsp; much longer context length. GPT-4 supported up to&nbsp;&nbsp;

- [00:00:53](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=53) 8k and in some cases up to 32k context length. But&nbsp; we know that isn't enough for many of you and what&nbsp;&nbsp;

- [00:00:59](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=59) you've heard. So let's get started. GPT-4 Turbo&nbsp; supports up to 128,000 tokens of context. That's&nbsp;&nbsp;

- [00:01:11](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=71) 300 pages of a standard book, 16 times longer than&nbsp; our 8k context. And in addition to longer context&nbsp;&nbsp;

- [00:01:18](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=78) length, you'll notice that the model is much&nbsp; more accurate over a long context. Number two,&nbsp;&nbsp;

- [00:01:26](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=86) more control. We've heard loud and clear that&nbsp; developers need more control over the model's&nbsp;&nbsp;

- [00:01:31](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=91) responses and outputs. So we've addressed that&nbsp; in a number of ways. We have a new feature called&nbsp;&nbsp;

- [00:01:37](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=97) JSON mode which ensures that the model will&nbsp; respond with valid JSON. This has been a huge&nbsp;&nbsp;

- [00:01:43](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=103) developer request. It'll make calling APIs much&nbsp; easier. The model is also much better at function&nbsp;&nbsp;

- [00:01:50](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=110) calling. You can now call many functions at once.&nbsp; And it'll do better at following instructions in&nbsp;&nbsp;

- [00:01:55](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=115) general. We're also introducing a new feature&nbsp; called reproducible outputs. You can pass a&nbsp;&nbsp;

- [00:02:02](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=122) seed parameter and it'll make the model return&nbsp; consistent outputs. This, of course, gives you a&nbsp;&nbsp;

- [00:02:06](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=126) higher degree of control over model behavior. This&nbsp; rolls out in beta today. And in the coming weeks,&nbsp;&nbsp;

- [00:02:17](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=137) we'll roll out a feature to let you view log probs&nbsp; in the API. All right. Number three, better world&nbsp;&nbsp;

- [00:02:26](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=146) knowledge. You want these models to be able to&nbsp; access better knowledge about the world. So do&nbsp;&nbsp;

- [00:02:30](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=150) we. So we're launching retrieval in the platform.&nbsp; You can bring knowledge from outside documents or&nbsp;&nbsp;

- [00:02:36](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=156) databases into whatever you're building. We're&nbsp; also updating the knowledge cutoff. We are&nbsp;&nbsp;

- [00:02:42](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=162) just as annoyed as all of you, probably more,&nbsp; that GPT-4's knowledge about the world ended&nbsp;&nbsp;

- [00:02:46](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=166) in 2021. We will try to never let it get that&nbsp; out of date again. GPT-4 Turbo has knowledge&nbsp;&nbsp;

- [00:02:52](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=172) about the world up to April of 2023. And we will&nbsp; continue to improve that over time. Number four,&nbsp;&nbsp;

- [00:03:01](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=181) new modalities. Surprising no one, DALI 3, GPT-4&nbsp; Turbo with vision, and the new text-to-speech&nbsp;&nbsp;

- [00:03:10](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=190) model are all going into the API today. We have&nbsp; a handful of customers that have just started&nbsp;&nbsp;

- [00:03:21](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=201) using DALI 3 to programmatically generate images&nbsp; and designs. Today, Coke is launching a campaign&nbsp;&nbsp;

- [00:03:29](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=209) that lets its customers generate Diwali cards&nbsp; using DALI 3. And of course, our safety systems&nbsp;&nbsp;

- [00:03:34](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=214) help developers protect their applications&nbsp; against misuse. Those tools are available in&nbsp;&nbsp;

- [00:03:38](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=218) the API. GPT-4 Turbo can now accept images as&nbsp; inputs via the API. It can generate captions,&nbsp;&nbsp;

- [00:03:46](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=226) classifications, and analysis. For example, Be My&nbsp; Eyes uses this technology to help people who are&nbsp;&nbsp;

- [00:03:52](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=232) blind or have low vision with their daily tasks&nbsp; like identifying products in front of them. And&nbsp;&nbsp;

- [00:04:00](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=240) with our new text-to-speech model, you'll be&nbsp; able to generate incredibly natural sounding&nbsp;&nbsp;

- [00:04:05](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=245) audio from text in the API with six preset&nbsp; voices to choose from. I'll play an example.&nbsp;&nbsp;

- [00:04:12](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=252) Did you know that Alexander Graham Bell, the&nbsp; eminent inventor, was enchanted by the world&nbsp;&nbsp;

- [00:04:16](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=256) of sounds? His ingenious mind led to the creation&nbsp; of the graphophone, which etches sounds onto wax,&nbsp;&nbsp;

- [00:04:22](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=262) making voices whisper through time. This is much&nbsp; more natural than anything else we've heard out&nbsp;&nbsp;

- [00:04:28](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=268) there. Voice can make apps more natural to&nbsp; interact with and more accessible. It also&nbsp;&nbsp;

- [00:04:34](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=274) unlocks a lot of use cases like language learning&nbsp; and voice assistance. Speaking of new modalities,&nbsp;&nbsp;

- [00:04:41](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=281) we're also releasing the next version of&nbsp; our open-source speech recognition model,&nbsp;&nbsp;

- [00:04:45](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=285) Whisper V3, today, and it'll be coming soon&nbsp; to the API. It features improved performance&nbsp;&nbsp;

- [00:04:50](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=290) across many languages, and we think you're&nbsp; really going to like it. Okay, number five,&nbsp;&nbsp;

- [00:04:56](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=296) customization. We've been doing really well for&nbsp; GPT-3.5 since we launched it a few months ago.&nbsp;&nbsp;

- [00:05:03](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=303) Starting today, we're going to expand that to the&nbsp; 16K version of the model. Also starting today,&nbsp;&nbsp;

- [00:05:10](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=310) we're inviting active fine-tuning users to apply&nbsp; for the GPT-4 fine-tuning experimental access&nbsp;&nbsp;

- [00:05:15](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=315) program. The fine-tuning API is great for adapting&nbsp; our models to achieve better performance in a wide&nbsp;&nbsp;

- [00:05:22](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=322) variety of applications with a relatively small&nbsp; amount of data. But you may want a model to&nbsp;&nbsp;

- [00:05:27](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=327) learn a completely new knowledge domain. Or&nbsp; to use a lot of proprietary data. So today,&nbsp;&nbsp;

- [00:05:32](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=332) we're launching a new program called Custom&nbsp; Models. With Custom Models, our researchers will&nbsp;&nbsp;

- [00:05:38](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=338) work closely with the company to help them make a&nbsp; great custom model, especially for them and their&nbsp;&nbsp;

- [00:05:44](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=344) use case using our tools. This includes modifying&nbsp; every step of the model training process,&nbsp;&nbsp;

- [00:05:50](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=350) doing additional domain-specific pre-training,&nbsp; a custom RL post-training process tailored&nbsp;&nbsp;

- [00:05:55](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=355) for a specific domain, and whatever else. We&nbsp; won't be able to do this with the many companies&nbsp;&nbsp;

- [00:05:59](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=359) to start. It'll take a lot of work, and in the&nbsp; interest of expectations, at least initially,&nbsp;&nbsp;

- [00:06:04](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=364) it won't be cheap. But if you're excited to&nbsp; push things as far as they can currently go,&nbsp;&nbsp;

- [00:06:08](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=368) please get in touch with us, and we think we&nbsp; can do something pretty great. Okay. And then&nbsp;&nbsp;

- [00:06:13](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=373) number six, higher rate limits. We're doubling the&nbsp; tokens per minute for all of our established GPT-4&nbsp;&nbsp;

- [00:06:19](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=379) customers so that it's easier to do more. And&nbsp; you'll be able to request changes to further rate&nbsp;&nbsp;

- [00:06:24](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=384) limits and quotas directly in your API account&nbsp; settings. In addition to the these rate limits,&nbsp;&nbsp;

- [00:06:30](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=390) it's important to do everything we can do to&nbsp; make you successful building on our platform.&nbsp;&nbsp;

- [00:06:37](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=397) So we're introducing Copyright Shield. Copyright&nbsp; Shield means that we will step in and defend&nbsp;&nbsp;

- [00:06:42](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=402) our customers and pay the costs incurred if you&nbsp; face legal claims around copyright infringement,&nbsp;&nbsp;

- [00:06:48](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=408) and this applies both to ChatGPT Enterprise and&nbsp; the API. And let me be clear, this is a good time&nbsp;&nbsp;

- [00:06:55](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=415) to remind people, we do not train on data from the&nbsp; API or ChatGPT Enterprise ever. All right. There's&nbsp;&nbsp;

- [00:07:04](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=424) actually one more developer request that's been&nbsp; even bigger than all of these. And so I'd like&nbsp;&nbsp;

- [00:07:09](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=429) to talk about that now. And that's pricing. GPT-4&nbsp; Turbo is the industry leading model. It delivers&nbsp;&nbsp;

- [00:07:21](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=441) a lot of improvements that we just covered, and&nbsp; it's a smarter model than GPT-4. We've heard from&nbsp;&nbsp;

- [00:07:28](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=448) developers that there are a lot of things that&nbsp; they want to build, but GPT-4 just costs too&nbsp;&nbsp;

- [00:07:33](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=453) much. They've told us that if we could decrease&nbsp; the cost by 20, 25 percent, that would be great,&nbsp;&nbsp;

- [00:07:39](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=459) a huge leap forward. I'm super excited to&nbsp; announce that we worked really hard on this,&nbsp;&nbsp;

- [00:07:45](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=465) and GPT-4 Turbo, a better model, is considerably&nbsp; cheaper than GPT-4 by a factor of 3x for prompt&nbsp;&nbsp;

- [00:07:53](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=473) tokens. And 2x for completion tokens starting&nbsp; today. So the new pricing is 1 cent per 1,000&nbsp;&nbsp;

- [00:08:11](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=491) prompt tokens and 3 cents per 1,000 completion&nbsp; tokens. For most customers, that will lead to&nbsp;&nbsp;

- [00:08:16](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=496) a blended rate more than 2.75 times cheaper&nbsp; to use for GPT-4 Turbo than GPT-4. We worked&nbsp;&nbsp;

- [00:08:22](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=502) super hard to make this happen. We hope you're&nbsp; as excited about it as we are. So, we've decided&nbsp;&nbsp;

- [00:08:32](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=512) to prioritize price first because we had to choose&nbsp; one or the other, but we're going to work on speed&nbsp;&nbsp;

- [00:08:36](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=516) next. We know that speed is important, too. Soon,&nbsp; you will notice GPT-4 Turbo becoming a lot faster.&nbsp;&nbsp;

- [00:08:44](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=524) We're also decreasing the cost of GPT-3.5 Turbo&nbsp; 16K. Also, input tokens are 3x less and output&nbsp;&nbsp;

- [00:08:52](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=532) tokens are 2x less, which means that GPT-3.5 16K&nbsp; is now cheaper than the previous GPT-3.5 4K salah.&nbsp;&nbsp;

- [00:08:59](https://www.youtube.com/watch?v=4mQXxk8FeEE&t=539) model. Running a fine-tuned GPT 3 .5 Turbo 16K&nbsp; version is also cheaper than the old fine-tuned 4K
