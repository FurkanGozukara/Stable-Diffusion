# Google I/O 2024 - Insane AI Update Introducing 1.5 Pro, 1.5 Flash & Gemini Nano - Revolution - 10/11

## Full tutorial link > https://www.youtube.com/watch?v=Q_JaA4lZHW8

[![Google I/O 2024 - Insane AI Update Introducing 1.5 Pro, 1.5 Flash & Gemini Nano - Revolution - 10/11](https://img.youtube.com/vi/Q_JaA4lZHW8/sddefault.jpg)](https://www.youtube.com/watch?v=Q_JaA4lZHW8 "Google I/O 2024 - Insane AI Update Introducing 1.5 Pro, 1.5 Flash & Gemini Nano - Revolution - 10/11")

[![image](https://img.shields.io/discord/772774097734074388?label=Discord&logo=discord)](https://discord.com/servers/software-engineering-courses-secourses-772774097734074388) [![Hits](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/Google-IO-2024-Insane-AI-Update-Introducing-15-Pro-15-Flash-and-Gemini-Nano-Revolution-1011.md.svg?style=plastic&label=Hits%20Since%2025.08.27&labelColor=007ec6&logo=SECourses)](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/Google-IO-2024-Insane-AI-Update-Introducing-15-Pro-15-Flash-and-Gemini-Nano-Revolution-1011.md)
[![Patreon](https://img.shields.io/badge/Patreon-Support%20Me-F2EB0E?style=for-the-badge&logo=patreon)](https://www.patreon.com/c/SECourses) [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&logoColor=black)](https://www.buymeacoffee.com/DrFurkan) [![Furkan Gözükara Medium](https://img.shields.io/badge/Medium-Follow%20Me-800080?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@furkangozukara) [![Codio](https://img.shields.io/static/v1?style=for-the-badge&message=Articles&color=4574E0&logo=Codio&logoColor=FFFFFF&label=CivitAI)](https://civitai.com/user/SECourses/articles) [![Furkan Gözükara Medium](https://img.shields.io/badge/DeviantArt-Follow%20Me-990000?style=for-the-badge&logo=deviantart&logoColor=white)](https://www.deviantart.com/monstermmorpg)

[![YouTube Channel](https://img.shields.io/badge/YouTube-SECourses-C50C0C?style=for-the-badge&logo=youtube)](https://www.youtube.com/SECourses)  [![Furkan Gözükara LinkedIn](https://img.shields.io/badge/LinkedIn-Follow%20Me-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/furkangozukara/)   [![Udemy](https://img.shields.io/static/v1?style=for-the-badge&message=Stable%20Diffusion%20Course&color=A435F0&logo=Udemy&logoColor=FFFFFF&label=Udemy)](https://www.udemy.com/course/stable-diffusion-dreambooth-lora-zero-to-hero/?referralCode=E327407C9BDF0CEA8156) [![Twitter Follow Furkan Gözükara](https://img.shields.io/badge/Twitter-Follow%20Me-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/GozukaraFurkan)


In this video, we dive into the exciting updates from Google I.O. 2024, focusing on the groundbreaking advancements in AI technology. Google introduces the Gemini 1.5 series, featuring 1.5 Pro and the brand new 1.5 Flash, both globally available in over 200 countries. These natively multimodal models can handle text, images, audio, and video inputs with a massive 1 million token context window.

We also explore the upcoming features like video frame extraction, parallel function calling, and context caching. Discover the pricing and best use cases for each model, and see a live demo of AI Studio, the fastest way to build with Gemini.

The video also covers Gemma, Google's family of open models, and introduces Poly Gemma, their first vision language open model. We get a sneak peek at Gemma 2, coming in June, with a powerful 27 billion parameter model.

Lastly, we showcase an inspiring story from India, where developers have used Gemma to create Navarasa, a set of instruction-tuned models that expand access to 15 Indic languages, contributing to Google's efforts in making information accessible in over 7,000 languages worldwide.

Video chapters

[00:00:00](https://youtu.be/Q_JaA4lZHW8?t=0) - Introduction to Google I.O. 2024 Part 10

[00:00:24](https://youtu.be/Q_JaA4lZHW8?t=24) - Gemini 1.5 Series: 1.5 Pro and 1.5 Flash

[00:01:55](https://youtu.be/Q_JaA4lZHW8?t=115) - Developer Features and Pricing

[00:02:35](https://youtu.be/Q_JaA4lZHW8?t=155) - Choosing the Right Model for Your Use Case

[00:03:03](https://youtu.be/Q_JaA4lZHW8?t=183) - AI Studio Demo

[00:04:41](https://youtu.be/Q_JaA4lZHW8?t=281) - Gemma: Google's Open Models

[00:05:54](https://youtu.be/Q_JaA4lZHW8?t=354) - Introducing Gemma 2

[00:06:59](https://youtu.be/Q_JaA4lZHW8?t=419) - Navarasa: Expanding Access to Indic Languages

The video, which is part 10 of the Google I.O. 2024 series, delves into the latest advancements in Google's AI technology. The main focus is on the Gemini 1.5 series, which includes the improved 1.5 Pro and the brand new 1.5 Flash. Both models are now globally available in over 200 countries and territories, and developers can access them through AI Studio or Vertex AI.

One of the key features of the Gemini 1.5 series is its native multimodal capabilities. This means that developers can interleave text, images, audio, and video as inputs within the impressive 1 million token context window. Google is also offering the option to sign up for a 2 million token context window for 1.5 Pro.

Several new developer features are being introduced, such as video frame extraction, parallel function calling, and context caching. Context caching, in particular, allows developers to send all their files to the model once, making the long context more useful and affordable.

The video also discusses the pricing for the Gemini 1.5 series. 1.5 Pro is priced at $7 per 1 million tokens, with a 50% discount for prompts up to 128k, bringing the price down to $3.50. 1.5 Flash starts at a more affordable 35 cents per 1 million tokens.

Developers are advised to use 1.5 Pro for complex tasks that require the highest quality response, even if it takes a little longer. In contrast, 1.5 Flash is better suited for quick tasks where the speed of the model is the top priority.

The video includes a live demo of AI Studio, showcasing how developers can quickly prototype and generate code for their projects. AI Studio is free to use and requires no configuration. If developers need more enterprise-grade features, they can seamlessly transition to using the same Gemini 1.5 models and configurations in Vertex AI.

The focus then shifts to Gemma, Google's family of open models. Gemma has been downloaded millions of times across major model hubs and is used by developers and researchers for customization and pre-trained variants like Recurrent Gemma and Code Gemma. The newest addition to the family is Poly Gemma, the first vision language open model, which is optimized for image captioning, visual Q&A, and other image labeling tasks.

Looking ahead, Google announces Gemma 2, the next generation of Gemma, set to release in June. Gemma 2 will feature a new 27 billion parameter model, addressing developers' requests for a larger model that is still easy to use. This size is optimized by NVIDIA to run on next-gen GPUs and can efficiently run on a single TPU host in Vertex AI, outperforming models more than twice its size.

The video concludes with an inspiring story from India, where developers have used Gemma's unique tokenization to create Navarasa. Navarasa is a set of instruction-tuned models that expand access to 15 Indic languages, contributing to Google's efforts to make information accessible in more than 7,000 languages worldwide. This showcases the power of generative AI in solving real-world problems and ensuring that no one is left behind in the technological advancement.



### Video Transcription


- [00:00:00](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=0) Ok, we continue to the Google I.O. 2024 with the part 10 right now.

- [00:00:07](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=7) Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone.

- [00:00:13](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=13) That was our plan all along to create a natively multimodal Gemini in a range of sizes

- [00:00:19](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=19) so you all, as developers, can choose the one that works best for you.

- [00:00:24](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=24) Throughout the morning, you've heard a lot about our Gemini 1.5 series.

- [00:00:29](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=29) And I want to talk about the two models you can access today.

- [00:00:32](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=32) 1.5 Pro, which is getting a series of quality improvements that go out right about now,

- [00:00:39](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=39) and the brand new 1.5 Flash.

- [00:00:42](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=42) Both are available today globally in over 200 countries and territories.

- [00:00:52](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=52) You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try.

- [00:00:57](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=57) Now both of these models are natively multimodal.

- [00:01:01](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=61) That means you can interleave text, images, audio, and video as inputs

- [00:01:06](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=66) and pack that massive 1 million token context window.

- [00:01:10](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=70) And if you go to AI.GoogleDev today, you can sign up to try the 2 million token context window for 1.5 Pro.

- [00:01:19](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=79) We're also adding a bunch of new developer features starting with video frame extraction.

- [00:01:25](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=85) That's going to be in the Gemini API.

- [00:01:27](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=87) Parallel function calling, so you can return more than one function call at a time.

- [00:01:32](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=92) And my favorite, context caching, so you can send all of your files to the model once

- [00:01:39](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=99) and not have to resend them over and over again.

- [00:01:42](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=102) That should make the long context even more useful and more affordable.

- [00:01:47](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=107) It ships next month.

- [00:01:55](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=115) Now we're using Google's infrastructure to serve these models,

- [00:01:59](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=119) so developers like all of you can get great prices.

- [00:02:03](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=123) 1.5 Pro is $7 per 1 million tokens.

- [00:02:08](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=128) And I'm excited to share that for prompts up to 128k, it'll be 50% less for $3.50.

- [00:02:17](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=137) And 1.5 Flash will start at 35 cents per 1 million tokens.

- [00:02:25](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=145) Now one thing you might be wondering is which model is best for your use case.

- [00:02:32](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=152) Here's how we've been thinking about it on the team.

- [00:02:35](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=155) We use 1.5 Pro for complex tasks where you really want the highest quality response.

- [00:02:41](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=161) And it's okay if it takes a little bit longer to come back.

- [00:02:44](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=164) We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most.

- [00:02:51](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=171) And as a developer, you can go try them both out today and see what works best for you.

- [00:02:57](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=177) Now I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini.

- [00:03:03](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=183) We'll pull it up here, and you can see this is AI Studio.

- [00:03:07](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=187) It's free to use. You don't have to configure anything to get going.

- [00:03:11](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=191) You just go to ai-studio.google.com, log in with your Google account,

- [00:03:16](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=196) and you can just pick the model here in the right that works best for you.

- [00:03:20](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=200) So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products.

- [00:03:28](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=208) Flash makes this possible with its low latency.

- [00:03:31](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=211) So what we did here is we just took a bunch of different feedback from our customer forums.

- [00:03:37](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=217) You can put it into Flash, load up a prompt, and hit run.

- [00:03:41](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=221) Now in the background, what it's going to do is it's going to go through that 93,000 token pile of information,

- [00:03:48](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=228) and you can see here, start streaming it back.

- [00:03:51](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=231) Now this is really helpful because it pulls out the themes for us.

- [00:03:54](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=234) It gives us all the right places where we can start to look,

- [00:03:57](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=237) and you can see this is from some of the benefits from Notebook LM, like we showed earlier.

- [00:04:02](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=242) Now what's great about this is that you can take something like this in AI Studio,

- [00:04:07](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=247) prototyped here in 10 seconds, and with one click in the upper left, get an API key,

- [00:04:14](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=254) or over here in the upper right, just tap get code,

- [00:04:17](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=257) and you've got all of the model configurations, the safety settings ready to go straight into your IDE.

- [00:04:24](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=264) Now over time, if you find that you need more enterprise grade features,

- [00:04:28](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=268) you can use the same Gemini 1.5 models and the same configurations right in Vertex AI.

- [00:04:35](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=275) That way you can scale up with Google Cloud as your enterprise needs grow.

- [00:04:41](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=281) So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash,

- [00:04:47](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=287) both of which are available today globally,

- [00:04:50](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=290) and you'll hear a lot more about them in the developer keynote later today.

- [00:05:02](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=302) Now let's shift gears and talk about Gemma, our family of open models

- [00:05:06](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=306) which are crucial for driving AI innovation and responsibility.

- [00:05:11](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=311) Gemma is built from the same research and technology as Gemini.

- [00:05:15](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=315) It offers top performance and comes in lightweight 7B and 2B sizes.

- [00:05:21](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=321) Now since it launched less than three months ago, it's been downloaded millions of times

- [00:05:26](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=326) across all the major model hubs.

- [00:05:29](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=329) Developers and researchers have been using it and customizing the base Gemma model

- [00:05:34](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=334) and using some of our pre-trained variants like Recurrent Gemma and Code Gemma.

- [00:05:40](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=340) And today's newest member, Poly Gemma, our first vision language open model,

- [00:05:47](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=347) and it's available right now.

- [00:05:54](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=354) It's optimized for a range of image captioning, visual Q&amp;A,

- [00:05:59](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=359) and other image labeling tasks, so go give it a try.

- [00:06:03](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=363) I'm also too excited to announce that we have Gemma 2 coming.

- [00:06:08](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=368) It's the next generation of Gemma, and it will be available in June.

- [00:06:13](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=373) One of the top requests we've heard from developers is for a bigger Gemma model,

- [00:06:18](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=378) but it's still going to fit in a size that's easy for all of you to use.

- [00:06:22](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=382) So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2.

- [00:06:28](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=388) And here's what's great about it.

- [00:06:31](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=391) This size is optimized by NVIDIA to run on next gen GPUs

- [00:06:36](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=396) and can run efficiently on a single TPU host in Vertex AI.

- [00:06:42](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=402) So this quality to size ratio is amazing

- [00:06:45](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=405) because it'll outperform models more than twice its size.

- [00:06:49](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=409) We can't wait to see what you're going to build with it.

- [00:06:52](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=412) Thank you.

- [00:06:59](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=419) To wrap up, I want to share this inspiring story from India

- [00:07:02](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=422) where developers have been using Gemma and its unique tokenization

- [00:07:06](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=426) to create Navarasa, a set of instruction-tuned models

- [00:07:11](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=431) to expand access to 15 Indic languages.

- [00:07:15](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=435) This builds on our efforts to make information accessible

- [00:07:18](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=438) in more than 7,000 languages around the world.

- [00:07:21](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=441) Take a look.

- [00:07:32](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=452) Language is a very interesting problem to solve, actually.

- [00:07:36](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=456) Given India has a huge variety of languages

- [00:07:40](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=460) and it changes every five kilometers.

- [00:07:45](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=465) When technology is developed for a particular culture,

- [00:07:48](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=468) it won't be able to solve and understand the nuances of a country like India.

- [00:07:54](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=474) One of Gemma's features is an incredibly powerful tokenizer

- [00:07:58](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=478) which enables the model to use hundreds of thousands of words, symbols, and characters

- [00:08:03](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=483) across so many alphabets and language systems.

- [00:08:06](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=486) This large vocabulary is critical to adapting Gemma to power projects like Navarasa.

- [00:08:13](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=493) Navarasa is a model that's trained for Indic languages.

- [00:08:17](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=497) It's a fine-tuned model based on Google's Gemma.

- [00:08:20](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=500) We built Navarasa to make large language models culturally rooted

- [00:08:24](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=504) where people can talk in their native language

- [00:08:27](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=507) and get the responses in their native language.

- [00:08:30](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=510) Our biggest dream is to build a model to include everyone from all corners of India.

- [00:08:37](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=517) Using AI, we need a technology where no one is left behind and everyone can use it.

- [00:08:43](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=523) Today, the language that you speak in could be the tool and the technology that you use

- [00:08:48](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=528) for solving your real-world problems.

- [00:08:51](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=531) And that's the power of generative AI

- [00:08:53](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=533) that we want to bring to every corner of India and the entire world.

- [00:09:07](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=547) www.mooji.org

- [00:09:09](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=549) Copyright © 2020 Mooji Media Ltd. All Rights Reserved.

- [00:09:12](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=552) No part of this recording may be reproduced

- [00:09:15](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=555) without Mooji Media Ltd.'s express consent.

- [00:09:18](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=558) No part of this recording may be reproduced

- [00:09:21](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=561) without Mooji Media Ltd.'s express consent.

- [00:09:24](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=564) No part of this recording may be reproduced

- [00:09:27](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=567) without Mooji Media Ltd.'s express consent.

- [00:09:30](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=570) No part of this recording may be reproduced

- [00:09:33](https://www.youtube.com/watch?v=Q_JaA4lZHW8&t=573) without Mooji Media Ltd.'s express consent.
