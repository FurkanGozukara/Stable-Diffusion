# Wan 2.1 Text-to-Video T2V & Image-to-Video I2V Tutorial for SwarmUI with CausVid LoRA Extreme Speed

## Full tutorial link > https://www.youtube.com/watch?v=XNcn845UXdw

[![Wan 2.1 Text-to-Video T2V & Image-to-Video I2V Tutorial for SwarmUI with CausVid LoRA Extreme Speed](https://img.youtube.com/vi/XNcn845UXdw/sddefault.jpg)](https://www.youtube.com/watch?v=XNcn845UXdw "Wan 2.1 Text-to-Video T2V & Image-to-Video I2V Tutorial for SwarmUI with CausVid LoRA Extreme Speed")

[![image](https://img.shields.io/discord/772774097734074388?label=Discord&logo=discord)](https://discord.com/servers/software-engineering-courses-secourses-772774097734074388) [![Hits](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/Wan-21-Text-to-Video-T2V-and-Image-to-Video-I2V-Tutorial-for-SwarmUI-with-CausVid-LoRA-Extreme-Speed.md.svg?style=plastic&label=Hits%20Since%2025.08.27&labelColor=007ec6&logo=SECourses)](https://hits.sh/github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/Wan-21-Text-to-Video-T2V-and-Image-to-Video-I2V-Tutorial-for-SwarmUI-with-CausVid-LoRA-Extreme-Speed.md)
[![Patreon](https://img.shields.io/badge/Patreon-Support%20Me-F2EB0E?style=for-the-badge&logo=patreon)](https://www.patreon.com/c/SECourses) [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&logoColor=black)](https://www.buymeacoffee.com/DrFurkan) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/Medium-Follow%20Me-800080?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@furkangozukara) [![Codio](https://img.shields.io/static/v1?style=for-the-badge&message=Articles&color=4574E0&logo=Codio&logoColor=FFFFFF&label=CivitAI)](https://civitai.com/user/SECourses/articles) [![Furkan G√∂z√ºkara Medium](https://img.shields.io/badge/DeviantArt-Follow%20Me-990000?style=for-the-badge&logo=deviantart&logoColor=white)](https://www.deviantart.com/monstermmorpg)

[![YouTube Channel](https://img.shields.io/badge/YouTube-SECourses-C50C0C?style=for-the-badge&logo=youtube)](https://www.youtube.com/SECourses)  [![Furkan G√∂z√ºkara LinkedIn](https://img.shields.io/badge/LinkedIn-Follow%20Me-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/furkangozukara/)   [![Udemy](https://img.shields.io/static/v1?style=for-the-badge&message=Stable%20Diffusion%20Course&color=A435F0&logo=Udemy&logoColor=FFFFFF&label=Udemy)](https://www.udemy.com/course/stable-diffusion-dreambooth-lora-zero-to-hero/?referralCode=E327407C9BDF0CEA8156) [![Twitter Follow Furkan G√∂z√ºkara](https://img.shields.io/badge/Twitter-Follow%20Me-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/GozukaraFurkan)


Wan 2.1 is still the very best local AI video generation model and it just became even more amazing and faster with CausVid LoRA. Now with utilizing ComfyUI backend power inside SwarmUI and my automatic installers to utilize Sage Attention, you can very fast generate very high quality AI videos with Wan 2.1 and CausVid LoRA at just 8 steps.

üîóFollow below link to download the zip file that contains SwarmUI installer and AI models downloader Gradio App - the one used in the tutorial ‚§µÔ∏è

‚ñ∂Ô∏è [https://www.patreon.com/posts/SwarmUI-Installer-AI-Videos-Downloader-114517862](https://www.patreon.com/posts/SwarmUI-Installer-AI-Videos-Downloader-114517862)

‚ñ∂Ô∏è How to install SwarmUI main tutorial : [https://youtu.be/fTzlQ0tjxj0](https://youtu.be/fTzlQ0tjxj0)

üîóFollow below link to download the zip file that contains ComfyUI 1-click installer that has all the Flash Attention, Sage Attention, xFormers, Triton, DeepSpeed, RTX 5000 series support ‚§µÔ∏è

‚ñ∂Ô∏è [https://www.patreon.com/posts/Advanced-ComfyUI-1-Click-Installer-105023709](https://www.patreon.com/posts/Advanced-ComfyUI-1-Click-Installer-105023709)

üîó Python, Git, CUDA, C++, FFMPEG, MSVC installation tutorial - needed for ComfyUI ‚§µÔ∏è

‚ñ∂Ô∏è [https://youtu.be/DrhUHnYfwC0](https://youtu.be/DrhUHnYfwC0)

üîó SECourses Official Discord 10500+ Members ‚§µÔ∏è

‚ñ∂Ô∏è [https://discord.com/servers/software-engineering-courses-secourses-772774097734074388](https://discord.com/servers/software-engineering-courses-secourses-772774097734074388)

üîó Stable Diffusion, FLUX, Generative AI Tutorials and Resources GitHub ‚§µÔ∏è

‚ñ∂Ô∏è [https://github.com/FurkanGozukara/Stable-Diffusion](https://github.com/FurkanGozukara/Stable-Diffusion)

üîó SECourses Official Reddit - Stay Subscribed To Learn All The News and More ‚§µÔ∏è

‚ñ∂Ô∏è [https://www.reddit.com/r/SECourses/](https://www.reddit.com/r/SECourses/)

‚ñ∂Ô∏è CausVid LoRA Official repo : [https://github.com/tianweiy/CausVid](https://github.com/tianweiy/CausVid)

VIDEO CHAPTERS

[00:00:00](https://youtu.be/XNcn845UXdw?t=0) Introduction & Amazing Demo

[00:00:23](https://youtu.be/XNcn845UXdw?t=23) Tutorial Goals: Video Gen (1.2.1), Speedups (CowsWith, Rife)

[00:00:35](https://youtu.be/XNcn845UXdw?t=35) SwarmUI: Installation & Update Process

[00:00:57](https://youtu.be/XNcn845UXdw?t=57) SwarmUI Downloader: 1.2.1 Model & CowsWith Lora

[00:01:48](https://youtu.be/XNcn845UXdw?t=108) Optional: Integrating Models with ComfyUI

[00:02:12](https://youtu.be/XNcn845UXdw?t=132) SwarmUI Start, Config & Rife Interpolation

[00:02:38](https://youtu.be/XNcn845UXdw?t=158) Image-to-Video: Importing Presets

[00:02:51](https://youtu.be/XNcn845UXdw?t=171) Image-to-Video: GGUF Model Selection & VRAM

[00:03:09](https://youtu.be/XNcn845UXdw?t=189) Image-to-Video: Optimal Resolution & Aspect Ratio

[00:03:21](https://youtu.be/XNcn845UXdw?t=201) Image-to-Video: CowsWith Lora & "Fast CowsWith" Preset

[00:03:44](https://youtu.be/XNcn845UXdw?t=224) Image-to-Video: Key Parameters (Steps, CFG, Init Image)

[00:04:04](https://youtu.be/XNcn845UXdw?t=244) Image-to-Video: Creativity (0) & Frame Count

[00:04:26](https://youtu.be/XNcn845UXdw?t=266) VRAM Management: Avoiding Shared VRAM Slowdowns

[00:04:40](https://youtu.be/XNcn845UXdw?t=280) Image-to-Video: Rife x2 (Double FPS) & Advanced Settings

[00:04:56](https://youtu.be/XNcn845UXdw?t=296) Image-to-Video: Trimming Frames & Crafting Prompt

[00:05:24](https://youtu.be/XNcn845UXdw?t=324) Dual GPU I2V Gen: RTX 5090 vs 3090Ti

[00:05:55](https://youtu.be/XNcn845UXdw?t=355) I2V Speed, VRAM & First Result Analysis (RTX 5090: 5.7s/it)

[00:06:35](https://youtu.be/XNcn845UXdw?t=395) First I2V Result Review & Iteration Needs

[00:06:52](https://youtu.be/XNcn845UXdw?t=412) Second I2V Result: AI Fixes Missing Parts!

[00:07:20](https://youtu.be/XNcn845UXdw?t=440) Recap: Power of Optimized SwarmUI

[00:07:33](https://youtu.be/XNcn845UXdw?t=453) Text-to-Video: Switching & Model Setup

[00:08:04](https://youtu.be/XNcn845UXdw?t=484) Text-to-Video: Applying T2V Presets

[00:08:19](https://youtu.be/XNcn845UXdw?t=499) Text-to-Video: Key Parameter Differences

[00:08:34](https://youtu.be/XNcn845UXdw?t=514) Text-to-Video: Setting Resolution & Rife

[00:09:02](https://youtu.be/XNcn845UXdw?t=542) Text-to-Video: Prompting & Ensuring Lora

[00:09:17](https://youtu.be/XNcn845UXdw?t=557) Speed vs Quality: T-Cash & Sage Attention

[00:09:30](https://youtu.be/XNcn845UXdw?t=570) Text-to-Video: Dual GPU Generation Start & Setup

[00:10:03](https://youtu.be/XNcn845UXdw?t=603) Text-to-Video: VRAM Check & Speed Expectations

[00:11:51](https://youtu.be/XNcn845UXdw?t=711) Text-to-Video Speed Analysis: 5090 (8.4s/it) vs 3090Ti (18.2s/it)

[00:12:01](https://youtu.be/XNcn845UXdw?t=721) Text-to-Video Result (576x1008) Review: "Really Great!"

[00:12:55](https://youtu.be/XNcn845UXdw?t=775) Teaser: "My Diffusion Based Upscaler" & Quick Peek

[00:13:10](https://youtu.be/XNcn845UXdw?t=790) Upscaler Features: Splitting, Per-Clip Prompting/Upscaling

[00:13:35](https://youtu.be/XNcn845UXdw?t=815) Upscaler Features: Auto-Caption (CogVLM2), Ratio Control

[00:13:45](https://youtu.be/XNcn845UXdw?t=825) Upscaler Features: Batch Processing & Max Frame Control

[00:13:51](https://youtu.be/XNcn845UXdw?t=831) Upscaler: Quality Goal (10x+), Optimizations & Ideas

[00:18:01](https://youtu.be/XNcn845UXdw?t=1081) Upscaler: FFmpeg Presets, Dev Status & Vision

[00:18:13](https://youtu.be/XNcn845UXdw?t=1093) Final Recap: Hope You Enjoyed & Generated Videos Review

[00:18:20](https://youtu.be/XNcn845UXdw?t=1100) Generated Video Quality & Time Assessment: "Magnificent!"

[00:18:24](https://youtu.be/XNcn845UXdw?t=1104) Final Timings: RTX 3090Ti (170s) vs RTX 5090 (90s)

[00:18:32](https://youtu.be/XNcn845UXdw?t=1112) Your Choice: Resolution, Frames, Speed & VRAM Balance

ABSTRACT

Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching.

Music provided by NoCopyrightSounds :

Music  Credits :  [https://gist.github.com/FurkanGozukara/8e16fcf8e82b68e71352e41cdc0ea395](https://gist.github.com/FurkanGozukara/8e16fcf8e82b68e71352e41cdc0ea395)



### Video Transcription


- [00:00:00](https://www.youtube.com/watch?v=XNcn845UXdw&t=0) Just from this input image, this video has&nbsp; been generated under 60 seconds locally on my&nbsp;&nbsp;

- [00:00:06](https://www.youtube.com/watch?v=XNcn845UXdw&t=6) computer. So, this is the power of SwarmUI&nbsp; and ComfyUI backend with my installation,&nbsp;&nbsp;

- [00:00:12](https://www.youtube.com/watch?v=XNcn845UXdw&t=12) with my workflow, setup, and optimizations.&nbsp; And it is just amazing, amazing motion,&nbsp;&nbsp;

- [00:00:18](https://www.youtube.com/watch?v=XNcn845UXdw&t=18) amazing movement. It also completed the&nbsp; missing arm, so this is just perfect.&nbsp;

- [00:00:23](https://www.youtube.com/watch?v=XNcn845UXdw&t=23) Greetings everyone. Today, I am going to introduce&nbsp; you to how to generate videos inside SwarmUI with&nbsp;&nbsp;

- [00:00:29](https://www.youtube.com/watch?v=XNcn845UXdw&t=29) Wan 2.1 model, including both image-to-video&nbsp; and also text-to-video, because how to use&nbsp;&nbsp;

- [00:00:37](https://www.youtube.com/watch?v=XNcn845UXdw&t=37) this model inside SwarmUI is not very clear, hard&nbsp; to understand. Moreover, there is something huge,&nbsp;&nbsp;

- [00:00:45](https://www.youtube.com/watch?v=XNcn845UXdw&t=45) something new, which is improving the speed&nbsp; of the generation in Wan 2.1 significantly,&nbsp;&nbsp;

- [00:00:52](https://www.youtube.com/watch?v=XNcn845UXdw&t=52) speeding up. It is called as CausVid, CausVid&nbsp; LoRA. Maybe you have heard of it. It was a very&nbsp;&nbsp;

- [00:00:59](https://www.youtube.com/watch?v=XNcn845UXdw&t=59) popular post on Stable Diffusion subreddit&nbsp; and also famous on Civitai AI as well.&nbsp;&nbsp;

- [00:01:04](https://www.youtube.com/watch?v=XNcn845UXdw&t=64) This is bringing a massive speed up, and today&nbsp; we are also going to use it to massively speed&nbsp;&nbsp;

- [00:01:10](https://www.youtube.com/watch?v=XNcn845UXdw&t=70) up our generation speed. Moreover, I will show how&nbsp; to double the FPS of the output video with Rife on&nbsp;&nbsp;

- [00:01:18](https://www.youtube.com/watch?v=XNcn845UXdw&t=78) SwarmUI. Everything will be, as usual, one-click&nbsp; install, download, and use. All you need to do&nbsp;&nbsp;

- [00:01:25](https://www.youtube.com/watch?v=XNcn845UXdw&t=85) will be just type your prompt and enjoy the view. So, let's begin with the installation. If you&nbsp;&nbsp;

- [00:01:32](https://www.youtube.com/watch?v=XNcn845UXdw&t=92) already have previously installed, just go to here&nbsp; and download the latest ZIP file. If you didn't&nbsp;&nbsp;

- [00:01:38](https://www.youtube.com/watch?v=XNcn845UXdw&t=98) yet, please watch this tutorial and learn how to&nbsp; install. The link will be in the description of&nbsp;&nbsp;

- [00:01:44](https://www.youtube.com/watch?v=XNcn845UXdw&t=104) the video. So, I will get this ZIP file into my&nbsp; SwarmUI installation, which is here. Paste it,&nbsp;&nbsp;

- [00:01:51](https://www.youtube.com/watch?v=XNcn845UXdw&t=111) then right-click and WinRAR extract here, and&nbsp; I will overwrite all the previous files. Then&nbsp;&nbsp;

- [00:01:57](https://www.youtube.com/watch?v=XNcn845UXdw&t=117) double-click the start download models app.bat&nbsp; file, more info, run anyway. This will update&nbsp;&nbsp;

- [00:02:03](https://www.youtube.com/watch?v=XNcn845UXdw&t=123) everything first, then it will start the&nbsp; SwarmUI downloader. Updating libraries may&nbsp;&nbsp;

- [00:02:08](https://www.youtube.com/watch?v=XNcn845UXdw&t=128) take like 1 minute or maybe more, depending on&nbsp; your internet speed. Okay, everything updated,&nbsp;&nbsp;

- [00:02:14](https://www.youtube.com/watch?v=XNcn845UXdw&t=134) and the downloader started. This is the latest&nbsp; version of downloader. Then I have implemented&nbsp;&nbsp;

- [00:02:20](https://www.youtube.com/watch?v=XNcn845UXdw&t=140) a Wan 2.1 GGUF bundle. You can download this,&nbsp; and you can also manually download the models&nbsp;&nbsp;

- [00:02:27](https://www.youtube.com/watch?v=XNcn845UXdw&t=147) from the video generation models you see inside&nbsp; Wan 2.1. There are a lot of models. So, let's&nbsp;&nbsp;

- [00:02:32](https://www.youtube.com/watch?v=XNcn845UXdw&t=152) just download the bundle for now. This will also&nbsp; download the necessary LoRA file. The LoRA file&nbsp;&nbsp;

- [00:02:38](https://www.youtube.com/watch?v=XNcn845UXdw&t=158) is also available inside the LoRA section here,&nbsp; various LoRA models, and you will see that Wan 2.1&nbsp;&nbsp;

- [00:02:45](https://www.youtube.com/watch?v=XNcn845UXdw&t=165) CausVid text-to-video, image-to-video LoRA model.&nbsp; Just wait for downloads to be completed. It will&nbsp;&nbsp;

- [00:02:51](https://www.youtube.com/watch?v=XNcn845UXdw&t=171) download everything automatically into accurate&nbsp; SwarmUI folder. If you want to use them inside&nbsp;&nbsp;

- [00:02:56](https://www.youtube.com/watch?v=XNcn845UXdw&t=176) ComfyUI, just check this checkbox and give the&nbsp; models path of your ComfyUI. For example, we are&nbsp;&nbsp;

- [00:03:04](https://www.youtube.com/watch?v=XNcn845UXdw&t=184) using the ComfyUI from here. So, I enter inside&nbsp; the ComfyUI models, copy this path, paste it here,&nbsp;&nbsp;

- [00:03:12](https://www.youtube.com/watch?v=XNcn845UXdw&t=192) check this box, then click the download button,&nbsp; and then it will download them into the accurate&nbsp;&nbsp;

- [00:03:17](https://www.youtube.com/watch?v=XNcn845UXdw&t=197) folders. So, since I previously have downloaded&nbsp; some of the models, they are all skipped. It&nbsp;&nbsp;

- [00:03:22](https://www.youtube.com/watch?v=XNcn845UXdw&t=202) will only download the new necessary files, as&nbsp; we are seeing right now. And the download is&nbsp;&nbsp;

- [00:03:27](https://www.youtube.com/watch?v=XNcn845UXdw&t=207) fully optimized. It will use your entire internet&nbsp; speed. If you get errors, you can uncheck this,&nbsp;&nbsp;

- [00:03:33](https://www.youtube.com/watch?v=XNcn845UXdw&t=213) but before unchecking it, restart the application,&nbsp; and it should work. Also, if you get download&nbsp;&nbsp;

- [00:03:39](https://www.youtube.com/watch?v=XNcn845UXdw&t=219) errors, it could be due to your VPN, antivirus,&nbsp; so disable them, change them, and try again. So,&nbsp;&nbsp;

- [00:03:45](https://www.youtube.com/watch?v=XNcn845UXdw&t=225) all the models have been downloaded. We can see&nbsp; the queue size is zero. This is fully queue-based&nbsp;&nbsp;

- [00:03:50](https://www.youtube.com/watch?v=XNcn845UXdw&t=230) system, and everything has been downloaded. Now, what we need to do is, first of all,&nbsp;&nbsp;

- [00:03:55](https://www.youtube.com/watch?v=XNcn845UXdw&t=235) I recommend to update your SwarmUI. You will&nbsp; see that we have Windows update SwarmUI. So,&nbsp;&nbsp;

- [00:04:00](https://www.youtube.com/watch?v=XNcn845UXdw&t=240) double-click it, more info, run anyway. It&nbsp; will update the SwarmUI to the latest version,&nbsp;&nbsp;

- [00:04:05](https://www.youtube.com/watch?v=XNcn845UXdw&t=245) then click Windows start SwarmUI.bat file,&nbsp; run anyway. And the SwarmUI has been started.&nbsp;&nbsp;

- [00:04:10](https://www.youtube.com/watch?v=XNcn845UXdw&t=250) First of all, I recommend you to enable display&nbsp; advanced options from here. Then, what you should&nbsp;&nbsp;

- [00:04:17](https://www.youtube.com/watch?v=XNcn845UXdw&t=257) do is, in image-to-video, there is video frame&nbsp; interpolation method. So, first of all, you need&nbsp;&nbsp;

- [00:04:24](https://www.youtube.com/watch?v=XNcn845UXdw&t=264) to install it. Once you installed it, you will&nbsp; see a button here to install. Restart SwarmUI.&nbsp;&nbsp;

- [00:04:30](https://www.youtube.com/watch?v=XNcn845UXdw&t=270) If in the first restart, if it doesn't work, you&nbsp; can restart again, and you will have the video&nbsp;&nbsp;

- [00:04:35](https://www.youtube.com/watch?v=XNcn845UXdw&t=275) frame interpolation method automatically enabled&nbsp; and working. Then, what about the parameters, the&nbsp;&nbsp;

- [00:04:40](https://www.youtube.com/watch?v=XNcn845UXdw&t=280) settings? It is super important. I have prepared&nbsp; two presets. Let me delete them to show you. Okay,&nbsp;&nbsp;

- [00:04:47](https://www.youtube.com/watch?v=XNcn845UXdw&t=287) and okay. Then I will click import presets.&nbsp; There is upload JSON file. You can also overwrite&nbsp;&nbsp;

- [00:04:54](https://www.youtube.com/watch?v=XNcn845UXdw&t=294) existing presets, so choose file and amazing&nbsp; SwarmUI presets and import. And they are both&nbsp;&nbsp;

- [00:05:00](https://www.youtube.com/watch?v=XNcn845UXdw&t=300) imported. Don't worry, I will show every settings. So, first, let's begin with image-to-video model,&nbsp;&nbsp;

- [00:05:06](https://www.youtube.com/watch?v=XNcn845UXdw&t=306) which is more preferred model. So, we have our&nbsp; models here. There are a lot of GGUF versions,&nbsp;&nbsp;

- [00:05:11](https://www.youtube.com/watch?v=XNcn845UXdw&t=311) and each version has different quality and&nbsp; VRAM requirements. So, depending on your GPU,&nbsp;&nbsp;

- [00:05:18](https://www.youtube.com/watch?v=XNcn845UXdw&t=318) you should use the accurate version. So, I have&nbsp; RTX 5090, and I have RTX 3090 Ti. Therefore,&nbsp;&nbsp;

- [00:05:25](https://www.youtube.com/watch?v=XNcn845UXdw&t=325) I am going to use GGUF Q6, which is the default&nbsp; model. However, if you have a lower VRAM GPU,&nbsp;&nbsp;

- [00:05:32](https://www.youtube.com/watch?v=XNcn845UXdw&t=332) then you can download the lower GGUF version&nbsp; like GGUF Q4_K_M. This is the lowest GGUF.&nbsp;&nbsp;

- [00:05:40](https://www.youtube.com/watch?v=XNcn845UXdw&t=340) Unfortunately, with SwarmUI, I couldn't find yet,&nbsp; I will look for, there isn't block swapping. So,&nbsp;&nbsp;

- [00:05:45](https://www.youtube.com/watch?v=XNcn845UXdw&t=345) it is fully loading into VRAM. Else, it is&nbsp; using shared VRAM, and if you use shared VRAM,&nbsp;&nbsp;

- [00:05:50](https://www.youtube.com/watch?v=XNcn845UXdw&t=350) it will be so slow. So, first, let's pick the Wan&nbsp; 2.1 image-to-video model GGUF. I have selected&nbsp;&nbsp;

- [00:05:57](https://www.youtube.com/watch?v=XNcn845UXdw&t=357) it. There is one very crucial thing. When you&nbsp; click edit metadata, you need to see that it is&nbsp;&nbsp;

- [00:06:01](https://www.youtube.com/watch?v=XNcn845UXdw&t=361) accurately metadata like this and standard&nbsp; resolution. Now, this is super important.&nbsp;&nbsp;

- [00:06:06](https://www.youtube.com/watch?v=XNcn845UXdw&t=366) Depending on your GPU, I recommend to change this.&nbsp; 640 to 640 is really great, really working good,&nbsp;&nbsp;

- [00:06:13](https://www.youtube.com/watch?v=XNcn845UXdw&t=373) but natively, this model is 960 to 960. And what&nbsp; does this change do? This change will impact all&nbsp;&nbsp;

- [00:06:22](https://www.youtube.com/watch?v=XNcn845UXdw&t=382) of the aspect ratios that you use. So, you see,&nbsp; whatever the aspect ratio I choose here will use&nbsp;&nbsp;

- [00:06:29](https://www.youtube.com/watch?v=XNcn845UXdw&t=389) the base resolution the model has. Therefore, you&nbsp; can also download the 480p model. It is up to you,&nbsp;&nbsp;

- [00:06:36](https://www.youtube.com/watch?v=XNcn845UXdw&t=396) but I am using this model, and I am using 640 to&nbsp; 640. This is working great. So, we selected our&nbsp;&nbsp;

- [00:06:43](https://www.youtube.com/watch?v=XNcn845UXdw&t=403) model. This is our selected model. You can see&nbsp; Wan 2.1 14 billion parameters image-to-video,&nbsp;&nbsp;

- [00:06:48](https://www.youtube.com/watch?v=XNcn845UXdw&t=408) 720p GGUF Q6_K. This is a really, really&nbsp; high-quality model. Then we will select&nbsp;&nbsp;

- [00:06:55](https://www.youtube.com/watch?v=XNcn845UXdw&t=415) our LoRA. You see Wan 2.1 CausVid 14 billion&nbsp; parameters LoRA. The resolution here is not&nbsp;&nbsp;

- [00:07:01](https://www.youtube.com/watch?v=XNcn845UXdw&t=421) important. I had changed it, but it was like 960,&nbsp; 960 by default. Okay, we have selected our models.&nbsp;

- [00:07:08](https://www.youtube.com/watch?v=XNcn845UXdw&t=428) Now, how do we set up the parameters, which is&nbsp; one of the most important things. First of all,&nbsp;&nbsp;

- [00:07:14](https://www.youtube.com/watch?v=XNcn845UXdw&t=434) you will see that there is Fast CausVid Wan 2.1&nbsp; image-to-video. So, click here and direct apply,&nbsp;&nbsp;

- [00:07:20](https://www.youtube.com/watch?v=XNcn845UXdw&t=440) and it will apply all the settings for you.&nbsp; So, let's verify them. Step count is 8,&nbsp;&nbsp;

- [00:07:26](https://www.youtube.com/watch?v=XNcn845UXdw&t=446) but you can also use step count 4. So, either 8 or&nbsp; 4. CFG scale is 1. This is image-to-video model,&nbsp;&nbsp;

- [00:07:33](https://www.youtube.com/watch?v=XNcn845UXdw&t=453) so I uncheck this. This may be checked because of&nbsp; your other previous usages. Whichever the aspect&nbsp;&nbsp;

- [00:07:39](https://www.youtube.com/watch?v=XNcn845UXdw&t=459) ratio you prefer, it will depend on the image I&nbsp; will show. The sampling is UniPC. This is super&nbsp;&nbsp;

- [00:07:45](https://www.youtube.com/watch?v=XNcn845UXdw&t=465) important. You need to make init image selected.&nbsp; Choose file, select the image you want to animate.&nbsp;&nbsp;

- [00:07:51](https://www.youtube.com/watch?v=XNcn845UXdw&t=471) So, for example, let's choose this. Then,&nbsp; click this "res" and use closest aspect ratio,&nbsp;&nbsp;

- [00:07:56](https://www.youtube.com/watch?v=XNcn845UXdw&t=476) and it will set the aspect ratio. You can&nbsp; also make this use exact aspect ratio,&nbsp;&nbsp;

- [00:08:01](https://www.youtube.com/watch?v=XNcn845UXdw&t=481) and it will use the exact aspect ratio like this.&nbsp; Then, init image creativity has to be zero. This&nbsp;&nbsp;

- [00:08:07](https://www.youtube.com/watch?v=XNcn845UXdw&t=487) is super important, so pay attention to that. All&nbsp; other settings are like this. Then, we will select&nbsp;&nbsp;

- [00:08:12](https://www.youtube.com/watch?v=XNcn845UXdw&t=492) the image-to-video model from here, whichever&nbsp; the model you are using. I am using GGUF Q. So,&nbsp;&nbsp;

- [00:08:17](https://www.youtube.com/watch?v=XNcn845UXdw&t=497) number of frames, this is super important.&nbsp; This will determine your video duration,&nbsp;&nbsp;

- [00:08:23](https://www.youtube.com/watch?v=XNcn845UXdw&t=503) and if you make it longer, it will use more VRAM.&nbsp; Not only it will take longer to process, it will&nbsp;&nbsp;

- [00:08:29](https://www.youtube.com/watch?v=XNcn845UXdw&t=509) use more VRAM, and if you use entire GPU VRAM,&nbsp; it will be very slow. You will see on nvitop. Let&nbsp;&nbsp;

- [00:08:38](https://www.youtube.com/watch?v=XNcn845UXdw&t=518) me demonstrate you again how to open nvitop. pip&nbsp; install nvitop. Type nvitop, and here. So, if you&nbsp;&nbsp;

- [00:08:45](https://www.youtube.com/watch?v=XNcn845UXdw&t=525) are seeing a very low watt usage here, that means&nbsp; that your GPU is using shared VRAM. Therefore,&nbsp;&nbsp;

- [00:08:52](https://www.youtube.com/watch?v=XNcn845UXdw&t=532) it will be super slow, and you want to avoid that.&nbsp; As a demonstration, let's make it 49 frames. Video&nbsp;&nbsp;

- [00:08:58](https://www.youtube.com/watch?v=XNcn845UXdw&t=538) CFG is 1, and video steps is 8. We are not using&nbsp; video-to-video creativity right now. Video output,&nbsp;&nbsp;

- [00:09:04](https://www.youtube.com/watch?v=XNcn845UXdw&t=544) I prefer H.265, and frame interpolation, I'm&nbsp; going to use RIFE, and I will make this 2. So,&nbsp;&nbsp;

- [00:09:12](https://www.youtube.com/watch?v=XNcn845UXdw&t=552) it will become 16 FPS to 32 FPS. And advanced&nbsp; video options, this is also important. Video FPS&nbsp;&nbsp;

- [00:09:18](https://www.youtube.com/watch?v=XNcn845UXdw&t=558) is 16. Video min CFG is 1. At the very bottom,&nbsp; there is other fixes, and we can set the first&nbsp;&nbsp;

- [00:09:26](https://www.youtube.com/watch?v=XNcn845UXdw&t=566) four frames to be trimmed. This is an error due&nbsp; to the Wan 2.1. This is not mandatory, so it will trim&nbsp;&nbsp;

- [00:09:32](https://www.youtube.com/watch?v=XNcn845UXdw&t=572) four frames from the beginning if you have, if&nbsp; you are getting errors. This is recommended,&nbsp;&nbsp;

- [00:09:37](https://www.youtube.com/watch?v=XNcn845UXdw&t=577) so you can also make it zero. Okay, what else&nbsp; we need to set? We need to set a prompt for&nbsp;&nbsp;

- [00:09:42](https://www.youtube.com/watch?v=XNcn845UXdw&t=582) this. So, let's select a prompt from our previous&nbsp; generation. For example, this is the prompt. Okay.&nbsp;&nbsp;

- [00:09:50](https://www.youtube.com/watch?v=XNcn845UXdw&t=590) The prompt is very simple. A red ant performing&nbsp; a weightlifting repetition, lifting the barbell&nbsp;&nbsp;

- [00:09:56](https://www.youtube.com/watch?v=XNcn845UXdw&t=596) up and then slowly lowering it. The ant's legs&nbsp; show effort. Our image is here, and hit generate.&nbsp;

- [00:10:02](https://www.youtube.com/watch?v=XNcn845UXdw&t=602) By the way, I have dual GPU, RTX 5090 and RTX&nbsp; 3090 Ti, and we can see that I have added them&nbsp;&nbsp;

- [00:10:10](https://www.youtube.com/watch?v=XNcn845UXdw&t=610) as a at the backend with the Sage Attention, as&nbsp; I have shown in the previous video. So, when I go&nbsp;&nbsp;

- [00:10:15](https://www.youtube.com/watch?v=XNcn845UXdw&t=615) to models and click load now, then I will click&nbsp; generate again, and it will start generating on&nbsp;&nbsp;

- [00:10:22](https://www.youtube.com/watch?v=XNcn845UXdw&t=622) my second GPU as well. So, let's just wait. If it&nbsp; doesn't work, I will show again. So, it is going&nbsp;&nbsp;

- [00:10:29](https://www.youtube.com/watch?v=XNcn845UXdw&t=629) to load. Yes, it started. So, we can compare&nbsp; their speed. As I said, pay attention to the&nbsp;&nbsp;

- [00:10:35](https://www.youtube.com/watch?v=XNcn845UXdw&t=635) watt usages here. It should be as much as close to&nbsp; your maximum watt. We can see that it fit into my&nbsp;&nbsp;

- [00:10:42](https://www.youtube.com/watch?v=XNcn845UXdw&t=642) VRAM of my GPU. It is using 19.5 GB of VRAM. So,&nbsp; it is using my entire GPU power. It is not using&nbsp;&nbsp;

- [00:10:50](https://www.youtube.com/watch?v=XNcn845UXdw&t=650) any shared VRAM or anything. Therefore, both of&nbsp; them is at their maximum speed. And when we go to&nbsp;&nbsp;

- [00:10:55](https://www.youtube.com/watch?v=XNcn845UXdw&t=655) the logs, we can see the speed of the each GPU.&nbsp; So, the RTX 5090 was 5.74 second IT. So, it took&nbsp;&nbsp;

- [00:11:06](https://www.youtube.com/watch?v=XNcn845UXdw&t=666) total like, let's see, 70 seconds to generate this&nbsp; video. Okay. This is the video. This didn't turn&nbsp;&nbsp;

- [00:11:14](https://www.youtube.com/watch?v=XNcn845UXdw&t=674) out to be very animated, so we need to try again&nbsp; with another seed or perhaps change the prompt.&nbsp;&nbsp;

- [00:11:20](https://www.youtube.com/watch?v=XNcn845UXdw&t=680) Let's see the RTX 3090 Ti speed. So, the RTX 3090&nbsp; Ti is like 11 seconds per IT because it is fitting&nbsp;&nbsp;

- [00:11:29](https://www.youtube.com/watch?v=XNcn845UXdw&t=689) into my VRAM. This is very crucial and important.&nbsp; So, the RTX 3090 Ti will take like 1.5 minutes.&nbsp;&nbsp;

- [00:11:36](https://www.youtube.com/watch?v=XNcn845UXdw&t=696) There isn't too much difference because both of&nbsp; them are fitting into VRAM. However, if it was&nbsp;&nbsp;

- [00:11:42](https://www.youtube.com/watch?v=XNcn845UXdw&t=702) not fitting into my VRAM, then the things would&nbsp; be very, very different, and that 8 GB of extra&nbsp;&nbsp;

- [00:11:49](https://www.youtube.com/watch?v=XNcn845UXdw&t=709) VRAM that RTX 5090 has is very, very crucial. So,&nbsp; this is the logic. We upload our image, we type&nbsp;&nbsp;

- [00:11:56](https://www.youtube.com/watch?v=XNcn845UXdw&t=716) our prompt, we set our settings. And if you want&nbsp; a longer videos, longer duration of generation,&nbsp;&nbsp;

- [00:12:02](https://www.youtube.com/watch?v=XNcn845UXdw&t=722) then you need to use video frames. Okay, this one&nbsp; has more motion. By the way, you see that it has&nbsp;&nbsp;

- [00:12:08](https://www.youtube.com/watch?v=XNcn845UXdw&t=728) a cropped leg because it is in the original&nbsp; image. I mean, it is not the model mistake.&nbsp;&nbsp;

- [00:12:14](https://www.youtube.com/watch?v=XNcn845UXdw&t=734) We can see that this is the original image that&nbsp; I am using. So, the leg is also missing here.&nbsp;&nbsp;

- [00:12:19](https://www.youtube.com/watch?v=XNcn845UXdw&t=739) And this is the animated image. I am also working&nbsp; an very, very amazing upscaler, video upscaler,&nbsp;&nbsp;

- [00:12:26](https://www.youtube.com/watch?v=XNcn845UXdw&t=746) so you will be able to generate videos in lower&nbsp; resolution and upscale later. It is diffusion&nbsp;&nbsp;

- [00:12:32](https://www.youtube.com/watch?v=XNcn845UXdw&t=752) based, like the newest model of Topaz AI, but it&nbsp; will be better than Topaz AI. You will see it. So,&nbsp;&nbsp;

- [00:12:38](https://www.youtube.com/watch?v=XNcn845UXdw&t=758) our videos are getting generated. So, where they&nbsp; are saved? They are saved inside the SwarmUI,&nbsp;&nbsp;

- [00:12:43](https://www.youtube.com/watch?v=XNcn845UXdw&t=763) inside output, inside local, raw, and they will&nbsp; be inside here, the date of my generations. So,&nbsp;&nbsp;

- [00:12:50](https://www.youtube.com/watch?v=XNcn845UXdw&t=770) you can directly share them. They are saved as MP4&nbsp; files, and you can play them like this. And you&nbsp;&nbsp;

- [00:12:56](https://www.youtube.com/watch?v=XNcn845UXdw&t=776) can see that it is completing that missing leg at&nbsp; a part of the video. Yes, I can see here. Yes, you&nbsp;&nbsp;

- [00:13:02](https://www.youtube.com/watch?v=XNcn845UXdw&t=782) see, the model is just completing it itself. Wow,&nbsp; this turned out to be really, really great. You&nbsp;&nbsp;

- [00:13:07](https://www.youtube.com/watch?v=XNcn845UXdw&t=787) see the motion? This is a full motion. Just from&nbsp; this input image, this video has been generated&nbsp;&nbsp;

- [00:13:14](https://www.youtube.com/watch?v=XNcn845UXdw&t=794) under 60 seconds locally on my computer. So, this&nbsp; is the power of SwarmUI and ComfyUI backend with&nbsp;&nbsp;

- [00:13:21](https://www.youtube.com/watch?v=XNcn845UXdw&t=801) my installation, with my workflow, setup, and&nbsp; optimizations. And it is just amazing, amazing&nbsp;&nbsp;

- [00:13:28](https://www.youtube.com/watch?v=XNcn845UXdw&t=808) motion, amazing movement. It also completed&nbsp; the missing arm, so this is just perfect.&nbsp;

- [00:13:33](https://www.youtube.com/watch?v=XNcn845UXdw&t=813) Okay, so let's move to text-to-video generation.&nbsp; For text-to-video generation, we are going&nbsp;&nbsp;

- [00:13:37](https://www.youtube.com/watch?v=XNcn845UXdw&t=817) to use Wan 2.1 text-to-video. By the way, you&nbsp; can set different images to every one of these&nbsp;&nbsp;

- [00:13:44](https://www.youtube.com/watch?v=XNcn845UXdw&t=824) models if you are getting confused. For example,&nbsp; upload the image here, click here, edit metadata,&nbsp;&nbsp;

- [00:13:50](https://www.youtube.com/watch?v=XNcn845UXdw&t=830) and say use image and save, and it will use that&nbsp; image. So, you won't get confused with that. So,&nbsp;&nbsp;

- [00:13:57](https://www.youtube.com/watch?v=XNcn845UXdw&t=837) where is text-to-video model here? Wan 2.1&nbsp; text-to-video GGUF. So, I will select it, and&nbsp;&nbsp;

- [00:14:02](https://www.youtube.com/watch?v=XNcn845UXdw&t=842) I will say load now. And I will go to presets, and&nbsp; I will say text-to-video apply, direct apply. Now,&nbsp;&nbsp;

- [00:14:10](https://www.youtube.com/watch?v=XNcn845UXdw&t=850) it will apply all the text-to-video presets. In&nbsp; the text-to-video, we are not using init image.&nbsp;&nbsp;

- [00:14:14](https://www.youtube.com/watch?v=XNcn845UXdw&t=854) This is important. What are the other changes?&nbsp; Images 1, seed -1. Okay, CFG is 1. Steps is 8.&nbsp;&nbsp;

- [00:14:21](https://www.youtube.com/watch?v=XNcn845UXdw&t=861) Text-to-video is selected. Text-to-video FPS 16.&nbsp; Number of frames 49. You see frame interpolation,&nbsp;&nbsp;

- [00:14:28](https://www.youtube.com/watch?v=XNcn845UXdw&t=868) RIFE is also available here. So, I am making it 2&nbsp; so that it will increase, double the FPS. And the&nbsp;&nbsp;

- [00:14:34](https://www.youtube.com/watch?v=XNcn845UXdw&t=874) resolution, as I said, you can set the resolution&nbsp; of the base model to something different from 960&nbsp;&nbsp;

- [00:14:40](https://www.youtube.com/watch?v=XNcn845UXdw&t=880) to like 640x640 or 768x768. So, depending&nbsp; on your GPU and the time you want to wait,&nbsp;&nbsp;

- [00:14:48](https://www.youtube.com/watch?v=XNcn845UXdw&t=888) then I can select any custom aspect ratio, and it&nbsp; will set the resolution accordingly. By the way,&nbsp;&nbsp;

- [00:14:53](https://www.youtube.com/watch?v=XNcn845UXdw&t=893) switch model, so it will update it. Okay, let's&nbsp; generate 9 to 16. This is the resolution. Let's&nbsp;&nbsp;

- [00:14:59](https://www.youtube.com/watch?v=XNcn845UXdw&t=899) copy one of the prompts from the older. Okay, this&nbsp; is the prompt. Let's copy paste it here. Then what&nbsp;&nbsp;

- [00:15:05](https://www.youtube.com/watch?v=XNcn845UXdw&t=905) other changes we have? I am using sampling as&nbsp; UniPC. Scheduler is default. No init image. There&nbsp;&nbsp;

- [00:15:11](https://www.youtube.com/watch?v=XNcn845UXdw&t=911) is no image-to-video, so uncheck this. Advanced&nbsp; video, video FPS 16, and video CFG is 1. So,&nbsp;&nbsp;

- [00:15:18](https://www.youtube.com/watch?v=XNcn845UXdw&t=918) these are all the settings. I am also using other&nbsp; fixes, trim video start 4 frames. Then all I need&nbsp;&nbsp;

- [00:15:24](https://www.youtube.com/watch?v=XNcn845UXdw&t=924) to do is generate. By the way, don't forget&nbsp; to select LoRA as well because with this LoRA,&nbsp;&nbsp;

- [00:15:29](https://www.youtube.com/watch?v=XNcn845UXdw&t=929) we are able to get this quality only in&nbsp; 8 steps. And furthermore, to speed up,&nbsp;&nbsp;

- [00:15:34](https://www.youtube.com/watch?v=XNcn845UXdw&t=934) you can install TeaCache. However, we are already&nbsp; doing 8 steps, so TeaCache probably would break it.&nbsp;&nbsp;

- [00:15:40](https://www.youtube.com/watch?v=XNcn845UXdw&t=940) And if you want a higher quality, you can always&nbsp; remove the Sage Attention, which is speeding up&nbsp;&nbsp;

- [00:15:46](https://www.youtube.com/watch?v=XNcn845UXdw&t=946) the generation, and get even a better quality. So, let's see the speed of the text-to-video&nbsp;&nbsp;

- [00:15:52](https://www.youtube.com/watch?v=XNcn845UXdw&t=952) generation on both of the GPUs. Pay attention that&nbsp; both of them should fit into VRAM, otherwise that&nbsp;&nbsp;

- [00:15:58](https://www.youtube.com/watch?v=XNcn845UXdw&t=958) wouldn't be a fair comparison. Okay. So, with 49&nbsp; frames and with this resolution, which is the,&nbsp;&nbsp;

- [00:16:05](https://www.youtube.com/watch?v=XNcn845UXdw&t=965) let me show you one more time, 576 to 1008. It is&nbsp; using 19.8 GB of VRAM. Let's see the speed in the&nbsp;&nbsp;

- [00:16:15](https://www.youtube.com/watch?v=XNcn845UXdw&t=975) logs, debug. Okay. So, the speed of RTX 5090 is&nbsp; 8.42 seconds per IT. Let's see the speed of the&nbsp;&nbsp;

- [00:16:24](https://www.youtube.com/watch?v=XNcn845UXdw&t=984) 3090 Ti. The ComfyUI 0 is my 5090, and the ComfyUI&nbsp; 1 is my 3090 Ti. Yes. So, it is 8.46 seconds to&nbsp;&nbsp;

- [00:16:34](https://www.youtube.com/watch?v=XNcn845UXdw&t=994) 18.24 seconds. However, I am recording a video,&nbsp; and it is also using a lot of GPU power as well.&nbsp;&nbsp;

- [00:16:40](https://www.youtube.com/watch?v=XNcn845UXdw&t=1000) Therefore, this is not a very fair comparison,&nbsp; but you are seeing the speed difference. It is&nbsp;&nbsp;

- [00:16:45](https://www.youtube.com/watch?v=XNcn845UXdw&t=1005) more than twice. All right, the first video&nbsp; has been generated, and it is looking really,&nbsp;&nbsp;

- [00:16:50](https://www.youtube.com/watch?v=XNcn845UXdw&t=1010) really great. By the way, I used this prompt from&nbsp; CivitAI, and this is the output. And hopefully,&nbsp;&nbsp;

- [00:16:56](https://www.youtube.com/watch?v=XNcn845UXdw&t=1016) when the my diffusion based upscaler arrives, it&nbsp; will be way better. Let me also show you a quick&nbsp;&nbsp;

- [00:17:04](https://www.youtube.com/watch?v=XNcn845UXdw&t=1024) peek of it. It will be inside ultimate video. So,&nbsp; let's click try Star.bat file. I am adding so&nbsp;&nbsp;

- [00:17:10](https://www.youtube.com/watch?v=XNcn845UXdw&t=1030) many features to this video upscaler application.&nbsp; It will be able to automatically split video into&nbsp;&nbsp;

- [00:17:18](https://www.youtube.com/watch?v=XNcn845UXdw&t=1038) the clips so that you will be able to caption&nbsp; prompt each part separately and upscale each part&nbsp;&nbsp;

- [00:17:26](https://www.youtube.com/watch?v=XNcn845UXdw&t=1046) separately so that the changed motions will have&nbsp; accurate descriptions. So, this will be the basic&nbsp;&nbsp;

- [00:17:32](https://www.youtube.com/watch?v=XNcn845UXdw&t=1052) interface. I am still improving it and adding new&nbsp; features. It will be able to automatically caption&nbsp;&nbsp;

- [00:17:38](https://www.youtube.com/watch?v=XNcn845UXdw&t=1058) each video with CogVLM2 video model. Then it will&nbsp; support ratio upscale, downscale, then upscale,&nbsp;&nbsp;

- [00:17:45](https://www.youtube.com/watch?v=XNcn845UXdw&t=1065) a lot of features. It will support max frame&nbsp; per batch. So, with using a more powerful GPU,&nbsp;&nbsp;

- [00:17:51](https://www.youtube.com/watch?v=XNcn845UXdw&t=1071) you will be able to get 10 times, more than 10&nbsp; times better quality than the paid premium Topaz&nbsp;&nbsp;

- [00:17:57](https://www.youtube.com/watch?v=XNcn845UXdw&t=1077) AI model. It will have all kind of optimizations,&nbsp; FFmpeg presets. So, if you have, you know,&nbsp;&nbsp;

- [00:18:03](https://www.youtube.com/watch?v=XNcn845UXdw&t=1083) any ideas, you can message me about this. This&nbsp; is not done yet. It is still in development,&nbsp;&nbsp;

- [00:18:08](https://www.youtube.com/watch?v=XNcn845UXdw&t=1088) and it is taking huge time, but hopefully, this&nbsp; will be amazing, the very best video upscaler.&nbsp;

- [00:18:13](https://www.youtube.com/watch?v=XNcn845UXdw&t=1093) I hope that you have enjoyed the today's video.&nbsp; So, we can see that two videos have been generated&nbsp;&nbsp;

- [00:18:19](https://www.youtube.com/watch?v=XNcn845UXdw&t=1099) on each GPU. The quality is magnificent. The time&nbsp; it takes is magnificent. So, the RTX 3090 Ti took&nbsp;&nbsp;

- [00:18:27](https://www.youtube.com/watch?v=XNcn845UXdw&t=1107) 170 seconds. RTX 5090 took 90 seconds, under&nbsp; 2 minutes. Only if you increase the resolution&nbsp;&nbsp;

- [00:18:34](https://www.youtube.com/watch?v=XNcn845UXdw&t=1114) and if you increase the frame count, it will&nbsp; use more VRAM, and it will take more time. So,&nbsp;&nbsp;

- [00:18:39](https://www.youtube.com/watch?v=XNcn845UXdw&t=1119) it is up to you to decide which resolution&nbsp; and which frame. Hopefully, see you later.
