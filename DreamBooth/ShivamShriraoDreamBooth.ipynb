{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FurkanGozukara/Stable-Diffusion/blob/main/DreamBooth/ShivamShriraoDreamBooth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzM7j0ZSc_9c"
      },
      "source": [
        "# We have many times better workflow than this notebook\n",
        "# Read (public post) : https://www.patreon.com/posts/full-workflow-sd-98620163\n",
        "## Tutorial Video for this colab: https://youtu.be/mnCY8uM7E50\n",
        "### If you want to use custom model and to learn more about folder paths and directories watch tutorial video : https://youtu.be/kIyqAdd_i10\n",
        "### Our Discord : https://discord.com/servers/software-engineering-courses-secourses-772774097734074388\n",
        "### Ignore warnings and errors. Leave a comment if not works\n",
        "### Updated and Verified Working 16 May 2025\n",
        "### DreamBooth master tutorial : https://youtu.be/Bdl-jWR3Ukc\n",
        "### Best DreamBooth settings : https://youtu.be/sRdtVanSRl4\n",
        "### Stable Diffusion playlist : https://www.youtube.com/playlist?list=PL_pbwdIyffsmclLl0O144nQRnezKlNdx3\n",
        "### Stable Diffusion repo : https://github.com/FurkanGozukara/Stable-Diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnTMyW41cC1E"
      },
      "source": [
        "## Install Requirements - ignore warnings and errors. if training won't work leave a comment to the tutorial video and hopefully I will fix asap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1 super important\n",
        "import os\n",
        "!pip install --quiet --force-reinstall --no-deps numpy==1.26.4\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "2lqTvGmaBxbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLWXPZqjsZVV"
      },
      "outputs": [],
      "source": [
        "!wget -q https://gist.githubusercontent.com/FurkanGozukara/be7be5f9f7820d0bb85a3052874f184e/raw/d8d179da6cab0735bd5832029c2dec5163db87b4/train_dreambooth.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "%pip uninstall torchtext --yes\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install torch==2.2.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --upgrade\n",
        "%pip install -q -U --pre triton --upgrade\n",
        "%pip install -q transformers ftfy gradio natsort safetensors\n",
        "%pip install bitsandbytes==0.41.3  --upgrade\n",
        "%pip install xformers==0.0.24  --upgrade\n",
        "%pip install triton==2.2.0 --upgrade\n",
        "%pip install diffusers==0.27.0 --upgrade\n",
        "%pip install huggingface_hub==0.25.2\n",
        "%pip install numpy==1.26.4\n",
        "%pip install transformers==4.43\n",
        "%pip install torchao==0.9.0\n",
        "%pip install accelerate==1.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NV324ZcL9L"
      },
      "source": [
        "## Settings and run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rxg0y5MBudmd",
        "outputId": "a273389f-dff2-4e61-f0b9-8f0953298048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Weights will be saved at /content/stable_diffusion_weights/ohwx\n"
          ]
        }
      ],
      "source": [
        "#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "save_to_gdrive = False #@param {type:\"boolean\"}\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Name/Path of the initial model.\n",
        "MODEL_NAME = \"stable-diffusion-v1-5/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = \"stable_diffusion_weights/ohwx\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "# Start Training\n",
        "\n",
        "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ioxxvHoicPs"
      },
      "source": [
        "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
        "\n",
        "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
        "\n",
        "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vDpCxId1aCm"
      },
      "outputs": [],
      "source": [
        "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
        "\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      \"ohwx man\",\n",
        "        \"class_prompt\":         \"photo of man\",\n",
        "        \"instance_data_dir\":    \"/content/data/ohwx\",\n",
        "        \"class_data_dir\":       \"/content/data/man\"\n",
        "    },\n",
        "#     {\n",
        "#         \"instance_prompt\":      \"photo of ukj person\",\n",
        "#         \"class_prompt\":         \"photo of a person\",\n",
        "#         \"instance_data_dir\":    \"/content/data/ukj\",\n",
        "#         \"class_data_dir\":       \"/content/data/person\"\n",
        "#     }\n",
        "]\n",
        "\n",
        "# `class_data_dir` contains regularization images\n",
        "import json\n",
        "import os\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32gYIDDR1aCp"
      },
      "outputs": [],
      "source": [
        "#@markdown Upload your images by running this cell.\n",
        "\n",
        "#@markdown OR\n",
        "\n",
        "#@markdown You can use the file manager on the left panel to upload (drag and drop) to each `instance_data_dir` (it uploads faster). You can also upload your own class images in `class_data_dir` if u don't wanna generate with SD.\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "for c in concepts_list:\n",
        "    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
        "        shutil.move(filename, dst_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If you use another custom repo don't forget to change revision to main"
      ],
      "metadata": {
        "id": "ofWvxTEyVVbU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b78c8ad0-039e-46e2-ffa4-4dead081815a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uintx feature requires torch 2.3+, please upgrade pytorch\n",
            "2025-05-16 09:29:25.225388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747387765.558198    6927 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747387765.644248    6927 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 547/547 [00:00<00:00, 4.11MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 291MB/s]\n",
            "model_index.json: 100% 541/541 [00:00<00:00, 4.27MB/s]\n",
            "Fetching 11 files:   0% 0/11 [00:00<?, ?it/s]\n",
            "model.safetensors:   0% 0.00/492M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "preprocessor_config.json: 100% 342/342 [00:00<00:00, 2.54MB/s]\n",
            "Fetching 11 files:   9% 1/11 [00:00<00:01,  6.27it/s]\n",
            "\n",
            "config.json: 100% 617/617 [00:00<00:00, 4.32MB/s]\n",
            "\n",
            "\n",
            "merges.txt:   0% 0.00/525k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer_config.json: 100% 806/806 [00:00<00:00, 7.48MB/s]\n",
            "\n",
            "\n",
            "\n",
            "scheduler_config.json: 100% 308/308 [00:00<00:00, 3.05MB/s]\n",
            "\n",
            "\n",
            "\n",
            "vocab.json:   0% 0.00/1.06M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   4% 21.0M/492M [00:00<00:02, 174MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   0% 0.00/3.44G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "special_tokens_map.json: 100% 472/472 [00:00<00:00, 3.30MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "config.json: 100% 743/743 [00:00<00:00, 4.12MB/s]\n",
            "\n",
            "\n",
            "merges.txt: 100% 525k/525k [00:00<00:00, 2.80MB/s]\n",
            "\n",
            "model.safetensors:  11% 52.4M/492M [00:00<00:02, 214MB/s]\u001b[A\n",
            "\n",
            "\n",
            "vocab.json: 100% 1.06M/1.06M [00:00<00:00, 5.36MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 21.0M/3.44G [00:00<00:26, 127MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  17% 83.9M/492M [00:00<00:02, 204MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 41.9M/3.44G [00:00<00:22, 153MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  21% 105M/492M [00:00<00:02, 173MB/s] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 62.9M/3.44G [00:00<00:24, 139MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  26% 126M/492M [00:00<00:02, 160MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 83.9M/3.44G [00:00<00:24, 137MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  30% 147M/492M [00:00<00:02, 155MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   3% 105M/3.44G [00:00<00:22, 147MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  34% 168M/492M [00:01<00:02, 150MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   4% 126M/3.44G [00:00<00:23, 143MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  38% 189M/492M [00:01<00:01, 162MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 157M/3.44G [00:01<00:18, 174MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  45% 220M/492M [00:01<00:01, 165MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 178M/3.44G [00:01<00:20, 161MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  49% 241M/492M [00:01<00:01, 147MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   6% 199M/3.44G [00:01<00:22, 142MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  53% 262M/492M [00:01<00:01, 131MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   6% 220M/3.44G [00:01<00:24, 129MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  58% 283M/492M [00:01<00:01, 108MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   7% 241M/3.44G [00:01<00:30, 104MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   8% 262M/3.44G [00:12<08:14, 6.43MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  62% 304M/492M [00:12<00:28, 6.54MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   9% 294M/3.44G [00:12<05:03, 10.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  68% 336M/492M [00:12<00:15, 10.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   9% 315M/3.44G [00:12<03:45, 13.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  75% 367M/492M [00:12<00:07, 15.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  10% 336M/3.44G [00:12<02:48, 18.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  79% 388M/492M [00:12<00:05, 20.4MB/s]\u001b[A\n",
            "model.safetensors:  83% 409M/492M [00:13<00:03, 26.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  10% 357M/3.44G [00:12<02:04, 24.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  87% 430M/492M [00:13<00:01, 34.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  11% 377M/3.44G [00:13<01:33, 32.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  12% 398M/3.44G [00:13<01:11, 42.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:  92% 451M/492M [00:13<00:00, 43.3MB/s]\u001b[A\n",
            "model.safetensors:  96% 472M/492M [00:13<00:00, 54.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  12% 419M/3.44G [00:13<00:59, 51.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors: 100% 492M/492M [00:13<00:00, 64.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors: 100% 492M/492M [00:13<00:00, 35.9MB/s]\n",
            "Fetching 11 files:  45% 5/11 [00:13<00:17,  2.94s/it]\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  13% 461M/3.44G [00:13<00:38, 77.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  14% 493M/3.44G [00:13<00:27, 107MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  15% 514M/3.44G [00:13<00:23, 122MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  16% 545M/3.44G [00:14<00:19, 150MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  17% 577M/3.44G [00:14<00:16, 178MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  18% 608M/3.44G [00:14<00:14, 192MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  19% 640M/3.44G [00:14<00:13, 215MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  20% 671M/3.44G [00:14<00:11, 238MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  20% 703M/3.44G [00:14<00:10, 256MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  21% 734M/3.44G [00:14<00:12, 213MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  23% 776M/3.44G [00:14<00:10, 245MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  23% 807M/3.44G [00:15<00:10, 243MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  24% 839M/3.44G [00:15<00:10, 259MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  25% 870M/3.44G [00:15<00:09, 271MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  26% 902M/3.44G [00:15<00:10, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  27% 944M/3.44G [00:15<00:09, 260MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  29% 986M/3.44G [00:15<00:09, 270MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  30% 1.02G/3.44G [00:15<00:11, 217MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  30% 1.05G/3.44G [00:16<00:10, 237MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  31% 1.08G/3.44G [00:16<00:09, 253MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  32% 1.11G/3.44G [00:16<00:08, 265MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  34% 1.15G/3.44G [00:16<00:08, 283MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  34% 1.18G/3.44G [00:16<00:10, 218MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  35% 1.22G/3.44G [00:16<00:09, 235MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  36% 1.25G/3.44G [00:16<00:08, 250MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  37% 1.28G/3.44G [00:16<00:08, 261MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  38% 1.31G/3.44G [00:17<00:07, 273MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  39% 1.34G/3.44G [00:17<00:08, 237MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  40% 1.37G/3.44G [00:17<00:08, 253MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  41% 1.42G/3.44G [00:17<00:07, 275MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  42% 1.45G/3.44G [00:17<00:07, 253MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  43% 1.48G/3.44G [00:17<00:08, 238MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  44% 1.51G/3.44G [00:17<00:07, 252MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  45% 1.55G/3.44G [00:17<00:06, 279MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  46% 1.59G/3.44G [00:18<00:06, 301MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  47% 1.63G/3.44G [00:18<00:08, 221MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  48% 1.66G/3.44G [00:18<00:07, 228MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  49% 1.69G/3.44G [00:18<00:07, 244MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  50% 1.72G/3.44G [00:18<00:07, 244MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  51% 1.75G/3.44G [00:18<00:06, 258MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  52% 1.78G/3.44G [00:18<00:06, 272MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  53% 1.81G/3.44G [00:19<00:06, 265MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  54% 1.85G/3.44G [00:19<00:06, 249MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  55% 1.88G/3.44G [00:19<00:06, 229MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  56% 1.91G/3.44G [00:19<00:06, 228MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  57% 1.95G/3.44G [00:19<00:05, 253MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  58% 1.98G/3.44G [00:19<00:05, 256MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  59% 2.01G/3.44G [00:19<00:05, 255MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  59% 2.04G/3.44G [00:19<00:05, 267MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  60% 2.08G/3.44G [00:20<00:05, 256MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  61% 2.11G/3.44G [00:20<00:05, 256MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  62% 2.14G/3.44G [00:20<00:04, 270MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  63% 2.17G/3.44G [00:20<00:04, 256MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  64% 2.20G/3.44G [00:20<00:04, 249MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  65% 2.23G/3.44G [00:20<00:04, 262MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  66% 2.26G/3.44G [00:20<00:04, 240MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  67% 2.31G/3.44G [00:20<00:04, 267MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  68% 2.35G/3.44G [00:21<00:04, 271MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  69% 2.38G/3.44G [00:21<00:04, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  70% 2.41G/3.44G [00:21<00:04, 217MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  71% 2.44G/3.44G [00:21<00:04, 232MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  72% 2.47G/3.44G [00:21<00:03, 248MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  73% 2.52G/3.44G [00:21<00:03, 274MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  74% 2.56G/3.44G [00:21<00:03, 292MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  75% 2.59G/3.44G [00:22<00:03, 248MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  76% 2.62G/3.44G [00:22<00:03, 228MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  77% 2.65G/3.44G [00:22<00:03, 222MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  78% 2.68G/3.44G [00:22<00:03, 228MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  79% 2.72G/3.44G [00:22<00:03, 217MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  80% 2.75G/3.44G [00:22<00:02, 235MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  81% 2.78G/3.44G [00:22<00:02, 251MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  82% 2.81G/3.44G [00:23<00:02, 229MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  83% 2.84G/3.44G [00:23<00:02, 225MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  84% 2.87G/3.44G [00:23<00:02, 235MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  84% 2.90G/3.44G [00:23<00:02, 254MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  85% 2.94G/3.44G [00:23<00:02, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  86% 2.97G/3.44G [00:23<00:02, 231MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  87% 3.00G/3.44G [00:23<00:01, 236MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  88% 3.03G/3.44G [00:24<00:02, 170MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  89% 3.06G/3.44G [00:24<00:01, 194MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  90% 3.09G/3.44G [00:24<00:01, 206MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 3.12G/3.44G [00:24<00:01, 212MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  92% 3.16G/3.44G [00:24<00:01, 229MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  93% 3.19G/3.44G [00:24<00:01, 241MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  94% 3.22G/3.44G [00:30<00:12, 17.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  95% 3.26G/3.44G [00:30<00:06, 26.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  96% 3.30G/3.44G [00:30<00:03, 38.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  97% 3.33G/3.44G [00:30<00:02, 50.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  98% 3.37G/3.44G [00:31<00:01, 64.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  99% 3.40G/3.44G [00:31<00:00, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [00:32<00:00, 105MB/s] \n",
            "Fetching 11 files: 100% 11/11 [00:32<00:00,  2.99s/it]\n",
            "Loading pipeline components...: 100% 6/6 [00:04<00:00,  1.28it/s]\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "Generating class images: 100% 6/6 [00:23<00:00,  3.86s/it]\n",
            "config.json: 100% 547/547 [00:00<00:00, 4.80MB/s]\n",
            "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 275MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
            "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "Caching latents: 100% 6/6 [00:02<00:00,  2.64it/s]\n",
            "Steps:  17% 136/800 [01:45<08:48,  1.26it/s, loss=0.216, lr=1e-6]"
          ]
        }
      ],
      "source": [
        "!python3 train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"main\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=6 \\\n",
        "  --sample_batch_size=1 \\\n",
        "  --max_train_steps=800 \\\n",
        "  --save_interval=10000 \\\n",
        "  --save_sample_prompt=\"photo of ohwx man\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "89Az5NUxOWdy"
      },
      "outputs": [],
      "source": [
        "#@markdown Specify the weights directory to use (leave blank for latest)\n",
        "WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4UJm2ikBa836"
      },
      "outputs": [],
      "source": [
        "#@markdown Run to generate a grid of preview images from the last saved weights.\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "weights_folder = OUTPUT_DIR\n",
        "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
        "\n",
        "row = len(folders)\n",
        "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
        "scale = 4\n",
        "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "    folder_path = os.path.join(weights_folder, folder)\n",
        "    image_folder = os.path.join(folder_path, \"samples\")\n",
        "    images = [f for f in os.listdir(image_folder)]\n",
        "    for j, image in enumerate(images):\n",
        "        if row == 1:\n",
        "            currAxes = axes[j]\n",
        "        else:\n",
        "            currAxes = axes[i, j]\n",
        "        if i == 0:\n",
        "            currAxes.set_title(f\"Image {j}\")\n",
        "        if j == 0:\n",
        "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        img = mpimg.imread(image_path)\n",
        "        currAxes.imshow(img, cmap='gray')\n",
        "        currAxes.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('grid.png', dpi=72)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V8wgU0HN-Kq"
      },
      "source": [
        "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dcXzsUyG1aCy"
      },
      "outputs": [],
      "source": [
        "#@markdown Run conversion.\n",
        "ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
        "\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNG4fd_dTbF"
      },
      "source": [
        "## Inference\n",
        "### set your model_path parameter according to the above [*] Weights saved at message printed on training logs\n",
        "### alternatively watch this tutorial to learn : https://youtu.be/kIyqAdd_i10\n",
        "### e.g. model_path = '/content/stable_diffusion_weights/ohwx/800'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW15FjffdTID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = '/content/stable_diffusion_weights/ohwx/800'             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "g_cuda = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oIzkltjpVO_f"
      },
      "outputs": [],
      "source": [
        "#@markdown Can set random seed here for reproducibility.\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K6xoHWSsbcS3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#@title Run for generating images.\n",
        "\n",
        "prompt = \"photo of ohwx man in tomer hanuka style\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "num_samples = 4 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "num_inference_steps = 24 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WMCqQ5Tcdsm2"
      },
      "outputs": [],
      "source": [
        "#@markdown Run Gradio UI for generating images.\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        return pipe(\n",
        "                prompt, height=int(height), width=int(width),\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_images_per_prompt=int(num_samples),\n",
        "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "                generator=g_cuda\n",
        "            ).images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=24)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lJoOgLQHnC8L"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Delete diffuser and old weights and only keep the ckpt to free up drive space.\n",
        "\n",
        "#@markdown [ ! ] Caution, Only execute if you are sure u want to delete the diffuser format weights and only use the ckpt.\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "for f in glob(OUTPUT_DIR+os.sep+\"*\"):\n",
        "    if f != WEIGHTS_DIR:\n",
        "        shutil.rmtree(f)\n",
        "        print(\"Deleted\", f)\n",
        "for f in glob(WEIGHTS_DIR+\"/*\"):\n",
        "    if not f.endswith(\".ckpt\") or not f.endswith(\".json\"):\n",
        "        try:\n",
        "            shutil.rmtree(f)\n",
        "        except NotADirectoryError:\n",
        "            continue\n",
        "        print(\"Deleted\", f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXgi8HM4c-DA"
      },
      "outputs": [],
      "source": [
        "#@title Free runtime memory\n",
        "exit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}